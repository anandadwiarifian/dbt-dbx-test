

============================== 2022-03-22 12:13:40.705921 | aebf66c3-ebb1-4e42-b0b4-aeb0aefd4b4d ==============================
12:13:40.705921 [info ] [MainThread]: Running with dbt=1.0.3
12:13:40.706635 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
12:13:40.706960 [debug] [MainThread]: Tracking: tracking
12:13:40.747848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a87640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a877f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a87310>]}
12:13:41.074824 [debug] [MainThread]: Executing "git --help"
12:13:41.088328 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
12:13:41.089349 [debug] [MainThread]: STDERR: "b''"
12:13:41.132967 [debug] [MainThread]: Acquiring new databricks connection "debug"
12:13:41.134750 [debug] [MainThread]: Using databricks connection "debug"
12:13:41.135113 [debug] [MainThread]: On debug: select 1 as id
12:13:41.135512 [debug] [MainThread]: Opening a new connection, currently in state init
12:13:41.890838 [debug] [MainThread]: SQL status: OK in 0.76 seconds
12:13:41.893218 [debug] [MainThread]: On debug: Close
12:13:42.114883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c8af10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c90c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c90b80>]}
12:13:43.405689 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-22 12:15:34.486214 | cc2dfaea-b626-40ea-b816-c0786992393a ==============================
12:15:34.486214 [info ] [MainThread]: Running with dbt=1.0.3
12:15:34.486924 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=None, exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
12:15:34.487257 [debug] [MainThread]: Tracking: tracking
12:15:34.509498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113107f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113107f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113107700>]}
12:15:34.536085 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
12:15:34.536479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cc2dfaea-b626-40ea-b816-c0786992393a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11311e940>]}
12:15:35.191877 [debug] [MainThread]: Parsing macros/adapters.sql
12:15:35.216869 [debug] [MainThread]: Parsing macros/materializations/seed.sql
12:15:35.221166 [debug] [MainThread]: Parsing macros/materializations/view.sql
12:15:35.221803 [debug] [MainThread]: Parsing macros/materializations/table.sql
12:15:35.225025 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
12:15:35.243251 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
12:15:35.247940 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
12:15:35.251674 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
12:15:35.256974 [debug] [MainThread]: Parsing macros/adapters.sql
12:15:35.292955 [debug] [MainThread]: Parsing macros/materializations/seed.sql
12:15:35.301423 [debug] [MainThread]: Parsing macros/materializations/view.sql
12:15:35.301929 [debug] [MainThread]: Parsing macros/materializations/table.sql
12:15:35.304508 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
12:15:35.332275 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
12:15:35.337639 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
12:15:35.343794 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
12:15:35.349067 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
12:15:35.352792 [debug] [MainThread]: Parsing macros/materializations/configs.sql
12:15:35.354878 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
12:15:35.356343 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
12:15:35.371451 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
12:15:35.381465 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
12:15:35.392377 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
12:15:35.396416 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
12:15:35.398019 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
12:15:35.399742 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
12:15:35.403711 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
12:15:35.413873 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
12:15:35.415222 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
12:15:35.424624 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
12:15:35.438718 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
12:15:35.445426 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
12:15:35.447972 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
12:15:35.454419 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
12:15:35.455580 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
12:15:35.457959 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
12:15:35.460014 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
12:15:35.465453 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
12:15:35.480246 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
12:15:35.481527 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
12:15:35.483646 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
12:15:35.485004 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
12:15:35.485773 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
12:15:35.486265 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
12:15:35.486879 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
12:15:35.488069 [debug] [MainThread]: Parsing macros/etc/statement.sql
12:15:35.492232 [debug] [MainThread]: Parsing macros/etc/datetime.sql
12:15:35.499692 [debug] [MainThread]: Parsing macros/adapters/schema.sql
12:15:35.501590 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
12:15:35.503947 [debug] [MainThread]: Parsing macros/adapters/relation.sql
12:15:35.512368 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
12:15:35.514903 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
12:15:35.518762 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
12:15:35.525323 [debug] [MainThread]: Parsing macros/adapters/columns.sql
12:15:35.533826 [debug] [MainThread]: Parsing tests/generic/builtin.sql
12:15:35.765694 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
12:15:35.775030 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
12:15:35.835688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc2dfaea-b626-40ea-b816-c0786992393a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131f68b0>]}
12:15:35.842402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc2dfaea-b626-40ea-b816-c0786992393a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131f67c0>]}
12:15:35.842736 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:15:35.844094 [info ] [MainThread]: 
12:15:35.844583 [debug] [MainThread]: Acquiring new databricks connection "master"
12:15:35.845400 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
12:15:35.855024 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:15:35.855362 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
12:15:35.855545 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
12:15:35.855748 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:15:38.093941 [debug] [ThreadPool]: SQL status: OK in 2.24 seconds
12:15:38.868274 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
12:15:38.868619 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:15:38.868854 [debug] [ThreadPool]: On list_None_ss_finance: Close
12:15:39.128057 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:15:39.128493 [info ] [MainThread]: 
12:15:39.147077 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
12:15:39.147591 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
12:15:39.150255 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
12:15:39.150502 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
12:15:39.153530 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
12:15:39.155461 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.155748 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
12:15:39.155985 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.156447 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
12:15:39.157254 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
12:15:39.157757 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
12:15:39.157971 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
12:15:39.158187 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
12:15:39.161203 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
12:15:39.178477 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.178765 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
12:15:39.178974 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.179384 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
12:15:39.179614 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:15:39.180052 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
12:15:39.180409 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:15:39.180631 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:15:39.193897 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
12:15:39.195057 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.195280 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:15:39.195482 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.195876 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:15:39.196109 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:15:39.196445 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
12:15:39.196728 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:15:39.197063 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:15:39.204059 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
12:15:39.205042 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.205417 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:15:39.205668 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.206275 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:15:39.206546 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:15:39.207198 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
12:15:39.207557 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:15:39.207763 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:15:39.211713 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
12:15:39.228703 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.229132 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:15:39.229494 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.230083 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:15:39.230377 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:15:39.230917 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
12:15:39.231192 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:15:39.231431 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:15:39.236016 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
12:15:39.237011 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.237272 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:15:39.237486 [debug] [Thread-1  ]: finished collecting timing info
12:15:39.237929 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:15:39.239036 [debug] [MainThread]: Connection 'master' was properly closed.
12:15:39.239505 [debug] [MainThread]: Connection 'test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
12:15:39.284807 [info ] [MainThread]: Done.
12:15:39.285261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131a0640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131f6940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11339b190>]}


============================== 2022-03-22 12:52:33.227537 | 0718b0ed-8519-49e2-ace5-bf20fef841ac ==============================
12:52:33.227537 [info ] [MainThread]: Running with dbt=1.0.3
12:52:33.228629 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=None, exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
12:52:33.228924 [debug] [MainThread]: Tracking: tracking
12:52:33.259341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9a9be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9a99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9a9fd0>]}
12:52:33.353709 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
12:52:33.354260 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_second_dbt_model.sql
12:52:33.354587 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_first_dbt_model.sql
12:52:33.365071 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
12:52:33.381019 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
12:52:33.415799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0718b0ed-8519-49e2-ace5-bf20fef841ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb70fd0>]}
12:52:33.435154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0718b0ed-8519-49e2-ace5-bf20fef841ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9b6ac0>]}
12:52:33.435931 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
12:52:33.438110 [info ] [MainThread]: 
12:52:33.438800 [debug] [MainThread]: Acquiring new databricks connection "master"
12:52:33.439945 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
12:52:33.450819 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:52:33.451114 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
12:52:33.451302 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
12:52:33.451477 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:52:34.567501 [debug] [ThreadPool]: SQL status: OK in 1.12 seconds
12:52:34.942808 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
12:52:34.943141 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:52:34.943379 [debug] [ThreadPool]: On list_None_ss_finance: Close
12:52:35.114449 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:52:35.115084 [info ] [MainThread]: 
12:52:35.123019 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
12:52:35.123535 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
12:52:35.126447 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
12:52:35.126798 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
12:52:35.129931 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
12:52:35.134105 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.134357 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
12:52:35.134596 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.135043 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
12:52:35.135695 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
12:52:35.136112 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
12:52:35.136316 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
12:52:35.136511 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
12:52:35.140085 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
12:52:35.143324 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.143687 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
12:52:35.143950 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.144561 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
12:52:35.145444 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:52:35.146061 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
12:52:35.146332 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:52:35.146551 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:52:35.158125 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
12:52:35.161218 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.161468 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:52:35.161707 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.162131 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
12:52:35.162630 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:52:35.163094 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
12:52:35.163299 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:52:35.163487 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:52:35.170470 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
12:52:35.173059 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.173291 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:52:35.173491 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.173868 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
12:52:35.174153 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:52:35.174601 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
12:52:35.174793 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:52:35.174970 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:52:35.178715 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
12:52:35.181383 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.181619 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:52:35.181817 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.182195 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
12:52:35.182436 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:52:35.182886 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
12:52:35.183084 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:52:35.183289 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:52:35.187130 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
12:52:35.189884 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.190107 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:52:35.190315 [debug] [Thread-1  ]: finished collecting timing info
12:52:35.190699 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
12:52:35.191537 [debug] [MainThread]: Connection 'master' was properly closed.
12:52:35.191734 [debug] [MainThread]: Connection 'test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
12:52:35.204529 [info ] [MainThread]: Done.
12:52:35.205065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb97b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ea85d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec48250>]}


============================== 2022-03-22 13:12:29.038642 | b980ac39-4c4f-43d8-ac9e-a833112522bf ==============================
13:12:29.038642 [info ] [MainThread]: Running with dbt=1.0.3
13:12:29.040186 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:12:29.040543 [debug] [MainThread]: Tracking: tracking
13:12:29.066115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050130d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105013670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502a7c0>]}
13:12:29.146905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
13:12:29.147430 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_second_dbt_model.sql
13:12:29.147814 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_first_dbt_model.sql
13:12:29.160834 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
13:12:29.175240 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
13:12:29.216537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b980ac39-4c4f-43d8-ac9e-a833112522bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051e4f40>]}
13:12:29.224294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b980ac39-4c4f-43d8-ac9e-a833112522bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105029f70>]}
13:12:29.224638 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:12:29.226048 [info ] [MainThread]: 
13:12:29.226591 [debug] [MainThread]: Acquiring new databricks connection "master"
13:12:29.227837 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
13:12:29.238394 [debug] [ThreadPool]: Using databricks connection "list_schemas"
13:12:29.238682 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
13:12:29.238874 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:12:30.074687 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
13:12:30.407615 [debug] [ThreadPool]: On list_schemas: Close
13:12:30.582684 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:12:30.591865 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:12:30.592144 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:12:30.592336 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:12:30.592516 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:12:31.414220 [debug] [ThreadPool]: SQL status: OK in 0.82 seconds
13:12:31.766702 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:12:31.767094 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:12:31.767331 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:12:31.941050 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:12:31.941442 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:12:31.941980 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:12:31.942468 [info ] [MainThread]: 
13:12:31.950946 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:12:31.951597 [info ] [Thread-1  ]: 1 of 2 START table model ss_finance.my_first_dbt_model.......................... [RUN]
13:12:31.952510 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:12:31.956720 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:12:31.958214 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:12:31.962705 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:12:31.964071 [debug] [Thread-1  ]: finished collecting timing info
13:12:31.964430 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:12:32.011930 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_first_dbt_model"
13:12:32.014018 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:12:32.014231 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_first_dbt_model"
13:12:32.014397 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:12:32.014557 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:12:35.822776 [debug] [Thread-1  ]: SQL status: OK in 3.81 seconds
13:12:35.829775 [debug] [Thread-1  ]: finished collecting timing info
13:12:35.830017 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: ROLLBACK
13:12:35.830209 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:12:35.830389 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: Close
13:12:36.099169 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b980ac39-4c4f-43d8-ac9e-a833112522bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105100040>]}
13:12:36.099578 [info ] [Thread-1  ]: 1 of 2 OK created table model ss_finance.my_first_dbt_model..................... [[32mOK[0m in 4.15s]
13:12:36.099904 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:12:36.100433 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:12:36.100706 [info ] [Thread-1  ]: 2 of 2 START table model ss_finance.my_second_dbt_model......................... [RUN]
13:12:36.101100 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
13:12:36.103034 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
13:12:36.103214 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
13:12:36.105767 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
13:12:36.106601 [debug] [Thread-1  ]: finished collecting timing info
13:12:36.106769 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
13:12:36.108961 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_second_dbt_model"
13:12:36.109669 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:12:36.109822 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_second_dbt_model"
13:12:36.109956 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_second_dbt_model"} */

      create or replace table ss_finance.my_second_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_second_dbt_model'
    
    as
      -- Use the `ref` function to select from other models



select *
from ss_finance.my_first_dbt_model
where id = 1
13:12:36.110083 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:12:39.907104 [debug] [Thread-1  ]: SQL status: OK in 3.8 seconds
13:12:39.910024 [debug] [Thread-1  ]: finished collecting timing info
13:12:39.910325 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: ROLLBACK
13:12:39.910569 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:12:39.910795 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: Close
13:12:40.075785 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b980ac39-4c4f-43d8-ac9e-a833112522bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052f5ac0>]}
13:12:40.076233 [info ] [Thread-1  ]: 2 of 2 OK created table model ss_finance.my_second_dbt_model.................... [[32mOK[0m in 3.97s]
13:12:40.076570 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:12:40.077649 [debug] [MainThread]: Acquiring new databricks connection "master"
13:12:40.077889 [debug] [MainThread]: On master: ROLLBACK
13:12:40.078060 [debug] [MainThread]: Opening a new connection, currently in state init
13:12:40.187487 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:12:40.187934 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:12:40.188223 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:12:40.188519 [debug] [MainThread]: On master: ROLLBACK
13:12:40.188788 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:12:40.189052 [debug] [MainThread]: On master: Close
13:12:40.292071 [info ] [MainThread]: 
13:12:40.292682 [info ] [MainThread]: Finished running 2 table models in 11.07s.
13:12:40.293154 [debug] [MainThread]: Connection 'master' was properly closed.
13:12:40.293432 [debug] [MainThread]: Connection 'model.test_dbx.my_second_dbt_model' was properly closed.
13:12:40.306524 [info ] [MainThread]: 
13:12:40.306918 [info ] [MainThread]: [32mCompleted successfully[0m
13:12:40.307287 [info ] [MainThread]: 
13:12:40.307593 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
13:12:40.307995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050296a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105155a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105301fd0>]}


============================== 2022-03-22 13:28:31.422767 | 3a3402fe-f283-4efa-b208-53e1a331477b ==============================
13:28:31.422767 [info ] [MainThread]: Running with dbt=1.0.3
13:28:31.423946 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, store_failures=False, indirect_selection='eager', threads=None, select=['example'], exclude=None, selector_name=None, state=None, defer=None, cls=<class 'dbt.task.test.TestTask'>, which='test', rpc_method='test')
13:28:31.424284 [debug] [MainThread]: Tracking: tracking
13:28:31.465933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11360f970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11360f3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11360fa00>]}
13:28:31.578844 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:28:31.579105 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:28:31.586693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a3402fe-f283-4efa-b208-53e1a331477b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113767f40>]}
13:28:31.595188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a3402fe-f283-4efa-b208-53e1a331477b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136e53a0>]}
13:28:31.595625 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:28:31.597129 [info ] [MainThread]: 
13:28:31.597659 [debug] [MainThread]: Acquiring new databricks connection "master"
13:28:31.598714 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:28:31.609096 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:28:31.609375 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:28:31.609569 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:28:31.609744 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:28:32.689137 [debug] [ThreadPool]: SQL status: OK in 1.08 seconds
13:28:33.118270 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:28:33.118620 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:28:33.118860 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:28:33.333762 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:28:33.334161 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:28:33.334652 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:28:33.335046 [info ] [MainThread]: 
13:28:33.341881 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:28:33.342287 [info ] [Thread-1  ]: 1 of 4 START test not_null_my_first_dbt_model_id................................ [RUN]
13:28:33.343820 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:28:33.346400 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:28:33.346660 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:28:33.359310 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:28:33.360465 [debug] [Thread-1  ]: finished collecting timing info
13:28:33.360712 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:28:33.379977 [debug] [Thread-1  ]: Writing runtime SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:28:33.381294 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:28:33.381560 [debug] [Thread-1  ]: Using databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:28:33.381771 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select *
from ss_finance.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
13:28:33.381974 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:28:34.003479 [debug] [Thread-1  ]: SQL status: OK in 0.62 seconds
13:28:34.297222 [debug] [Thread-1  ]: finished collecting timing info
13:28:34.297576 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710: ROLLBACK
13:28:34.297811 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:28:34.298032 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710: Close
13:28:34.472385 [error] [Thread-1  ]: 1 of 4 FAIL 1 not_null_my_first_dbt_model_id.................................... [[31mFAIL 1[0m in 1.13s]
13:28:34.473110 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:28:34.473405 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:28:34.473806 [info ] [Thread-1  ]: 2 of 4 START test not_null_my_second_dbt_model_id............................... [RUN]
13:28:34.474417 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:28:34.477468 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:28:34.477757 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:28:34.482855 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:28:34.484017 [debug] [Thread-1  ]: finished collecting timing info
13:28:34.484291 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:28:34.486555 [debug] [Thread-1  ]: Writing runtime SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:28:34.487398 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:28:34.487621 [debug] [Thread-1  ]: Using databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:28:34.487813 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select *
from ss_finance.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
13:28:34.487997 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:28:35.115434 [debug] [Thread-1  ]: SQL status: OK in 0.63 seconds
13:28:35.408894 [debug] [Thread-1  ]: finished collecting timing info
13:28:35.409336 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_second_dbt_model_id.151b76d778: ROLLBACK
13:28:35.409570 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:28:35.409788 [debug] [Thread-1  ]: On test.test_dbx.not_null_my_second_dbt_model_id.151b76d778: Close
13:28:35.582116 [info ] [Thread-1  ]: 2 of 4 PASS not_null_my_second_dbt_model_id..................................... [[32mPASS[0m in 1.11s]
13:28:35.582855 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:28:35.583238 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:28:35.583692 [info ] [Thread-1  ]: 3 of 4 START test unique_my_first_dbt_model_id.................................. [RUN]
13:28:35.584485 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:28:35.588319 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:28:35.588813 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:28:35.596003 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:28:35.610553 [debug] [Thread-1  ]: finished collecting timing info
13:28:35.610794 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:28:35.612645 [debug] [Thread-1  ]: Writing runtime SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:28:35.613273 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:28:35.613474 [debug] [Thread-1  ]: Using databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:28:35.613627 [debug] [Thread-1  ]: On test.test_dbx.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from ss_finance.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
13:28:35.613763 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:28:36.416541 [debug] [Thread-1  ]: SQL status: OK in 0.8 seconds
13:28:36.715843 [debug] [Thread-1  ]: finished collecting timing info
13:28:36.716302 [debug] [Thread-1  ]: On test.test_dbx.unique_my_first_dbt_model_id.16e066b321: ROLLBACK
13:28:36.716554 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:28:36.716768 [debug] [Thread-1  ]: On test.test_dbx.unique_my_first_dbt_model_id.16e066b321: Close
13:28:36.915528 [info ] [Thread-1  ]: 3 of 4 PASS unique_my_first_dbt_model_id........................................ [[32mPASS[0m in 1.33s]
13:28:36.916118 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:28:36.916399 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:28:36.916800 [info ] [Thread-1  ]: 4 of 4 START test unique_my_second_dbt_model_id................................. [RUN]
13:28:36.917455 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:28:36.920278 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:28:36.920575 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:28:36.925049 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:28:36.926088 [debug] [Thread-1  ]: finished collecting timing info
13:28:36.926314 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:28:36.928424 [debug] [Thread-1  ]: Writing runtime SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:28:36.929270 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:28:36.929488 [debug] [Thread-1  ]: Using databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:28:36.929669 [debug] [Thread-1  ]: On test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from ss_finance.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
13:28:36.929857 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:28:37.615950 [debug] [Thread-1  ]: SQL status: OK in 0.69 seconds
13:28:37.887527 [debug] [Thread-1  ]: finished collecting timing info
13:28:37.887878 [debug] [Thread-1  ]: On test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
13:28:37.888109 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:28:37.888321 [debug] [Thread-1  ]: On test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493: Close
13:28:38.062046 [info ] [Thread-1  ]: 4 of 4 PASS unique_my_second_dbt_model_id....................................... [[32mPASS[0m in 1.14s]
13:28:38.062699 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:28:38.064213 [debug] [MainThread]: Acquiring new databricks connection "master"
13:28:38.064503 [debug] [MainThread]: On master: ROLLBACK
13:28:38.064740 [debug] [MainThread]: Opening a new connection, currently in state init
13:28:38.173170 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:28:38.173595 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:28:38.173873 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:28:38.174163 [debug] [MainThread]: On master: ROLLBACK
13:28:38.174424 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:28:38.174678 [debug] [MainThread]: On master: Close
13:28:38.265067 [info ] [MainThread]: 
13:28:38.265672 [info ] [MainThread]: Finished running 4 tests in 6.67s.
13:28:38.266147 [debug] [MainThread]: Connection 'master' was properly closed.
13:28:38.266449 [debug] [MainThread]: Connection 'test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
13:28:38.279682 [info ] [MainThread]: 
13:28:38.280080 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
13:28:38.280437 [info ] [MainThread]: 
13:28:38.280800 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
13:28:38.281268 [error] [MainThread]:   Got 1 result, configured to fail if != 0
13:28:38.281607 [info ] [MainThread]: 
13:28:38.281974 [info ] [MainThread]:   compiled SQL at target/compiled/test_dbx/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
13:28:38.282335 [info ] [MainThread]: 
13:28:38.282680 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
13:28:38.283153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138c1fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113735730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138bad90>]}


============================== 2022-03-22 13:37:46.881480 | 5fca8675-0f51-4c3d-af78-6a7e5d7f9c07 ==============================
13:37:46.881480 [info ] [MainThread]: Running with dbt=1.0.3
13:37:46.882202 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['example'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
13:37:46.882589 [debug] [MainThread]: Tracking: tracking
13:37:46.900226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aec5280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aec5220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aec5a90>]}
13:37:46.995065 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
13:37:46.995633 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_second_dbt_model.sql
13:37:46.996010 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_first_dbt_model.sql
13:37:47.008269 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
13:37:47.037931 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
13:37:47.080104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5fca8675-0f51-4c3d-af78-6a7e5d7f9c07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b08afa0>]}
13:37:47.086979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5fca8675-0f51-4c3d-af78-6a7e5d7f9c07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b04ee50>]}
13:37:47.087302 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:37:47.088633 [info ] [MainThread]: 
13:37:47.089130 [debug] [MainThread]: Acquiring new databricks connection "master"
13:37:47.089957 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:37:47.099901 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:37:47.100320 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:37:47.100552 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:37:47.100731 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:37:48.548475 [debug] [ThreadPool]: SQL status: OK in 1.45 seconds
13:37:49.010634 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:37:49.010961 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:37:49.011223 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:37:49.206601 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:37:49.207219 [info ] [MainThread]: 
13:37:49.224867 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:37:49.225382 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:37:49.228081 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:37:49.228324 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:37:49.231522 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:37:49.232752 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.232994 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:37:49.233229 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.233673 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:37:49.234309 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:37:49.234716 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
13:37:49.234935 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
13:37:49.235202 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
13:37:49.238713 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
13:37:49.241117 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.241375 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
13:37:49.241588 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.242015 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:37:49.242257 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:37:49.242859 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:37:49.243120 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:37:49.243334 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:37:49.255127 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
13:37:49.256811 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.257124 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:37:49.257351 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.257808 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
13:37:49.258059 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:37:49.258424 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:37:49.258739 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:37:49.258958 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:37:49.266100 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
13:37:49.274526 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.274854 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:37:49.275086 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.275542 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
13:37:49.275793 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:37:49.276156 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:37:49.276402 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:37:49.276638 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:37:49.280856 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
13:37:49.281717 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.281939 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:37:49.282144 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.282520 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
13:37:49.282755 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:37:49.283193 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:37:49.283397 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:37:49.283586 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:37:49.287369 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
13:37:49.288243 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.288442 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:37:49.288630 [debug] [Thread-1  ]: finished collecting timing info
13:37:49.288982 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
13:37:49.289790 [debug] [MainThread]: Connection 'master' was properly closed.
13:37:49.289991 [debug] [MainThread]: Connection 'test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
13:37:49.296357 [info ] [MainThread]: Done.
13:37:49.296772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b19e790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b163cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b163d30>]}


============================== 2022-03-22 13:38:18.083260 | 1c6a089d-a9ee-4d8a-a3cc-7aa7662364d7 ==============================
13:38:18.083260 [info ] [MainThread]: Running with dbt=1.0.3
13:38:18.083891 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['example'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:38:18.084431 [debug] [MainThread]: Tracking: tracking
13:38:18.101615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ed7970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ed73d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ed7a00>]}
13:38:18.196845 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:38:18.197159 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:38:18.204445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c6a089d-a9ee-4d8a-a3cc-7aa7662364d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a02ff40>]}
13:38:18.226304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c6a089d-a9ee-4d8a-a3cc-7aa7662364d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109fad3a0>]}
13:38:18.227016 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:38:18.228922 [info ] [MainThread]: 
13:38:18.229632 [debug] [MainThread]: Acquiring new databricks connection "master"
13:38:18.230728 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
13:38:18.241674 [debug] [ThreadPool]: Using databricks connection "list_schemas"
13:38:18.241989 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
13:38:18.242194 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:38:18.665229 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
13:38:19.028810 [debug] [ThreadPool]: On list_schemas: Close
13:38:19.282664 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:38:19.292046 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:38:19.292293 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:38:19.292485 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:38:19.292662 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:38:19.981511 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
13:38:20.864822 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:38:20.865177 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:38:20.865408 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:38:21.107936 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:38:21.108341 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:38:21.108919 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:38:21.109400 [info ] [MainThread]: 
13:38:21.129250 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:38:21.129699 [info ] [Thread-1  ]: 1 of 2 START table model ss_finance.my_first_dbt_model.......................... [RUN]
13:38:21.130329 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:38:21.132959 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:38:21.133411 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:38:21.136824 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:38:21.137858 [debug] [Thread-1  ]: finished collecting timing info
13:38:21.138095 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:38:21.186201 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_first_dbt_model"
13:38:21.189258 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:38:21.189493 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_first_dbt_model"
13:38:21.189678 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:38:21.189851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:38:21.592308 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:38:21.592658 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: bronze_loan_stats; line 38 pos 4;
'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@580d5ea5, ss_finance.my_first_dbt_model, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model], true
+- 'Project [*]
   +- 'Filter ('loan_amnt >= 10000)
      +- 'UnresolvedRelation [bronze_loan_stats], [], false

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: bronze_loan_stats; line 38 pos 4;
'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@580d5ea5, ss_finance.my_first_dbt_model, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model], true
+- 'Project [*]
   +- 'Filter ('loan_amnt >= 10000)
      +- 'UnresolvedRelation [bronze_loan_stats], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

13:38:21.593105 [debug] [Thread-1  ]: finished collecting timing info
13:38:21.593359 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: ROLLBACK
13:38:21.593575 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:38:21.593782 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: Close
13:38:21.800456 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: Table or view not found: bronze_loan_stats; line 38 pos 4;
  'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@580d5ea5, ss_finance.my_first_dbt_model, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model], true
  +- 'Project [*]
     +- 'Filter ('loan_amnt >= 10000)
        +- 'UnresolvedRelation [bronze_loan_stats], [], false
13:38:21.801097 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c6a089d-a9ee-4d8a-a3cc-7aa7662364d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a127040>]}
13:38:21.801593 [error] [Thread-1  ]: 1 of 2 ERROR creating table model ss_finance.my_first_dbt_model................. [[31mERROR[0m in 0.67s]
13:38:21.802061 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:38:21.802831 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:38:21.803162 [info ] [Thread-1  ]: 2 of 2 SKIP relation ss_finance.my_second_dbt_model............................. [[33mSKIP[0m]
13:38:21.803610 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:38:21.804799 [debug] [MainThread]: Acquiring new databricks connection "master"
13:38:21.805052 [debug] [MainThread]: On master: ROLLBACK
13:38:21.805242 [debug] [MainThread]: Opening a new connection, currently in state init
13:38:21.944179 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:38:21.944603 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:38:21.944874 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:38:21.945156 [debug] [MainThread]: On master: ROLLBACK
13:38:21.945417 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:38:21.945670 [debug] [MainThread]: On master: Close
13:38:22.077413 [info ] [MainThread]: 
13:38:22.077942 [info ] [MainThread]: Finished running 2 table models in 3.85s.
13:38:22.078325 [debug] [MainThread]: Connection 'master' was properly closed.
13:38:22.078644 [debug] [MainThread]: Connection 'model.test_dbx.my_first_dbt_model' was properly closed.
13:38:22.088203 [info ] [MainThread]: 
13:38:22.088589 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
13:38:22.088935 [info ] [MainThread]: 
13:38:22.089233 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
13:38:22.089530 [error] [MainThread]:   Query execution failed.
13:38:22.089805 [error] [MainThread]:   Error message: org.apache.spark.sql.AnalysisException: Table or view not found: bronze_loan_stats; line 38 pos 4;
13:38:22.090074 [error] [MainThread]:   'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@580d5ea5, ss_finance.my_first_dbt_model, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model], true
13:38:22.090343 [error] [MainThread]:   +- 'Project [*]
13:38:22.090608 [error] [MainThread]:      +- 'Filter ('loan_amnt >= 10000)
13:38:22.090870 [error] [MainThread]:         +- 'UnresolvedRelation [bronze_loan_stats], [], false
13:38:22.091148 [info ] [MainThread]: 
13:38:22.091443 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
13:38:22.091840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0893d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0890d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a127fa0>]}


============================== 2022-03-22 13:39:23.787441 | a8ef7a7e-cb10-41e3-a1a7-54a085d8e932 ==============================
13:39:23.787441 [info ] [MainThread]: Running with dbt=1.0.3
13:39:23.788218 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['example'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:39:23.788490 [debug] [MainThread]: Tracking: tracking
13:39:23.806230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae82130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae82f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae82ee0>]}
13:39:23.897459 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
13:39:23.898039 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/example/my_first_dbt_model.sql
13:39:23.910944 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
13:39:23.983284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a8ef7a7e-cb10-41e3-a1a7-54a085d8e932', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0490d0>]}
13:39:23.990840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a8ef7a7e-cb10-41e3-a1a7-54a085d8e932', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0496d0>]}
13:39:23.991209 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:39:23.992549 [info ] [MainThread]: 
13:39:23.993085 [debug] [MainThread]: Acquiring new databricks connection "master"
13:39:23.994109 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
13:39:24.005006 [debug] [ThreadPool]: Using databricks connection "list_schemas"
13:39:24.005304 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
13:39:24.005508 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:39:24.445977 [debug] [ThreadPool]: SQL status: OK in 0.44 seconds
13:39:24.833381 [debug] [ThreadPool]: On list_schemas: Close
13:39:25.101953 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:39:25.110858 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:39:25.111154 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:39:25.111352 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:39:25.111534 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:39:26.246944 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
13:39:27.631037 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:39:27.631389 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:39:27.631581 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:39:27.891004 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:39:27.891334 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:39:27.891824 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:39:27.892251 [info ] [MainThread]: 
13:39:27.897904 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:39:27.898338 [info ] [Thread-1  ]: 1 of 2 START table model ss_finance.my_first_dbt_model.......................... [RUN]
13:39:27.898909 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:39:27.901955 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:39:27.902288 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:39:27.905366 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:39:27.906446 [debug] [Thread-1  ]: finished collecting timing info
13:39:27.906717 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:39:27.956468 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_first_dbt_model"
13:39:27.957449 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:39:27.957661 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_first_dbt_model"
13:39:27.957922 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    ss_finance.bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:39:27.958148 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:39:32.374349 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    ss_finance.bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:39:32.374789 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:161)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:112)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:150)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:364)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:149)
	at org.apache.spark.sql.hive.HiveExternalCatalog.alterTable(HiveExternalCatalog.scala:661)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.alterTable(ExternalCatalogWithListener.scala:126)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.alterTable(SessionCatalog.scala:792)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.alterTable(ManagedCatalogSessionCatalog.scala:475)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:403)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:235)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:122)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:53)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:120)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:53)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:203)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:602)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:515)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:500)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:495)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:193)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:225)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:41)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:41)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:47)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:853)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12.alterTable(HiveShim.scala:445)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$alterTable$1(HiveClientImpl.scala:646)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:348)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:287)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:330)
	at org.apache.spark.sql.hive.client.HiveClientImpl.alterTable(HiveClientImpl.scala:635)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$alterTable$1(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$alterTable$1$adapted(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:145)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.alterTable(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.HiveClient.alterTable(HiveClient.scala:107)
	at org.apache.spark.sql.hive.client.HiveClient.alterTable$(HiveClient.scala:106)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.alterTable(PoolingHiveClient.scala:43)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$alterTable$1(HiveExternalCatalog.scala:748)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:151)
	... 85 more
Caused by: MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) )
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:4613)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2820)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy50.alter_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:267)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:262)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy51.alter_table(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:402)
	... 109 more
Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.put(JoinMapStore.java:318)
	at org.datanucleus.store.types.wrappers.backed.HashMap.put(HashMap.java:585)
	at org.datanucleus.store.types.SCOUtils.updateMapWithMapKeysValues(SCOUtils.java:995)
	at org.datanucleus.store.types.wrappers.backed.HashMap.initialise(HashMap.java:124)
	at org.datanucleus.store.types.wrappers.backed.HashMap.initialise(HashMap.java:43)
	at org.datanucleus.store.types.SCOUtils.wrapAndReplaceSCOField(SCOUtils.java:176)
	at org.datanucleus.state.StateManagerImpl.setObjectField(StateManagerImpl.java:1804)
	at org.apache.hadoop.hive.metastore.model.MTable.dnSetparameters(MTable.java)
	at org.apache.hadoop.hive.metastore.model.MTable.setParameters(MTable.java:126)
	at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2654)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy49.alterTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:207)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2800)
	... 124 more
Caused by: org.datanucleus.store.rdbms.exceptions.MappedDatastoreException: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1074)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.put(JoinMapStore.java:308)
	... 141 more
Caused by: java.sql.SQLIntegrityConstraintViolationException: (conn=64166) Duplicate entry '900-totalSize' for key 'PRIMARY'
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.get(ExceptionMapper.java:171)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.getException(ExceptionMapper.java:110)
	at org.mariadb.jdbc.MariaDbStatement.executeExceptionEpilogue(MariaDbStatement.java:228)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternal(MariaDbPreparedStatementClient.java:216)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.execute(MariaDbPreparedStatementClient.java:150)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeUpdate(MariaDbPreparedStatementClient.java:183)
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:393)
	at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:431)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1065)
	... 142 more
Caused by: java.sql.SQLException: Duplicate entry '900-totalSize' for key 'PRIMARY'
Query is: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) , parameters ['0',900,'totalSize']
	at org.mariadb.jdbc.internal.util.LogQueryTool.exceptionWithQuery(LogQueryTool.java:153)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:255)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternal(MariaDbPreparedStatementClient.java:209)
	... 149 more

13:39:32.375366 [debug] [Thread-1  ]: finished collecting timing info
13:39:32.375698 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: ROLLBACK
13:39:32.375984 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:39:32.376253 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: Close
13:39:32.572973 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
13:39:32.573608 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a8ef7a7e-cb10-41e3-a1a7-54a085d8e932', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b144190>]}
13:39:32.574218 [error] [Thread-1  ]: 1 of 2 ERROR creating table model ss_finance.my_first_dbt_model................. [[31mERROR[0m in 4.67s]
13:39:32.574845 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:39:32.575635 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:39:32.575935 [info ] [Thread-1  ]: 2 of 2 SKIP relation ss_finance.my_second_dbt_model............................. [[33mSKIP[0m]
13:39:32.576546 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:39:32.577820 [debug] [MainThread]: Acquiring new databricks connection "master"
13:39:32.578126 [debug] [MainThread]: On master: ROLLBACK
13:39:32.578354 [debug] [MainThread]: Opening a new connection, currently in state init
13:39:32.687779 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:39:32.688211 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:39:32.688492 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:39:32.688780 [debug] [MainThread]: On master: ROLLBACK
13:39:32.689043 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:39:32.689299 [debug] [MainThread]: On master: Close
13:39:32.779710 [info ] [MainThread]: 
13:39:32.780338 [info ] [MainThread]: Finished running 2 table models in 8.79s.
13:39:32.780841 [debug] [MainThread]: Connection 'master' was properly closed.
13:39:32.781140 [debug] [MainThread]: Connection 'model.test_dbx.my_first_dbt_model' was properly closed.
13:39:32.791126 [info ] [MainThread]: 
13:39:32.791545 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
13:39:32.791908 [info ] [MainThread]: 
13:39:32.792300 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
13:39:32.792651 [error] [MainThread]:   Query execution failed.
13:39:32.792957 [error] [MainThread]:   Error message: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
13:39:32.793281 [info ] [MainThread]: 
13:39:32.793641 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
13:39:32.794212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b118760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b138520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b144130>]}


============================== 2022-03-22 13:40:29.294324 | 18a83b25-e27f-43ce-a1cd-b2e0ebba49d6 ==============================
13:40:29.294324 [info ] [MainThread]: Running with dbt=1.0.3
13:40:29.295026 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['example'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:40:29.295327 [debug] [MainThread]: Tracking: tracking
13:40:29.315494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111754160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117546a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111754220>]}
13:40:29.410542 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:40:29.410859 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:40:29.417542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18a83b25-e27f-43ce-a1cd-b2e0ebba49d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118c2fa0>]}
13:40:29.431623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18a83b25-e27f-43ce-a1cd-b2e0ebba49d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111832310>]}
13:40:29.431990 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:40:29.433389 [info ] [MainThread]: 
13:40:29.433942 [debug] [MainThread]: Acquiring new databricks connection "master"
13:40:29.434970 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
13:40:29.446333 [debug] [ThreadPool]: Using databricks connection "list_schemas"
13:40:29.446631 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
13:40:29.446854 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:40:29.987981 [debug] [ThreadPool]: SQL status: OK in 0.54 seconds
13:40:30.343993 [debug] [ThreadPool]: On list_schemas: Close
13:40:30.522588 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:40:30.532186 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:40:30.532473 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:40:30.532674 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:40:30.532860 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:40:31.179012 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
13:40:31.477834 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:40:31.478274 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:40:31.478534 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:40:31.655716 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:40:31.656113 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:40:31.656590 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:40:31.656980 [info ] [MainThread]: 
13:40:31.661890 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:40:31.662323 [info ] [Thread-1  ]: 1 of 2 START table model ss_finance.my_first_dbt_model.......................... [RUN]
13:40:31.663007 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:40:31.665708 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:40:31.666200 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:40:31.669243 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:40:31.678907 [debug] [Thread-1  ]: finished collecting timing info
13:40:31.679211 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:40:31.723773 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_first_dbt_model"
13:40:31.728928 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:40:31.729190 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_first_dbt_model"
13:40:31.729381 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    ss_finance.bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:40:31.729555 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:40:35.881968 [debug] [Thread-1  ]: SQL status: OK in 4.15 seconds
13:40:35.889399 [debug] [Thread-1  ]: finished collecting timing info
13:40:35.889646 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: ROLLBACK
13:40:35.889839 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:40:35.890022 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: Close
13:40:36.084112 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a83b25-e27f-43ce-a1cd-b2e0ebba49d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118c2370>]}
13:40:36.084845 [info ] [Thread-1  ]: 1 of 2 OK created table model ss_finance.my_first_dbt_model..................... [[32mOK[0m in 4.42s]
13:40:36.085443 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:40:36.086297 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:40:36.086753 [info ] [Thread-1  ]: 2 of 2 START table model ss_finance.my_second_dbt_model......................... [RUN]
13:40:36.087370 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
13:40:36.090381 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
13:40:36.090667 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
13:40:36.094525 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
13:40:36.095772 [debug] [Thread-1  ]: finished collecting timing info
13:40:36.096017 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
13:40:36.098952 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_second_dbt_model"
13:40:36.110372 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:40:36.110650 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_second_dbt_model"
13:40:36.110851 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_second_dbt_model"} */

      create or replace table ss_finance.my_second_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_second_dbt_model'
    
    as
      -- Use the `ref` function to select from other models



select *
from ss_finance.my_first_dbt_model
where emp_title = 'Engineer'
13:40:36.111040 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:40:39.524957 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_second_dbt_model"} */

      create or replace table ss_finance.my_second_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_second_dbt_model'
    
    as
      -- Use the `ref` function to select from other models



select *
from ss_finance.my_first_dbt_model
where emp_title = 'Engineer'
13:40:39.525412 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:161)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:112)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:150)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:364)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:149)
	at org.apache.spark.sql.hive.HiveExternalCatalog.alterTable(HiveExternalCatalog.scala:661)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.alterTable(ExternalCatalogWithListener.scala:126)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.alterTable(SessionCatalog.scala:792)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.alterTable(ManagedCatalogSessionCatalog.scala:475)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:403)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:235)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:122)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:53)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:120)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:53)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:203)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:602)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:515)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:500)
	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:495)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:193)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:225)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:41)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:41)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:47)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:235)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:853)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_12.alterTable(HiveShim.scala:445)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$alterTable$1(HiveClientImpl.scala:646)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:348)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:287)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:330)
	at org.apache.spark.sql.hive.client.HiveClientImpl.alterTable(HiveClientImpl.scala:635)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$alterTable$1(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$alterTable$1$adapted(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:145)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.alterTable(PoolingHiveClient.scala:342)
	at org.apache.spark.sql.hive.client.HiveClient.alterTable(HiveClient.scala:107)
	at org.apache.spark.sql.hive.client.HiveClient.alterTable$(HiveClient.scala:106)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.alterTable(PoolingHiveClient.scala:43)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$alterTable$1(HiveExternalCatalog.scala:748)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:151)
	... 85 more
Caused by: MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) )
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:4613)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2820)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy50.alter_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:267)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:262)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy51.alter_table(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:402)
	... 109 more
Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.put(JoinMapStore.java:318)
	at org.datanucleus.store.types.wrappers.backed.HashMap.put(HashMap.java:585)
	at org.datanucleus.store.types.SCOUtils.updateMapWithMapKeysValues(SCOUtils.java:995)
	at org.datanucleus.store.types.wrappers.backed.HashMap.initialise(HashMap.java:124)
	at org.datanucleus.store.types.wrappers.backed.HashMap.initialise(HashMap.java:43)
	at org.datanucleus.store.types.SCOUtils.wrapAndReplaceSCOField(SCOUtils.java:176)
	at org.datanucleus.state.StateManagerImpl.setObjectField(StateManagerImpl.java:1804)
	at org.apache.hadoop.hive.metastore.model.MTable.dnSetparameters(MTable.java)
	at org.apache.hadoop.hive.metastore.model.MTable.setParameters(MTable.java:126)
	at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2654)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy49.alterTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:207)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2800)
	... 124 more
Caused by: org.datanucleus.store.rdbms.exceptions.MappedDatastoreException: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1074)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.put(JoinMapStore.java:308)
	... 141 more
Caused by: java.sql.SQLTransactionRollbackException: (conn=64166) Deadlock found when trying to get lock; try restarting transaction
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.get(ExceptionMapper.java:179)
	at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.getException(ExceptionMapper.java:110)
	at org.mariadb.jdbc.MariaDbStatement.executeExceptionEpilogue(MariaDbStatement.java:228)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternal(MariaDbPreparedStatementClient.java:216)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.execute(MariaDbPreparedStatementClient.java:150)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeUpdate(MariaDbPreparedStatementClient.java:183)
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:393)
	at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:431)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1065)
	... 142 more
Caused by: java.sql.SQLException: Deadlock found when trying to get lock; try restarting transaction
Query is: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) , parameters ['-1',901,'numRows']
	at org.mariadb.jdbc.internal.util.LogQueryTool.exceptionWithQuery(LogQueryTool.java:153)
	at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:255)
	at org.mariadb.jdbc.MariaDbPreparedStatementClient.executeInternal(MariaDbPreparedStatementClient.java:209)
	... 149 more

13:40:39.526000 [debug] [Thread-1  ]: finished collecting timing info
13:40:39.526323 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: ROLLBACK
13:40:39.526593 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:40:39.526851 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: Close
13:40:39.698462 [debug] [Thread-1  ]: Runtime Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
13:40:39.699045 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a83b25-e27f-43ce-a1cd-b2e0ebba49d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119fdb20>]}
13:40:39.699615 [error] [Thread-1  ]: 2 of 2 ERROR creating table model ss_finance.my_second_dbt_model................ [[31mERROR[0m in 3.61s]
13:40:39.700201 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:40:39.701594 [debug] [MainThread]: Acquiring new databricks connection "master"
13:40:39.701868 [debug] [MainThread]: On master: ROLLBACK
13:40:39.702085 [debug] [MainThread]: Opening a new connection, currently in state init
13:40:39.817425 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:40:39.817834 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:40:39.818106 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:40:39.818394 [debug] [MainThread]: On master: ROLLBACK
13:40:39.818652 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:40:39.818909 [debug] [MainThread]: On master: Close
13:40:39.911257 [info ] [MainThread]: 
13:40:39.911842 [info ] [MainThread]: Finished running 2 table models in 10.48s.
13:40:39.912319 [debug] [MainThread]: Connection 'master' was properly closed.
13:40:39.912587 [debug] [MainThread]: Connection 'model.test_dbx.my_second_dbt_model' was properly closed.
13:40:39.931725 [info ] [MainThread]: 
13:40:39.932183 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
13:40:39.932723 [info ] [MainThread]: 
13:40:39.933094 [error] [MainThread]: [33mRuntime Error in model my_second_dbt_model (models/example/my_second_dbt_model.sql)[0m
13:40:39.933468 [error] [MainThread]:   Query execution failed.
13:40:39.933911 [error] [MainThread]:   Error message: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table.
13:40:39.934275 [info ] [MainThread]: 
13:40:39.934626 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
13:40:39.935075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118c2d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118c2b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118ab040>]}


============================== 2022-03-22 13:41:24.992388 | 5b8c5f6d-5c33-45af-8f02-95eea8dca4ea ==============================
13:41:24.992388 [info ] [MainThread]: Running with dbt=1.0.3
13:41:24.993453 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['example'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
13:41:24.993783 [debug] [MainThread]: Tracking: tracking
13:41:25.010465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10759d970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10759d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10759da00>]}
13:41:25.092918 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
13:41:25.093208 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
13:41:25.099780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b8c5f6d-5c33-45af-8f02-95eea8dca4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f5f40>]}
13:41:25.106778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b8c5f6d-5c33-45af-8f02-95eea8dca4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076713a0>]}
13:41:25.107100 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
13:41:25.108379 [info ] [MainThread]: 
13:41:25.108889 [debug] [MainThread]: Acquiring new databricks connection "master"
13:41:25.109892 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
13:41:25.119551 [debug] [ThreadPool]: Using databricks connection "list_schemas"
13:41:25.119813 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
13:41:25.119989 [debug] [ThreadPool]: Opening a new connection, currently in state init
13:41:25.583002 [debug] [ThreadPool]: SQL status: OK in 0.46 seconds
13:41:26.279134 [debug] [ThreadPool]: On list_schemas: Close
13:41:26.528274 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
13:41:26.537926 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
13:41:26.538241 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
13:41:26.538451 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
13:41:26.538648 [debug] [ThreadPool]: Opening a new connection, currently in state closed
13:41:27.159454 [debug] [ThreadPool]: SQL status: OK in 0.62 seconds
13:41:27.518851 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
13:41:27.519261 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
13:41:27.519516 [debug] [ThreadPool]: On list_None_ss_finance: Close
13:41:27.787074 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:41:27.787479 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:41:27.788039 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
13:41:27.788538 [info ] [MainThread]: 
13:41:27.793603 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
13:41:27.794106 [info ] [Thread-1  ]: 1 of 2 START table model ss_finance.my_first_dbt_model.......................... [RUN]
13:41:27.794903 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
13:41:27.797684 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
13:41:27.798216 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
13:41:27.801285 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
13:41:27.802371 [debug] [Thread-1  ]: finished collecting timing info
13:41:27.802601 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
13:41:27.848182 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_first_dbt_model"
13:41:27.849298 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:41:27.849508 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_first_dbt_model"
13:41:27.849681 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_first_dbt_model"} */

      create or replace table ss_finance.my_first_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_first_dbt_model'
    
    as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



-- with source_data as (

--     select 1 as id
--     union all
--     select null as id

-- )

-- select *
-- from source_data

SELECT
    *
FROM
    ss_finance.bronze_loan_stats
WHERE
    loan_amnt >= 10000

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
13:41:27.849851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:41:32.217545 [debug] [Thread-1  ]: SQL status: OK in 4.37 seconds
13:41:32.224173 [debug] [Thread-1  ]: finished collecting timing info
13:41:32.224488 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: ROLLBACK
13:41:32.224692 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:41:32.224879 [debug] [Thread-1  ]: On model.test_dbx.my_first_dbt_model: Close
13:41:32.425526 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b8c5f6d-5c33-45af-8f02-95eea8dca4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10774f550>]}
13:41:32.426098 [info ] [Thread-1  ]: 1 of 2 OK created table model ss_finance.my_first_dbt_model..................... [[32mOK[0m in 4.63s]
13:41:32.426585 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
13:41:32.427362 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
13:41:32.427684 [info ] [Thread-1  ]: 2 of 2 START table model ss_finance.my_second_dbt_model......................... [RUN]
13:41:32.428211 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
13:41:32.430868 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
13:41:32.431104 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
13:41:32.434202 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
13:41:32.435313 [debug] [Thread-1  ]: finished collecting timing info
13:41:32.435543 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
13:41:32.438267 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.my_second_dbt_model"
13:41:32.439201 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
13:41:32.439430 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.my_second_dbt_model"
13:41:32.439624 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.my_second_dbt_model"} */

      create or replace table ss_finance.my_second_dbt_model
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/my_second_dbt_model'
    
    as
      -- Use the `ref` function to select from other models



select *
from ss_finance.my_first_dbt_model
where emp_title = 'Engineer'
13:41:32.439807 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
13:41:36.098972 [debug] [Thread-1  ]: SQL status: OK in 3.66 seconds
13:41:36.101463 [debug] [Thread-1  ]: finished collecting timing info
13:41:36.101751 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: ROLLBACK
13:41:36.101985 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
13:41:36.102208 [debug] [Thread-1  ]: On model.test_dbx.my_second_dbt_model: Close
13:41:36.282502 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b8c5f6d-5c33-45af-8f02-95eea8dca4ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d85d6a0>]}
13:41:36.283177 [info ] [Thread-1  ]: 2 of 2 OK created table model ss_finance.my_second_dbt_model.................... [[32mOK[0m in 3.85s]
13:41:36.283745 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
13:41:36.285161 [debug] [MainThread]: Acquiring new databricks connection "master"
13:41:36.285446 [debug] [MainThread]: On master: ROLLBACK
13:41:36.285666 [debug] [MainThread]: Opening a new connection, currently in state init
13:41:36.390570 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:41:36.391003 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
13:41:36.391286 [debug] [MainThread]: Spark adapter: NotImplemented: commit
13:41:36.391581 [debug] [MainThread]: On master: ROLLBACK
13:41:36.391850 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
13:41:36.392113 [debug] [MainThread]: On master: Close
13:41:36.492599 [info ] [MainThread]: 
13:41:36.493214 [info ] [MainThread]: Finished running 2 table models in 11.38s.
13:41:36.493692 [debug] [MainThread]: Connection 'master' was properly closed.
13:41:36.493973 [debug] [MainThread]: Connection 'model.test_dbx.my_second_dbt_model' was properly closed.
13:41:36.506512 [info ] [MainThread]: 
13:41:36.506905 [info ] [MainThread]: [32mCompleted successfully[0m
13:41:36.507271 [info ] [MainThread]: 
13:41:36.507618 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
13:41:36.508034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077a86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107671700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076f5fd0>]}


============================== 2022-03-22 14:11:36.624915 | c431eb42-3c2a-46bb-8f98-b3758cfa9720 ==============================
14:11:36.624915 [info ] [MainThread]: Running with dbt=1.0.3
14:11:36.625995 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, compile=True, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, cls=<class 'dbt.task.generate.GenerateTask'>, which='generate', rpc_method='docs.generate')
14:11:36.626382 [debug] [MainThread]: Tracking: tracking
14:11:36.670023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abf9d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abf9b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abf9e20>]}
14:11:36.832735 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
14:11:36.832992 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
14:11:36.860473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c431eb42-3c2a-46bb-8f98-b3758cfa9720', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad61fd0>]}
14:11:36.931651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c431eb42-3c2a-46bb-8f98-b3758cfa9720', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acd7340>]}
14:11:36.932126 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 212 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
14:11:36.934132 [info ] [MainThread]: 
14:11:36.934831 [debug] [MainThread]: Acquiring new databricks connection "master"
14:11:36.935745 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
14:11:36.946724 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
14:11:36.947025 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
14:11:36.947225 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
14:11:36.947415 [debug] [ThreadPool]: Opening a new connection, currently in state init
14:11:38.519212 [debug] [ThreadPool]: SQL status: OK in 1.57 seconds
14:11:39.016447 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
14:11:39.016766 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
14:11:39.017006 [debug] [ThreadPool]: On list_None_ss_finance: Close
14:11:39.186658 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
14:11:39.187273 [info ] [MainThread]: 
14:11:39.208078 [debug] [Thread-1  ]: Began running node model.test_dbx.my_first_dbt_model
14:11:39.208577 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_first_dbt_model"
14:11:39.211044 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_first_dbt_model
14:11:39.211286 [debug] [Thread-1  ]: Compiling model.test_dbx.my_first_dbt_model
14:11:39.214742 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_first_dbt_model"
14:11:39.218220 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.218556 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_first_dbt_model
14:11:39.218840 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.219319 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_first_dbt_model
14:11:39.220085 [debug] [Thread-1  ]: Began running node model.test_dbx.my_second_dbt_model
14:11:39.220625 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.my_second_dbt_model"
14:11:39.220879 [debug] [Thread-1  ]: Began compiling node model.test_dbx.my_second_dbt_model
14:11:39.221123 [debug] [Thread-1  ]: Compiling model.test_dbx.my_second_dbt_model
14:11:39.224259 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.my_second_dbt_model"
14:11:39.232312 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.232643 [debug] [Thread-1  ]: Began executing node model.test_dbx.my_second_dbt_model
14:11:39.232929 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.233397 [debug] [Thread-1  ]: Finished running node model.test_dbx.my_second_dbt_model
14:11:39.233645 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
14:11:39.234040 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
14:11:39.234376 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
14:11:39.234737 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
14:11:39.246010 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710"
14:11:39.256214 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.256566 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
14:11:39.256821 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.257343 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_first_dbt_model_id.5fb22c2710
14:11:39.257635 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
14:11:39.258077 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
14:11:39.258424 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
14:11:39.258685 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_first_dbt_model_id.16e066b321
14:11:39.265556 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_first_dbt_model_id.16e066b321"
14:11:39.272886 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.273155 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
14:11:39.273355 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.273749 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_first_dbt_model_id.16e066b321
14:11:39.274027 [debug] [Thread-1  ]: Began running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
14:11:39.274440 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
14:11:39.274627 [debug] [Thread-1  ]: Began compiling node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
14:11:39.274808 [debug] [Thread-1  ]: Compiling test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
14:11:39.278734 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.not_null_my_second_dbt_model_id.151b76d778"
14:11:39.281297 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.281499 [debug] [Thread-1  ]: Began executing node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
14:11:39.281688 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.282049 [debug] [Thread-1  ]: Finished running node test.test_dbx.not_null_my_second_dbt_model_id.151b76d778
14:11:39.282266 [debug] [Thread-1  ]: Began running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
14:11:39.282674 [debug] [Thread-1  ]: Acquiring new databricks connection "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
14:11:39.282875 [debug] [Thread-1  ]: Began compiling node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
14:11:39.283054 [debug] [Thread-1  ]: Compiling test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
14:11:39.286714 [debug] [Thread-1  ]: Writing injected SQL for node "test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493"
14:11:39.307091 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.307429 [debug] [Thread-1  ]: Began executing node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
14:11:39.307684 [debug] [Thread-1  ]: finished collecting timing info
14:11:39.308183 [debug] [Thread-1  ]: Finished running node test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493
14:11:39.309235 [debug] [MainThread]: Connection 'master' was properly closed.
14:11:39.309497 [debug] [MainThread]: Connection 'test.test_dbx.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
14:11:39.323045 [info ] [MainThread]: Done.
14:11:40.308096 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
14:11:40.308459 [info ] [MainThread]: Building catalog
14:11:40.309811 [debug] [ThreadPool]: Acquiring new databricks connection "ss_finance"
14:11:40.310148 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation ss_finance.bronze_loan_stats
14:11:40.311837 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation ss_finance.gold_loan_stats
14:11:40.312109 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation ss_finance.my_first_dbt_model
14:11:40.313550 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation ss_finance.my_second_dbt_model
14:11:40.314924 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation ss_finance.silver_loan_stats
14:11:40.379896 [info ] [MainThread]: Catalog written to /Users/ananda.dwi/Documents/projects/test_dbx/target/catalog.json
14:11:40.380523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abf9d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae1f5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae1f4f0>]}
14:11:41.596835 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
14:11:41.597174 [debug] [MainThread]: Connection 'ss_finance' was properly closed.


============================== 2022-03-22 14:11:58.342289 | 774f5e9f-2f85-44c3-abb4-5e1d1daa491e ==============================
14:11:58.342289 [info ] [MainThread]: Running with dbt=1.0.3
14:11:58.343491 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, port=8080, open_browser=True, defer=None, state=None, cls=<class 'dbt.task.serve.ServeTask'>, which='serve', rpc_method=None)
14:11:58.343913 [debug] [MainThread]: Tracking: tracking
14:11:58.385241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b7b0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b7ba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b7b670>]}
14:11:59.210907 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
14:11:59.211501 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
14:11:59.211846 [info ] [MainThread]: 
14:11:59.212185 [info ] [MainThread]: 
14:11:59.212587 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-03-22 14:39:10.385599 | a4aae632-07f9-4560-bc76-90eb5834d4c3 ==============================
14:39:10.385599 [info ] [MainThread]: Running with dbt=1.0.3
14:39:10.386059 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, defer=None, state=None, cls=<class 'dbt.task.deps.DepsTask'>, which='deps', rpc_method='deps')
14:39:10.386255 [debug] [MainThread]: Tracking: tracking
14:39:10.409453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073ad7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073ad910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073ad940>]}
14:39:10.432902 [debug] [MainThread]: Set downloads directory='/var/folders/s4/8_vd44gd01569g443f_jqg1h0000gn/T/dbt-downloads-6_l2zj3e'
14:39:10.433763 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
14:39:11.279588 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
14:39:11.280068 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json
14:39:11.738915 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json 200
14:39:11.743015 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json
14:39:12.285426 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json 200
14:39:12.286119 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
14:39:12.749250 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
14:39:12.779131 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json
14:39:13.248609 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json 200
14:39:13.249044 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json
14:39:13.712253 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json 200
14:39:13.716351 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
14:39:14.185700 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
14:39:14.214331 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json
14:39:14.669957 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json 200
14:39:14.670426 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json
14:39:15.139617 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json 200
14:39:15.140104 [info ] [MainThread]: Installing Datavault-UK/dbtvault
14:39:15.754184 [info ] [MainThread]:   Installed from version 0.8.2
14:39:15.754936 [info ] [MainThread]:   Up to date!
14:39:15.755599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'a4aae632-07f9-4560-bc76-90eb5834d4c3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aded00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ade730>]}
14:39:15.756041 [info ] [MainThread]: Installing dbt-labs/dbt_utils
14:39:17.304245 [info ] [MainThread]:   Installed from version 0.8.2
14:39:17.304848 [info ] [MainThread]:   Up to date!
14:39:17.305394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'a4aae632-07f9-4560-bc76-90eb5834d4c3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106adea90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc5b20>]}
14:39:17.308651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10680b550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ade220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106adef70>]}


============================== 2022-03-22 23:20:31.228274 | 2ae3ad9f-9142-496f-9514-4b79f8a0a7e5 ==============================
23:20:31.228274 [info ] [MainThread]: Running with dbt=1.0.3
23:20:31.228924 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, defer=None, state=None, cls=<class 'dbt.task.deps.DepsTask'>, which='deps', rpc_method='deps')
23:20:31.229165 [debug] [MainThread]: Tracking: tracking
23:20:31.289393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083978e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10838f640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10838f4f0>]}
23:20:31.293716 [debug] [MainThread]: Set downloads directory='/var/folders/s4/8_vd44gd01569g443f_jqg1h0000gn/T/dbt-downloads-wtmfqjsh'
23:20:31.294781 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
23:20:31.665195 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
23:20:31.665616 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json
23:20:32.092060 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json 200
23:20:32.096005 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json
23:20:32.519170 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json 200
23:20:32.519724 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
23:20:33.199935 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
23:20:33.205886 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date/0.5.4.json
23:20:34.528018 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date/0.5.4.json 200
23:20:34.529077 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
23:20:34.617877 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
23:20:34.644054 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json
23:20:34.744223 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json 200
23:20:34.744736 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json
23:20:34.835793 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault.json 200
23:20:34.839724 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
23:20:34.935792 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
23:20:34.940753 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
23:20:35.040665 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
23:20:35.070301 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json
23:20:35.170321 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/Datavault-UK/dbtvault/0.8.2.json 200
23:20:35.170768 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date/0.5.4.json
23:20:35.266835 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date/0.5.4.json 200
23:20:35.267294 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json
23:20:35.365373 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.2.json 200
23:20:35.365755 [info ] [MainThread]: Installing Datavault-UK/dbtvault
23:20:36.533963 [info ] [MainThread]:   Installed from version 0.8.2
23:20:36.534352 [info ] [MainThread]:   Up to date!
23:20:36.534691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '2ae3ad9f-9142-496f-9514-4b79f8a0a7e5', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108382d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083820a0>]}
23:20:36.535019 [info ] [MainThread]: Installing calogica/dbt_date
23:20:36.916942 [info ] [MainThread]:   Installed from version 0.5.4
23:20:36.917322 [info ] [MainThread]:   Up to date!
23:20:36.917656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '2ae3ad9f-9142-496f-9514-4b79f8a0a7e5', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108382790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108382760>]}
23:20:36.918035 [info ] [MainThread]: Installing dbt-labs/dbt_utils
23:20:38.148522 [info ] [MainThread]:   Installed from version 0.8.2
23:20:38.149187 [info ] [MainThread]:   Up to date!
23:20:38.149766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '2ae3ad9f-9142-496f-9514-4b79f8a0a7e5', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108382bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108382520>]}
23:20:38.162059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b3bf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108343130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108414f40>]}


============================== 2022-03-22 23:26:23.742565 | 1bdd4797-07b6-4150-9280-f30d017edf9c ==============================
23:26:23.742565 [info ] [MainThread]: Running with dbt=1.0.3
23:26:23.743310 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:26:23.743615 [debug] [MainThread]: Tracking: tracking
23:26:23.794258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b989dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9a3310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9a37c0>]}
23:26:23.927820 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
23:26:23.928272 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:26:23.928597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1bdd4797-07b6-4150-9280-f30d017edf9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9a34c0>]}
23:26:25.784022 [debug] [MainThread]: Parsing macros/adapters.sql
23:26:25.812886 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:26:25.817131 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:26:25.817786 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:26:25.820997 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:26:25.839359 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:26:25.844118 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:26:25.847908 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:26:25.853209 [debug] [MainThread]: Parsing macros/adapters.sql
23:26:25.889961 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:26:25.898900 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:26:25.899481 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:26:25.902134 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:26:25.925602 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:26:25.930329 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:26:25.936267 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:26:25.941475 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:26:25.945009 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:26:25.947104 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:26:25.948594 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:26:25.964215 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:26:25.974790 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:26:25.985701 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:26:25.989760 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:26:25.991378 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:26:25.992987 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:26:25.996904 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:26:26.007269 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:26:26.008608 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:26:26.017846 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:26:26.032751 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:26:26.039864 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:26:26.043588 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:26:26.050143 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:26:26.051331 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:26:26.053738 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:26:26.055818 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:26:26.061542 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:26:26.077441 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:26:26.078896 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:26:26.081346 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:26:26.082827 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:26:26.083625 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:26:26.084145 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:26:26.084790 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:26:26.086026 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:26:26.089948 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:26:26.097807 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:26:26.099789 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:26:26.102943 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:26:26.111583 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:26:26.114262 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:26:26.118491 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:26:26.125347 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:26:26.134450 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:26:26.137255 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:26:26.138237 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:26:26.139432 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:26:26.140482 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:26:26.146233 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:26:26.147280 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:26:26.148660 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:26:26.151270 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:26:26.152326 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:26:26.154045 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:26:26.156308 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:26:26.165375 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:26:26.167057 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:26:26.168557 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:26:26.169879 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:26:26.171384 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:26:26.172659 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:26:26.174059 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:26:26.174924 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:26:26.177999 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:26:26.182765 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:26:26.184190 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:26:26.187350 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:26:26.189019 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:26:26.190435 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:26:26.192351 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:26:26.215193 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:26:26.216985 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:26:26.219830 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:26:26.221278 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:26:26.222312 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:26:26.223479 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:26:26.224511 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:26:26.225594 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:26:26.227136 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:26:26.228652 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:26:26.230827 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:26:26.232627 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:26:26.233760 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:26:26.236030 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:26:26.237926 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:26:26.239218 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:26:26.240356 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:26:26.243230 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:26:26.245028 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:26:26.246761 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:26:26.248717 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:26:26.251349 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:26:26.252696 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:26:26.255838 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:26:26.263927 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:26:26.267727 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:26:26.269348 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:26:26.272406 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:26:26.276285 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:26:26.279537 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:26:26.281117 [debug] [MainThread]: Parsing macros/sql/star.sql
23:26:26.285088 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:26:26.292523 [debug] [MainThread]: Parsing macros/sql/union.sql
23:26:26.302039 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:26:26.303269 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:26:26.306567 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:26:26.308038 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:26:26.309573 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:26:26.315337 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:26:26.320097 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:26:26.323853 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:26:26.326126 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:26:26.327302 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:26:26.332581 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:26:26.355471 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:26:26.360318 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:26:26.364133 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:26:26.365867 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:26:26.366472 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:26:26.367008 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:26:26.367681 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:26:26.368289 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:26:26.371358 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:26:26.373038 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:26:26.373809 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:26:26.376333 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:26:26.378665 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:26:26.379748 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:26:26.380304 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:26:26.380890 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:26:26.381585 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:26:26.382120 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:26:26.382661 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:26:26.384433 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:26:26.389536 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:26:26.390437 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:26:26.391728 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:26:26.392595 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:26:26.393569 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:26:26.394170 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:26:26.401051 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:26:26.403209 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:26:26.404110 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:26:26.406835 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:26:26.407518 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:26:26.409187 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:26:26.413493 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:26:26.414396 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:26:26.416863 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:26:26.419091 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:26:26.419722 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:26:26.420353 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:26:26.422006 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:26:26.423731 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:26:26.424525 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:26:26.440221 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:26:26.441125 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:26:26.449548 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:26:26.460349 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:26:26.473313 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:26:26.484309 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:26:26.493921 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:26:26.517365 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:26:26.530400 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:26:26.536723 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:26:26.547449 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:26:26.559742 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:26:26.574501 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:26:26.588154 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:26:26.628097 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:26:26.628961 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:26:26.644638 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:26:26.645582 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:26:26.654250 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:26:26.666714 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:26:26.679037 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:26:26.691087 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:26:26.694555 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:26:26.695688 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:26:26.705748 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:26:26.714478 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:26:26.720509 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:26:26.722884 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:26:26.735753 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:26:26.739112 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:26:26.741184 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:26:26.746087 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:26:26.748961 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:26:26.750414 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:26:26.752505 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:26:26.753114 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:26:26.757341 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:26:26.766011 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:26:26.781915 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:26:26.787431 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:26:26.788515 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:26:26.813040 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:26:26.815409 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:26:26.820523 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:26:26.826583 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:26:26.829808 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:26:26.833739 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:26:26.840714 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:26:26.851479 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:26:26.855749 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:26:26.861076 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:26:26.865883 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:26:26.866947 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:26:26.868537 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:26:26.889234 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:26:26.894679 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:26:26.898092 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:26:27.728031 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:26:27.772609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba13280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc04fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc1eeb0>]}


============================== 2022-03-22 23:34:36.431731 | 91dd01d5-9e4a-4ae6-90a9-766311e45ab9 ==============================
23:34:36.431731 [info ] [MainThread]: Running with dbt=1.0.3
23:34:36.432469 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:34:36.432777 [debug] [MainThread]: Tracking: tracking
23:34:36.470735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074bfeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074b5550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074b5f70>]}
23:34:36.549439 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
23:34:36.549833 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:34:36.550121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '91dd01d5-9e4a-4ae6-90a9-766311e45ab9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074bf490>]}
23:34:36.746860 [debug] [MainThread]: Parsing macros/adapters.sql
23:34:36.779418 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:34:36.783726 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:34:36.784385 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:34:36.787622 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:34:36.807019 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:34:36.811736 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:34:36.815751 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:34:36.821029 [debug] [MainThread]: Parsing macros/adapters.sql
23:34:36.858457 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:34:36.867210 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:34:36.867741 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:34:36.870394 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:34:36.893156 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:34:36.897841 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:34:36.903479 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:34:36.908766 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:34:36.912218 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:34:36.914318 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:34:36.915846 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:34:36.931097 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:34:36.942870 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:34:36.954060 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:34:36.958160 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:34:36.959847 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:34:36.961484 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:34:36.965657 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:34:36.976724 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:34:36.978105 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:34:36.987929 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:34:37.002556 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:34:37.009358 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:34:37.011957 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:34:37.018514 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:34:37.019718 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:34:37.022218 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:34:37.024406 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:34:37.030006 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:34:37.045196 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:34:37.046547 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:34:37.048736 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:34:37.050236 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:34:37.051045 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:34:37.051575 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:34:37.052400 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:34:37.053645 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:34:37.057775 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:34:37.065631 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:34:37.067626 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:34:37.070057 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:34:37.078953 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:34:37.081641 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:34:37.085815 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:34:37.092793 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:34:37.101778 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:34:37.104615 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:34:37.105613 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:34:37.106831 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:34:37.107884 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:34:37.113575 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:34:37.114643 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:34:37.116190 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:34:37.118768 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:34:37.119754 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:34:37.121441 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:34:37.123609 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:34:37.132493 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:34:37.134194 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:34:37.135558 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:34:37.136887 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:34:37.138439 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:34:37.139725 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:34:37.141232 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:34:37.142215 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:34:37.145341 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:34:37.150194 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:34:37.151646 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:34:37.154880 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:34:37.156616 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:34:37.157978 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:34:37.159716 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:34:37.182726 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:34:37.184545 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:34:37.186872 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:34:37.188317 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:34:37.189404 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:34:37.190564 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:34:37.191605 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:34:37.192692 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:34:37.194226 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:34:37.195714 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:34:37.197788 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:34:37.199381 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:34:37.200521 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:34:37.202724 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:34:37.204635 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:34:37.205944 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:34:37.207090 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:34:37.209680 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:34:37.211528 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:34:37.213362 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:34:37.215453 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:34:37.218003 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:34:37.219322 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:34:37.222549 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:34:37.230813 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:34:37.234621 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:34:37.236148 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:34:37.239357 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:34:37.243203 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:34:37.246265 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:34:37.247785 [debug] [MainThread]: Parsing macros/sql/star.sql
23:34:37.251725 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:34:37.259014 [debug] [MainThread]: Parsing macros/sql/union.sql
23:34:37.270224 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:34:37.271565 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:34:37.274820 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:34:37.276525 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:34:37.278025 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:34:37.283863 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:34:37.288929 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:34:37.292639 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:34:37.294826 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:34:37.296081 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:34:37.301503 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:34:37.324319 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:34:37.329225 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:34:37.333072 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:34:37.334704 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:34:37.335310 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:34:37.335845 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:34:37.336558 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:34:37.337570 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:34:37.341777 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:34:37.343415 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:34:37.344218 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:34:37.346867 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:34:37.349234 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:34:37.350224 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:34:37.350766 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:34:37.351340 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:34:37.352034 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:34:37.352565 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:34:37.353126 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:34:37.354878 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:34:37.360104 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:34:37.361093 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:34:37.362401 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:34:37.363281 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:34:37.364267 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:34:37.364873 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:34:37.371772 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:34:37.373883 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:34:37.374794 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:34:37.377610 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:34:37.378324 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:34:37.380074 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:34:37.384362 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:34:37.385246 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:34:37.387745 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:34:37.389981 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:34:37.390632 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:34:37.391364 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:34:37.393048 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:34:37.394783 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:34:37.395588 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:34:37.411501 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:34:37.412453 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:34:37.421202 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:34:37.432066 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:34:37.444618 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:34:37.455818 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:34:37.463689 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:34:37.487489 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:34:37.500603 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:34:37.507021 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:34:37.518298 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:34:37.531388 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:34:37.546376 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:34:37.559648 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:34:37.599131 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:34:37.600000 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:34:37.615884 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:34:37.616823 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:34:37.625335 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:34:37.637919 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:34:37.650249 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:34:37.662557 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:34:37.666245 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:34:37.667389 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:34:37.677830 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:34:37.686618 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:34:37.692574 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:34:37.694976 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:34:37.707651 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:34:37.710844 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:34:37.712915 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:34:37.717699 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:34:37.720590 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:34:37.722163 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:34:37.724293 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:34:37.724676 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:34:37.729053 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:34:37.737783 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:34:37.753132 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:34:37.758165 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:34:37.759177 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:34:37.783734 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:34:37.785984 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:34:37.792645 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:34:37.798606 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:34:37.801809 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:34:37.805430 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:34:37.812453 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:34:37.823197 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:34:37.827375 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:34:37.832527 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:34:37.837271 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:34:37.838342 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:34:37.839972 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:34:37.861181 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:34:37.866373 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:34:37.868371 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:34:38.630551 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:34:38.791427 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stg_customer.sql
23:34:38.793007 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:34:38.796033 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:34:38.898527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077d8520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077f8970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077f8730>]}


============================== 2022-03-22 23:35:36.726515 | 8f50a84c-3e82-4adf-aaab-c71adac85a66 ==============================
23:35:36.726515 [info ] [MainThread]: Running with dbt=1.0.3
23:35:36.727241 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:35:36.727494 [debug] [MainThread]: Tracking: tracking
23:35:36.746014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a70dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a7a6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a7a7f0>]}
23:35:36.825342 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
23:35:36.825754 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:35:36.826076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8f50a84c-3e82-4adf-aaab-c71adac85a66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a4f2b0>]}
23:35:37.035562 [debug] [MainThread]: Parsing macros/adapters.sql
23:35:37.057728 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:35:37.062129 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:35:37.062807 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:35:37.065996 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:35:37.085926 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:35:37.091542 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:35:37.095999 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:35:37.102143 [debug] [MainThread]: Parsing macros/adapters.sql
23:35:37.139907 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:35:37.148915 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:35:37.149478 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:35:37.152245 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:35:37.175303 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:35:37.180317 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:35:37.186046 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:35:37.192009 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:35:37.195522 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:35:37.197717 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:35:37.199363 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:35:37.215328 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:35:37.225907 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:35:37.236637 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:35:37.240781 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:35:37.242455 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:35:37.244102 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:35:37.248075 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:35:37.258996 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:35:37.260407 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:35:37.269906 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:35:37.284809 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:35:37.291794 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:35:37.294412 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:35:37.301022 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:35:37.302221 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:35:37.304647 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:35:37.306929 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:35:37.312617 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:35:37.328492 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:35:37.330080 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:35:37.332331 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:35:37.333754 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:35:37.334567 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:35:37.335095 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:35:37.335747 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:35:37.336990 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:35:37.341484 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:35:37.349189 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:35:37.351188 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:35:37.353663 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:35:37.362465 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:35:37.365145 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:35:37.369228 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:35:37.376111 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:35:37.385120 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:35:37.387905 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:35:37.388897 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:35:37.390245 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:35:37.391306 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:35:37.396829 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:35:37.397879 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:35:37.399075 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:35:37.401724 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:35:37.402734 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:35:37.404465 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:35:37.406651 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:35:37.415977 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:35:37.417716 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:35:37.419029 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:35:37.420354 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:35:37.421878 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:35:37.423391 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:35:37.424818 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:35:37.425691 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:35:37.428820 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:35:37.433562 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:35:37.435070 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:35:37.438224 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:35:37.440059 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:35:37.441415 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:35:37.443154 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:35:37.466451 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:35:37.468384 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:35:37.470749 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:35:37.472349 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:35:37.473376 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:35:37.474488 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:35:37.475523 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:35:37.476747 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:35:37.478288 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:35:37.479781 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:35:37.481989 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:35:37.483627 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:35:37.484772 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:35:37.487571 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:35:37.489542 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:35:37.491008 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:35:37.492349 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:35:37.494942 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:35:37.496873 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:35:37.498841 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:35:37.500839 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:35:37.503397 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:35:37.504731 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:35:37.507880 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:35:37.517801 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:35:37.522294 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:35:37.523854 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:35:37.527998 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:35:37.532192 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:35:37.535372 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:35:37.537091 [debug] [MainThread]: Parsing macros/sql/star.sql
23:35:37.541551 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:35:37.549163 [debug] [MainThread]: Parsing macros/sql/union.sql
23:35:37.558878 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:35:37.560115 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:35:37.563268 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:35:37.564758 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:35:37.566305 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:35:37.572162 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:35:37.577011 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:35:37.580839 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:35:37.582994 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:35:37.584198 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:35:37.589602 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:35:37.612455 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:35:37.617513 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:35:37.621430 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:35:37.623116 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:35:37.623746 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:35:37.624300 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:35:37.625004 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:35:37.625561 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:35:37.628651 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:35:37.630240 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:35:37.631020 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:35:37.633580 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:35:37.636052 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:35:37.637111 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:35:37.637684 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:35:37.638284 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:35:37.638999 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:35:37.639554 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:35:37.640118 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:35:37.641941 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:35:37.647376 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:35:37.648298 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:35:37.649621 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:35:37.650499 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:35:37.651493 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:35:37.652115 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:35:37.659085 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:35:37.661269 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:35:37.662203 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:35:37.665080 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:35:37.665789 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:35:37.667635 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:35:37.672151 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:35:37.673423 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:35:37.676623 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:35:37.679292 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:35:37.679965 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:35:37.680605 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:35:37.682310 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:35:37.684103 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:35:37.684941 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:35:37.700951 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:35:37.701906 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:35:37.710707 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:35:37.721517 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:35:37.734077 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:35:37.745355 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:35:37.753135 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:35:37.777295 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:35:37.790850 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:35:37.797422 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:35:37.807553 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:35:37.819676 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:35:37.834077 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:35:37.847781 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:35:37.887646 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:35:37.888515 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:35:37.904205 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:35:37.905235 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:35:37.916618 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:35:37.937765 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:35:37.953957 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:35:37.969575 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:35:37.974224 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:35:37.975776 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:35:37.988553 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:35:38.000143 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:35:38.007546 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:35:38.010551 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:35:38.025485 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:35:38.029283 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:35:38.031589 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:35:38.036640 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:35:38.039775 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:35:38.041336 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:35:38.043571 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:35:38.043964 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:35:38.048278 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:35:38.057110 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:35:38.072824 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:35:38.077937 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:35:38.078977 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:35:38.104152 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:35:38.106467 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:35:38.111630 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:35:38.118155 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:35:38.121416 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:35:38.125133 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:35:38.132292 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:35:38.143404 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:35:38.147733 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:35:38.152978 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:35:38.157841 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:35:38.158931 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:35:38.160562 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:35:38.181675 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:35:38.186840 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:35:38.188861 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:35:38.963722 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:35:39.122201 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stg_customer.sql
23:35:39.123760 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:35:39.126626 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:35:39.214781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d8a640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e19bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e19b80>]}


============================== 2022-03-22 23:43:11.892231 | 2755f8ca-a6fb-4605-9e0a-cd77e1d3f013 ==============================
23:43:11.892231 [info ] [MainThread]: Running with dbt=1.0.3
23:43:11.893536 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:43:11.893844 [debug] [MainThread]: Tracking: tracking
23:43:11.935890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8d9340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8d3700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8d3190>]}
23:43:12.031713 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
23:43:12.032176 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:43:12.032493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2755f8ca-a6fb-4605-9e0a-cd77e1d3f013', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8d90a0>]}
23:43:12.288498 [debug] [MainThread]: Parsing macros/adapters.sql
23:43:12.312996 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:43:12.317351 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:43:12.318012 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:43:12.321230 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:43:12.338815 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:43:12.343635 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:43:12.347513 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:43:12.353003 [debug] [MainThread]: Parsing macros/adapters.sql
23:43:12.390309 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:43:12.399191 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:43:12.399729 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:43:12.402496 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:43:12.425551 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:43:12.430205 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:43:12.436029 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:43:12.441207 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:43:12.444806 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:43:12.446927 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:43:12.448436 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:43:12.464014 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:43:12.474360 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:43:12.485630 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:43:12.489842 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:43:12.491543 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:43:12.493180 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:43:12.497485 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:43:12.508322 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:43:12.509809 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:43:12.519041 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:43:12.533845 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:43:12.540727 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:43:12.543516 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:43:12.550112 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:43:12.551307 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:43:12.553700 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:43:12.555810 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:43:12.561382 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:43:12.576510 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:43:12.577868 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:43:12.580089 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:43:12.581594 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:43:12.582405 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:43:12.582971 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:43:12.583707 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:43:12.584952 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:43:12.588914 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:43:12.596931 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:43:12.598940 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:43:12.601486 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:43:12.610424 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:43:12.613046 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:43:12.617081 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:43:12.624119 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:43:12.633092 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:43:12.635900 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:43:12.636873 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:43:12.638015 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:43:12.639052 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:43:12.644563 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:43:12.645605 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:43:12.646783 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:43:12.649342 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:43:12.650319 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:43:12.651996 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:43:12.654155 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:43:12.663275 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:43:12.664957 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:43:12.666252 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:43:12.667650 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:43:12.669160 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:43:12.670570 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:43:12.672029 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:43:12.672929 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:43:12.676086 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:43:12.680931 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:43:12.682383 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:43:12.685610 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:43:12.687305 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:43:12.688770 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:43:12.690721 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:43:12.713395 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:43:12.715170 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:43:12.717544 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:43:12.718980 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:43:12.720091 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:43:12.721276 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:43:12.722314 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:43:12.723407 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:43:12.724949 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:43:12.726457 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:43:12.728566 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:43:12.730272 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:43:12.731524 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:43:12.733732 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:43:12.735918 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:43:12.737263 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:43:12.738436 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:43:12.740990 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:43:12.742854 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:43:12.744732 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:43:12.746771 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:43:12.749560 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:43:12.750886 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:43:12.755815 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:43:12.764881 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:43:12.768780 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:43:12.770446 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:43:12.773703 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:43:12.777538 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:43:12.780681 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:43:12.782231 [debug] [MainThread]: Parsing macros/sql/star.sql
23:43:12.786217 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:43:12.793374 [debug] [MainThread]: Parsing macros/sql/union.sql
23:43:12.802963 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:43:12.804272 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:43:12.807607 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:43:12.809105 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:43:12.810633 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:43:12.816408 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:43:12.821236 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:43:12.825108 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:43:12.827256 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:43:12.828427 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:43:12.835411 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:43:12.859010 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:43:12.863902 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:43:12.867771 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:43:12.869405 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:43:12.870221 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:43:12.870904 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:43:12.871598 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:43:12.872145 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:43:12.875349 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:43:12.876960 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:43:12.877752 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:43:12.880342 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:43:12.882814 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:43:12.883822 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:43:12.884371 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:43:12.884949 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:43:12.885701 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:43:12.886230 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:43:12.886773 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:43:12.888536 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:43:12.894110 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:43:12.895046 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:43:12.896385 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:43:12.897268 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:43:12.898370 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:43:12.898976 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:43:12.905931 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:43:12.908009 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:43:12.908914 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:43:12.911655 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:43:12.912356 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:43:12.914077 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:43:12.918356 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:43:12.919212 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:43:12.921696 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:43:12.923946 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:43:12.924628 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:43:12.925259 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:43:12.926909 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:43:12.928615 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:43:12.929455 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:43:12.945428 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:43:12.946370 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:43:12.954986 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:43:12.965647 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:43:12.978293 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:43:12.989717 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:43:12.997508 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:43:13.021517 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:43:13.035041 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:43:13.042197 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:43:13.052322 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:43:13.064354 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:43:13.078483 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:43:13.097200 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:43:13.136581 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:43:13.137441 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:43:13.153172 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:43:13.154110 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:43:13.162778 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:43:13.175854 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:43:13.188261 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:43:13.200577 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:43:13.204065 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:43:13.205210 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:43:13.215264 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:43:13.223938 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:43:13.229642 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:43:13.232132 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:43:13.244697 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:43:13.247868 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:43:13.249865 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:43:13.254725 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:43:13.257647 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:43:13.259119 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:43:13.261368 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:43:13.261743 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:43:13.266092 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:43:13.276030 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:43:13.291868 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:43:13.297227 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:43:13.298369 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:43:13.325898 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:43:13.328430 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:43:13.333946 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:43:13.340089 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:43:13.343540 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:43:13.347126 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:43:13.354194 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:43:13.365242 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:43:13.370085 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:43:13.375905 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:43:13.381004 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:43:13.382285 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:43:13.383944 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:43:13.405502 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:43:13.410811 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:43:13.412893 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:43:14.185796 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:43:14.345151 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stg_customer.sql
23:43:14.346713 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:43:14.349604 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:43:14.352267 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:43:14.499979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2755f8ca-a6fb-4605-9e0a-cd77e1d3f013', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbfb0d0>]}
23:43:14.592987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2755f8ca-a6fb-4605-9e0a-cd77e1d3f013', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb20ac0>]}
23:43:14.593435 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:43:14.594846 [info ] [MainThread]: 
23:43:14.595403 [debug] [MainThread]: Acquiring new databricks connection "master"
23:43:14.596554 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_ss_finance"
23:43:14.608786 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:43:14.609077 [debug] [ThreadPool]: Using databricks connection "list_None_ss_finance"
23:43:14.609275 [debug] [ThreadPool]: On list_None_ss_finance: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_ss_finance"} */
show table extended in ss_finance like '*'
  
23:43:14.609456 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:43:21.347993 [debug] [ThreadPool]: SQL status: OK in 6.74 seconds
23:43:21.979333 [debug] [ThreadPool]: On list_None_ss_finance: ROLLBACK
23:43:21.979708 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:43:21.979958 [debug] [ThreadPool]: On list_None_ss_finance: Close
23:43:29.372846 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:43:29.374281 [info ] [MainThread]: 
23:43:29.393134 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
23:43:29.393642 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
23:43:29.396276 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
23:43:29.396522 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
23:43:29.410479 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:43:29.410751 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
23:43:29.410941 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended ss_finance.raw_customer
  
23:43:29.411122 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:43:29.823353 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended ss_finance.raw_customer
  
23:43:29.823754 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': ss_finance.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#372, data_type#373, comment#374]
+- 'UnresolvedTableOrView [ss_finance, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': ss_finance.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#372, data_type#373, comment#374]
+- 'UnresolvedTableOrView [ss_finance, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

23:43:29.824272 [debug] [Thread-1  ]: Databricks adapter: Error while running:
macro get_columns_in_relation
23:43:29.824553 [debug] [Thread-1  ]: Databricks adapter: Runtime Error
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': ss_finance.raw_customer; line 3 pos 6;
  'DescribeRelation true, [col_name#372, data_type#373, comment#374]
  +- 'UnresolvedTableOrView [ss_finance, raw_customer], DESCRIBE TABLE, true
23:43:29.827166 [debug] [Thread-1  ]: finished collecting timing info
23:43:29.827502 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
23:43:29.827739 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:43:29.827960 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
23:43:30.042251 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/staging/stg_customer.sql)
  Runtime Error
    Query execution failed.
    Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': ss_finance.raw_customer; line 3 pos 6;
    'DescribeRelation true, [col_name#372, data_type#373, comment#374]
    +- 'UnresolvedTableOrView [ss_finance, raw_customer], DESCRIBE TABLE, true
23:43:30.042859 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
23:43:30.043446 [debug] [MainThread]: Connection 'master' was properly closed.
23:43:30.043788 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
23:43:30.044208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbfba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbfbbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be98970>]}


============================== 2022-03-22 23:45:11.943920 | 0fb71cc7-cd1d-442f-ac29-7db16b836bea ==============================
23:45:11.943920 [info ] [MainThread]: Running with dbt=1.0.3
23:45:11.944888 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:45:11.945205 [debug] [MainThread]: Tracking: tracking
23:45:11.981289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11086edc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108776a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108777f0>]}
23:45:12.092135 [info ] [MainThread]: Unable to do partial parsing because profile has changed
23:45:12.092591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0fb71cc7-cd1d-442f-ac29-7db16b836bea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11084b700>]}
23:45:12.394891 [debug] [MainThread]: Parsing macros/adapters.sql
23:45:12.437149 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:45:12.442095 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:45:12.442845 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:45:12.446343 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:45:12.466578 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:45:12.472171 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:45:12.476698 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:45:12.482826 [debug] [MainThread]: Parsing macros/adapters.sql
23:45:12.519868 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:45:12.528631 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:45:12.529162 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:45:12.531911 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:45:12.554325 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:45:12.558975 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:45:12.564744 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:45:12.569991 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:45:12.573457 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:45:12.575564 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:45:12.577045 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:45:12.592676 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:45:12.603084 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:45:12.613669 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:45:12.617705 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:45:12.619324 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:45:12.621075 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:45:12.625040 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:45:12.635573 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:45:12.636961 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:45:12.646340 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:45:12.660689 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:45:12.667541 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:45:12.670234 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:45:12.676686 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:45:12.677855 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:45:12.680285 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:45:12.682341 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:45:12.687744 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:45:12.702605 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:45:12.703930 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:45:12.706111 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:45:12.708008 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:45:12.708840 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:45:12.709366 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:45:12.710007 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:45:12.711247 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:45:12.715196 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:45:12.722949 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:45:12.724890 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:45:12.727297 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:45:12.735966 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:45:12.738570 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:45:12.742508 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:45:12.749103 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:45:12.757894 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:45:12.760689 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:45:12.761659 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:45:12.762847 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:45:12.763870 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:45:12.769348 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:45:12.770376 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:45:12.771543 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:45:12.774083 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:45:12.775051 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:45:12.776709 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:45:12.778850 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:45:12.787722 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:45:12.789393 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:45:12.790677 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:45:12.791968 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:45:12.793538 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:45:12.794808 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:45:12.796211 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:45:12.797076 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:45:12.800117 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:45:12.804831 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:45:12.806249 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:45:12.809434 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:45:12.811072 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:45:12.812411 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:45:12.814112 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:45:12.836159 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:45:12.837906 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:45:12.840217 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:45:12.841635 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:45:12.842655 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:45:12.843861 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:45:12.844884 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:45:12.845959 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:45:12.847465 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:45:12.848928 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:45:12.850971 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:45:12.852532 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:45:12.853650 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:45:12.855838 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:45:12.857849 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:45:12.859154 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:45:12.860284 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:45:12.862846 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:45:12.864631 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:45:12.866354 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:45:12.868326 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:45:12.870949 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:45:12.872214 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:45:12.875364 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:45:12.883760 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:45:12.887506 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:45:12.888929 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:45:12.892307 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:45:12.896206 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:45:12.899247 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:45:12.900706 [debug] [MainThread]: Parsing macros/sql/star.sql
23:45:12.904572 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:45:12.912230 [debug] [MainThread]: Parsing macros/sql/union.sql
23:45:12.922915 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:45:12.924734 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:45:12.927795 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:45:12.929237 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:45:12.931025 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:45:12.936550 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:45:12.941407 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:45:12.945294 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:45:12.947596 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:45:12.948741 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:45:12.954002 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:45:12.976809 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:45:12.981627 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:45:12.985429 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:45:12.987049 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:45:12.987649 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:45:12.988180 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:45:12.988849 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:45:12.989387 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:45:12.992359 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:45:12.993894 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:45:12.994653 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:45:12.997185 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:45:12.999460 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:45:13.000433 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:45:13.000966 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:45:13.001529 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:45:13.002213 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:45:13.002730 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:45:13.003257 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:45:13.004972 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:45:13.010029 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:45:13.010911 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:45:13.012187 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:45:13.013034 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:45:13.013998 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:45:13.014602 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:45:13.021398 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:45:13.023422 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:45:13.024560 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:45:13.027245 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:45:13.027919 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:45:13.029567 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:45:13.033721 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:45:13.034602 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:45:13.037255 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:45:13.039508 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:45:13.040153 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:45:13.040787 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:45:13.042424 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:45:13.044130 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:45:13.044923 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:45:13.060333 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:45:13.061226 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:45:13.070235 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:45:13.085260 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:45:13.099687 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:45:13.112761 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:45:13.121909 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:45:13.146807 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:45:13.160478 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:45:13.166924 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:45:13.177223 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:45:13.189362 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:45:13.203773 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:45:13.217319 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:45:13.261420 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:45:13.262360 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:45:13.277813 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:45:13.278713 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:45:13.287282 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:45:13.299421 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:45:13.311481 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:45:13.323623 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:45:13.327039 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:45:13.328175 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:45:13.338276 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:45:13.346860 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:45:13.352586 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:45:13.354929 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:45:13.367344 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:45:13.371007 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:45:13.373047 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:45:13.377904 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:45:13.380790 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:45:13.382322 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:45:13.384401 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:45:13.384956 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:45:13.389411 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:45:13.398010 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:45:13.413481 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:45:13.418508 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:45:13.419551 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:45:13.443763 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:45:13.445938 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:45:13.450855 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:45:13.456855 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:45:13.460500 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:45:13.463995 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:45:13.470896 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:45:13.481686 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:45:13.485836 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:45:13.490986 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:45:13.495801 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:45:13.496883 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:45:13.498452 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:45:13.519098 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:45:13.524352 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:45:13.526576 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:45:14.312161 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:45:14.432897 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stg_customer.sql
23:45:14.434380 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:45:14.436821 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:45:14.439211 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:45:14.578311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0fb71cc7-cd1d-442f-ac29-7db16b836bea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109210d0>]}
23:45:14.593426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0fb71cc7-cd1d-442f-ac29-7db16b836bea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110877910>]}
23:45:14.593771 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:45:14.594950 [info ] [MainThread]: 
23:45:14.595464 [debug] [MainThread]: Acquiring new databricks connection "master"
23:45:14.596538 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:45:14.607229 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:45:14.607494 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:45:14.607672 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:45:14.607840 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:45:16.625701 [debug] [ThreadPool]: SQL status: OK in 2.02 seconds
23:45:17.030207 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:45:17.030513 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:45:17.030718 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:45:17.209537 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:45:17.210140 [info ] [MainThread]: 
23:45:17.223871 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
23:45:17.224565 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
23:45:17.227288 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
23:45:17.227524 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
23:45:17.241396 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:45:17.241677 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
23:45:17.241871 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:45:17.242052 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:45:17.601909 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:45:17.602239 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#476, data_type#477, comment#478]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#476, data_type#477, comment#478]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

23:45:17.602681 [debug] [Thread-1  ]: Databricks adapter: Error while running:
macro get_columns_in_relation
23:45:17.602898 [debug] [Thread-1  ]: Databricks adapter: Runtime Error
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
  'DescribeRelation true, [col_name#476, data_type#477, comment#478]
  +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:45:17.621430 [debug] [Thread-1  ]: finished collecting timing info
23:45:17.621856 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
23:45:17.622135 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:45:17.622403 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
23:45:17.827515 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/staging/stg_customer.sql)
  Runtime Error
    Query execution failed.
    Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
    'DescribeRelation true, [col_name#476, data_type#477, comment#478]
    +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:45:17.828118 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
23:45:17.828973 [debug] [MainThread]: Connection 'master' was properly closed.
23:45:17.829306 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
23:45:17.829743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110921760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110921640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110db4c70>]}


============================== 2022-03-22 23:47:00.880802 | 493f6069-ab52-457b-a75e-8c8b708be5ae ==============================
23:47:00.880802 [info ] [MainThread]: Running with dbt=1.0.3
23:47:00.881928 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:47:00.882309 [debug] [MainThread]: Tracking: tracking
23:47:00.902949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a5d970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a5d670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a5d460>]}
23:47:01.281653 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
23:47:01.282092 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/staging/stg_customer.sql
23:47:01.294485 [debug] [MainThread]: 1603: static parser failed on staging/stg_customer.sql
23:47:01.419751 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stg_customer.sql
23:47:01.420187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d5d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c1fb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c1f8b0>]}


============================== 2022-03-22 23:48:49.126183 | a4ab9626-8f93-4b5b-88f8-e918433ecfba ==============================
23:48:49.126183 [info ] [MainThread]: Running with dbt=1.0.3
23:48:49.126871 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:48:49.127152 [debug] [MainThread]: Tracking: tracking
23:48:49.149015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109618a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109628fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109628220>]}
23:48:49.251856 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:48:49.252269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a4ab9626-8f93-4b5b-88f8-e918433ecfba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109618070>]}
23:48:49.533310 [debug] [MainThread]: Parsing macros/adapters.sql
23:48:49.558195 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:48:49.562441 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:48:49.563094 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:48:49.566158 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:48:49.584408 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:48:49.589561 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:48:49.593415 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:48:49.598819 [debug] [MainThread]: Parsing macros/adapters.sql
23:48:49.635825 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:48:49.644731 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:48:49.645285 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:48:49.648047 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:48:49.671069 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:48:49.675806 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:48:49.681644 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:48:49.686960 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:48:49.690594 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:48:49.692758 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:48:49.694278 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:48:49.709859 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:48:49.720287 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:48:49.731151 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:48:49.735263 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:48:49.736880 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:48:49.738529 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:48:49.742660 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:48:49.753118 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:48:49.754513 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:48:49.763880 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:48:49.778205 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:48:49.784961 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:48:49.787541 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:48:49.793966 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:48:49.795145 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:48:49.797529 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:48:49.799590 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:48:49.805438 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:48:49.820547 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:48:49.821901 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:48:49.824089 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:48:49.825497 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:48:49.826294 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:48:49.826806 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:48:49.827441 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:48:49.828672 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:48:49.832716 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:48:49.840754 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:48:49.842718 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:48:49.845135 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:48:49.853718 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:48:49.856328 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:48:49.860377 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:48:49.866962 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:48:49.875758 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:48:49.878478 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:48:49.879462 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:48:49.880602 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:48:49.881633 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:48:49.887124 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:48:49.888150 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:48:49.889313 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:48:49.891935 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:48:49.893108 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:48:49.894787 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:48:49.896940 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:48:49.905846 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:48:49.907527 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:48:49.908812 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:48:49.910109 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:48:49.911599 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:48:49.912878 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:48:49.914283 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:48:49.915160 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:48:49.918233 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:48:49.922961 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:48:49.924382 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:48:49.927579 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:48:49.929227 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:48:49.930563 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:48:49.932269 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:48:49.954909 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:48:49.956686 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:48:49.959003 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:48:49.960423 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:48:49.961435 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:48:49.962541 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:48:49.963562 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:48:49.964633 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:48:49.966146 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:48:49.967625 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:48:49.969677 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:48:49.971211 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:48:49.972320 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:48:49.974463 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:48:49.976310 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:48:49.977582 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:48:49.978692 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:48:49.981155 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:48:49.982904 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:48:49.984582 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:48:49.986497 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:48:49.989105 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:48:49.990370 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:48:49.993469 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:48:50.001556 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:48:50.005180 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:48:50.006594 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:48:50.009710 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:48:50.013536 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:48:50.016524 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:48:50.018340 [debug] [MainThread]: Parsing macros/sql/star.sql
23:48:50.022188 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:48:50.029098 [debug] [MainThread]: Parsing macros/sql/union.sql
23:48:50.038687 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:48:50.040067 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:48:50.043110 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:48:50.044575 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:48:50.046133 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:48:50.051751 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:48:50.056378 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:48:50.060004 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:48:50.062159 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:48:50.063319 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:48:50.068605 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:48:50.090654 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:48:50.095297 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:48:50.099106 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:48:50.100843 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:48:50.101478 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:48:50.102000 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:48:50.102656 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:48:50.103175 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:48:50.106001 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:48:50.107518 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:48:50.108261 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:48:50.110690 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:48:50.112967 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:48:50.113923 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:48:50.114448 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:48:50.115005 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:48:50.115673 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:48:50.116181 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:48:50.116699 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:48:50.118478 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:48:50.123418 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:48:50.124288 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:48:50.125536 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:48:50.126617 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:48:50.127712 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:48:50.128303 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:48:50.134843 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:48:50.136827 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:48:50.137907 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:48:50.140596 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:48:50.141275 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:48:50.142916 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:48:50.146957 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:48:50.147811 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:48:50.150225 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:48:50.152368 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:48:50.152993 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:48:50.153624 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:48:50.155246 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:48:50.156920 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:48:50.157842 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:48:50.173530 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:48:50.174437 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:48:50.182790 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:48:50.193637 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:48:50.205707 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:48:50.216270 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:48:50.223818 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:48:50.246264 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:48:50.258874 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:48:50.265015 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:48:50.274614 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:48:50.286145 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:48:50.302704 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:48:50.315825 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:48:50.358125 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:48:50.359000 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:48:50.374179 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:48:50.375208 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:48:50.383894 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:48:50.396360 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:48:50.409491 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:48:50.421665 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:48:50.425108 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:48:50.426244 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:48:50.436145 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:48:50.445107 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:48:50.450912 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:48:50.453272 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:48:50.466618 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:48:50.470012 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:48:50.472032 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:48:50.476735 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:48:50.479636 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:48:50.481098 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:48:50.483270 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:48:50.483645 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:48:50.487835 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:48:50.497001 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:48:50.512244 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:48:50.517188 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:48:50.518306 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:48:50.543042 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:48:50.545363 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:48:50.550360 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:48:50.556264 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:48:50.559593 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:48:50.563159 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:48:50.570160 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:48:50.580724 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:48:50.584975 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:48:50.590151 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:48:50.594989 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:48:50.596048 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:48:50.597628 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:48:50.618165 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:48:50.623220 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:48:50.625198 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:48:51.410989 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:48:51.420590 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:48:51.422820 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:48:51.425196 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:48:51.537332 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:48:51.630390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10999a100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10998d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10998d940>]}


============================== 2022-03-22 23:51:06.425326 | 6d83f1a3-87e0-4075-83d9-895be257ba21 ==============================
23:51:06.425326 [info ] [MainThread]: Running with dbt=1.0.3
23:51:06.426229 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=None, exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:51:06.426519 [debug] [MainThread]: Tracking: tracking
23:51:06.444104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e8b4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e99fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e99220>]}
23:51:06.538869 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:51:06.539294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6d83f1a3-87e0-4075-83d9-895be257ba21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e8ba00>]}
23:51:06.829926 [debug] [MainThread]: Parsing macros/adapters.sql
23:51:06.851940 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:51:06.856345 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:51:06.857025 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:51:06.860178 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:51:06.877398 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:51:06.882095 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:51:06.885879 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:51:06.891120 [debug] [MainThread]: Parsing macros/adapters.sql
23:51:06.927652 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:51:06.936398 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:51:06.936942 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:51:06.939631 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:51:06.962109 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:51:06.966789 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:51:06.972421 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:51:06.977512 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:51:06.981168 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:51:06.983229 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:51:06.984736 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:51:07.001195 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:51:07.011382 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:51:07.022092 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:51:07.026021 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:51:07.027592 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:51:07.029197 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:51:07.033172 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:51:07.045352 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:51:07.046768 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:51:07.055734 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:51:07.069896 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:51:07.076588 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:51:07.079200 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:51:07.085766 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:51:07.086976 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:51:07.089929 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:51:07.092723 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:51:07.098875 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:51:07.114285 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:51:07.115684 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:51:07.118174 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:51:07.120418 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:51:07.121310 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:51:07.121846 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:51:07.122503 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:51:07.123770 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:51:07.127878 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:51:07.135839 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:51:07.137897 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:51:07.140369 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:51:07.150494 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:51:07.153220 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:51:07.157250 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:51:07.163993 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:51:07.173482 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:51:07.176418 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:51:07.177854 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:51:07.179117 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:51:07.180273 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:51:07.186091 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:51:07.187253 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:51:07.188513 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:51:07.191200 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:51:07.192237 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:51:07.193980 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:51:07.196233 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:51:07.205392 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:51:07.207171 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:51:07.208526 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:51:07.209886 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:51:07.211443 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:51:07.212781 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:51:07.214249 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:51:07.215162 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:51:07.218383 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:51:07.223312 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:51:07.224822 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:51:07.228132 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:51:07.229852 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:51:07.231256 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:51:07.233034 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:51:07.255918 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:51:07.257814 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:51:07.260232 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:51:07.261716 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:51:07.262777 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:51:07.263935 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:51:07.265013 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:51:07.266128 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:51:07.267706 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:51:07.269262 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:51:07.271427 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:51:07.273075 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:51:07.274267 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:51:07.276546 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:51:07.278525 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:51:07.279888 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:51:07.281084 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:51:07.283723 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:51:07.285631 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:51:07.287452 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:51:07.289515 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:51:07.292146 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:51:07.293505 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:51:07.296763 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:51:07.305466 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:51:07.309388 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:51:07.310903 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:51:07.314071 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:51:07.318166 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:51:07.321391 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:51:07.323007 [debug] [MainThread]: Parsing macros/sql/star.sql
23:51:07.327149 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:51:07.334480 [debug] [MainThread]: Parsing macros/sql/union.sql
23:51:07.344154 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:51:07.345488 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:51:07.348698 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:51:07.350234 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:51:07.351786 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:51:07.357697 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:51:07.362538 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:51:07.366363 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:51:07.368608 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:51:07.369835 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:51:07.375306 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:51:07.398552 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:51:07.403538 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:51:07.407528 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:51:07.409216 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:51:07.409853 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:51:07.410419 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:51:07.411127 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:51:07.411687 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:51:07.414831 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:51:07.416832 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:51:07.417728 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:51:07.420386 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:51:07.422974 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:51:07.424057 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:51:07.424631 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:51:07.425229 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:51:07.425946 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:51:07.426497 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:51:07.427125 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:51:07.429030 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:51:07.434542 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:51:07.435627 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:51:07.437186 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:51:07.438208 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:51:07.439377 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:51:07.440095 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:51:07.447633 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:51:07.449761 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:51:07.450693 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:51:07.453512 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:51:07.454220 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:51:07.455948 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:51:07.460213 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:51:07.461101 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:51:07.463658 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:51:07.465910 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:51:07.466564 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:51:07.467216 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:51:07.468915 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:51:07.470682 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:51:07.471498 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:51:07.487421 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:51:07.488349 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:51:07.497079 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:51:07.508094 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:51:07.520666 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:51:07.531710 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:51:07.539614 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:51:07.563373 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:51:07.576029 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:51:07.583633 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:51:07.593755 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:51:07.605498 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:51:07.619773 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:51:07.632717 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:51:07.672049 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:51:07.672899 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:51:07.688284 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:51:07.689190 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:51:07.697759 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:51:07.709833 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:51:07.722006 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:51:07.734062 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:51:07.737510 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:51:07.738660 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:51:07.748724 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:51:07.757369 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:51:07.763147 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:51:07.766417 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:51:07.780002 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:51:07.783198 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:51:07.785358 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:51:07.790039 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:51:07.792902 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:51:07.794350 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:51:07.796447 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:51:07.796822 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:51:07.801050 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:51:07.809496 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:51:07.824929 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:51:07.829816 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:51:07.830985 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:51:07.854726 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:51:07.857121 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:51:07.862015 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:51:07.867862 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:51:07.871274 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:51:07.874772 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:51:07.881616 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:51:07.892106 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:51:07.896231 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:51:07.901239 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:51:07.905964 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:51:07.907006 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:51:07.908578 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:51:07.929509 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:51:07.934667 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:51:07.937764 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:51:08.761753 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:51:08.772045 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:51:08.774479 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:51:08.777042 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:51:08.896436 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:51:08.985449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109216100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092098b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092095e0>]}


============================== 2022-03-22 23:51:34.121414 | 603eb735-9542-4f61-a61d-9a827fc800c3 ==============================
23:51:34.121414 [info ] [MainThread]: Running with dbt=1.0.3
23:51:34.122569 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['raw_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:51:34.123027 [debug] [MainThread]: Tracking: tracking
23:51:34.144916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106204f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11062c760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11062c310>]}
23:51:34.238722 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:51:34.239157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '603eb735-9542-4f61-a61d-9a827fc800c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110620a00>]}
23:51:34.502454 [debug] [MainThread]: Parsing macros/adapters.sql
23:51:34.534318 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:51:34.538843 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:51:34.539513 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:51:34.542712 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:51:34.561543 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:51:34.566187 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:51:34.569899 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:51:34.574989 [debug] [MainThread]: Parsing macros/adapters.sql
23:51:34.611490 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:51:34.620422 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:51:34.620956 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:51:34.623592 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:51:34.647234 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:51:34.651813 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:51:34.657371 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:51:34.662554 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:51:34.665975 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:51:34.668110 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:51:34.669613 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:51:34.685030 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:51:34.695374 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:51:34.706150 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:51:34.710209 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:51:34.711815 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:51:34.713469 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:51:34.717512 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:51:34.727783 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:51:34.729160 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:51:34.738475 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:51:34.752683 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:51:34.759478 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:51:34.762117 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:51:34.768691 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:51:34.769912 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:51:34.772564 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:51:34.774644 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:51:34.780249 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:51:34.795621 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:51:34.796985 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:51:34.799149 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:51:34.800540 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:51:34.801334 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:51:34.801843 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:51:34.802471 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:51:34.803693 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:51:34.807569 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:51:34.815216 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:51:34.817306 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:51:34.819839 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:51:34.828507 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:51:34.831189 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:51:34.835143 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:51:34.842538 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:51:34.852884 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:51:34.855697 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:51:34.856889 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:51:34.858024 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:51:34.859051 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:51:34.864513 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:51:34.865543 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:51:34.866787 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:51:34.870207 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:51:34.871272 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:51:34.873108 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:51:34.875380 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:51:34.884740 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:51:34.886544 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:51:34.888059 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:51:34.889808 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:51:34.891411 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:51:34.892759 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:51:34.894229 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:51:34.895146 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:51:34.898403 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:51:34.903511 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:51:34.904964 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:51:34.908182 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:51:34.909857 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:51:34.911217 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:51:34.913050 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:51:34.935967 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:51:34.937902 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:51:34.940281 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:51:34.941756 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:51:34.942804 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:51:34.943943 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:51:34.945085 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:51:34.946182 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:51:34.947720 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:51:34.949226 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:51:34.951320 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:51:34.952909 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:51:34.954070 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:51:34.956281 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:51:34.958210 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:51:34.959531 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:51:34.960687 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:51:34.963313 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:51:34.965135 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:51:34.966947 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:51:34.968963 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:51:34.971560 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:51:34.972885 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:51:34.976048 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:51:34.984540 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:51:34.988260 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:51:34.989716 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:51:34.992751 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:51:34.996606 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:51:34.999638 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:51:35.001145 [debug] [MainThread]: Parsing macros/sql/star.sql
23:51:35.005070 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:51:35.012266 [debug] [MainThread]: Parsing macros/sql/union.sql
23:51:35.021764 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:51:35.022993 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:51:35.026085 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:51:35.027563 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:51:35.029054 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:51:35.034864 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:51:35.039624 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:51:35.043386 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:51:35.045462 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:51:35.046624 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:51:35.051888 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:51:35.073971 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:51:35.078666 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:51:35.082708 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:51:35.084308 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:51:35.084901 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:51:35.085424 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:51:35.086082 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:51:35.086604 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:51:35.089511 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:51:35.091042 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:51:35.091790 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:51:35.094240 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:51:35.096479 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:51:35.097436 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:51:35.097963 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:51:35.098521 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:51:35.099196 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:51:35.099706 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:51:35.100227 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:51:35.101923 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:51:35.106850 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:51:35.107718 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:51:35.108978 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:51:35.109813 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:51:35.110761 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:51:35.111349 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:51:35.117965 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:51:35.119967 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:51:35.120831 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:51:35.123461 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:51:35.124121 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:51:35.125852 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:51:35.130025 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:51:35.130877 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:51:35.133328 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:51:35.135459 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:51:35.136075 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:51:35.136705 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:51:35.138321 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:51:35.140001 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:51:35.140785 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:51:35.156867 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:51:35.157821 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:51:35.166328 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:51:35.177367 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:51:35.190779 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:51:35.201631 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:51:35.209183 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:51:35.232123 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:51:35.244858 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:51:35.251184 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:51:35.261029 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:51:35.273487 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:51:35.288289 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:51:35.301666 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:51:35.341129 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:51:35.341991 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:51:35.357633 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:51:35.358553 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:51:35.367264 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:51:35.379701 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:51:35.392410 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:51:35.404550 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:51:35.408009 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:51:35.409150 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:51:35.419166 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:51:35.427731 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:51:35.433658 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:51:35.436023 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:51:35.448655 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:51:35.451811 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:51:35.453854 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:51:35.458577 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:51:35.461490 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:51:35.463010 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:51:35.465136 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:51:35.465509 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:51:35.470031 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:51:35.479007 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:51:35.494266 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:51:35.499299 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:51:35.500305 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:51:35.524554 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:51:35.526828 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:51:35.531821 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:51:35.537931 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:51:35.541226 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:51:35.544791 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:51:35.551702 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:51:35.562468 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:51:35.566799 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:51:35.571858 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:51:35.576562 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:51:35.577590 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:51:35.579123 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:51:35.599981 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:51:35.605604 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:51:35.607941 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:51:36.407401 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:51:36.417292 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:51:36.419608 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:51:36.422049 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:51:36.534785 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:51:36.626635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11099c100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11098e0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11098e3d0>]}


============================== 2022-03-22 23:53:00.529057 | e4767dcb-790c-4775-a7eb-f7c39f50f3b1 ==============================
23:53:00.529057 [info ] [MainThread]: Running with dbt=1.0.3
23:53:00.530179 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['raw_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:53:00.530531 [debug] [MainThread]: Tracking: tracking
23:53:00.553239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075454f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107551220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107551190>]}
23:53:00.650531 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
23:53:00.650943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e4767dcb-790c-4775-a7eb-f7c39f50f3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107545340>]}
23:53:00.982293 [debug] [MainThread]: Parsing macros/adapters.sql
23:53:01.004868 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:53:01.009136 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:53:01.009765 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:53:01.012806 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:53:01.029682 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:53:01.034387 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:53:01.038195 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:53:01.043461 [debug] [MainThread]: Parsing macros/adapters.sql
23:53:01.080131 [debug] [MainThread]: Parsing macros/materializations/seed.sql
23:53:01.089808 [debug] [MainThread]: Parsing macros/materializations/view.sql
23:53:01.090378 [debug] [MainThread]: Parsing macros/materializations/table.sql
23:53:01.093051 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
23:53:01.115583 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
23:53:01.120319 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
23:53:01.126061 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
23:53:01.131281 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
23:53:01.134817 [debug] [MainThread]: Parsing macros/materializations/configs.sql
23:53:01.136917 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
23:53:01.138409 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
23:53:01.153747 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
23:53:01.164093 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
23:53:01.174635 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
23:53:01.178569 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
23:53:01.180328 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
23:53:01.181927 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
23:53:01.185831 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
23:53:01.196057 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
23:53:01.197389 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
23:53:01.206377 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
23:53:01.220379 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
23:53:01.226914 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
23:53:01.229491 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
23:53:01.235902 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
23:53:01.237089 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
23:53:01.239442 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
23:53:01.241535 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
23:53:01.246886 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
23:53:01.261676 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
23:53:01.262992 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
23:53:01.265149 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
23:53:01.266546 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
23:53:01.267336 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
23:53:01.267857 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
23:53:01.268480 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
23:53:01.269676 [debug] [MainThread]: Parsing macros/etc/statement.sql
23:53:01.273476 [debug] [MainThread]: Parsing macros/etc/datetime.sql
23:53:01.281141 [debug] [MainThread]: Parsing macros/adapters/schema.sql
23:53:01.283018 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
23:53:01.285342 [debug] [MainThread]: Parsing macros/adapters/relation.sql
23:53:01.294138 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
23:53:01.298167 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
23:53:01.302509 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
23:53:01.309022 [debug] [MainThread]: Parsing macros/adapters/columns.sql
23:53:01.317598 [debug] [MainThread]: Parsing tests/generic/builtin.sql
23:53:01.320226 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
23:53:01.321175 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
23:53:01.322283 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
23:53:01.323279 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
23:53:01.328584 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
23:53:01.329672 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
23:53:01.330806 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
23:53:01.333283 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
23:53:01.334221 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
23:53:01.335829 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
23:53:01.337930 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
23:53:01.346722 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
23:53:01.348340 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
23:53:01.349701 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
23:53:01.350960 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
23:53:01.352400 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
23:53:01.353637 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
23:53:01.354999 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
23:53:01.356103 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
23:53:01.359072 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
23:53:01.363789 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
23:53:01.365166 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
23:53:01.368543 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
23:53:01.370214 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
23:53:01.371583 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
23:53:01.373453 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
23:53:01.396273 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
23:53:01.398100 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
23:53:01.400424 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
23:53:01.401852 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
23:53:01.402878 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
23:53:01.403992 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
23:53:01.405021 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
23:53:01.406101 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
23:53:01.407627 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
23:53:01.409106 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
23:53:01.411160 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
23:53:01.412731 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
23:53:01.413904 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
23:53:01.416193 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
23:53:01.418146 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
23:53:01.419446 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
23:53:01.420584 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
23:53:01.423106 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
23:53:01.424896 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
23:53:01.426619 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
23:53:01.428596 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
23:53:01.431266 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
23:53:01.432568 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
23:53:01.435693 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
23:53:01.444085 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
23:53:01.447870 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
23:53:01.449333 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
23:53:01.452383 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
23:53:01.456203 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
23:53:01.459299 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
23:53:01.460798 [debug] [MainThread]: Parsing macros/sql/star.sql
23:53:01.464846 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
23:53:01.471973 [debug] [MainThread]: Parsing macros/sql/union.sql
23:53:01.481220 [debug] [MainThread]: Parsing macros/sql/groupby.sql
23:53:01.482443 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
23:53:01.485573 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
23:53:01.487049 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
23:53:01.488561 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
23:53:01.494355 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
23:53:01.499107 [debug] [MainThread]: Parsing macros/sql/pivot.sql
23:53:01.502804 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
23:53:01.504879 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
23:53:01.506041 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
23:53:01.511316 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
23:53:01.533615 [debug] [MainThread]: Parsing macros/get_base_dates.sql
23:53:01.538520 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
23:53:01.542339 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
23:53:01.543972 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
23:53:01.544681 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
23:53:01.545224 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
23:53:01.545894 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
23:53:01.546429 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
23:53:01.549319 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
23:53:01.550871 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
23:53:01.551633 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
23:53:01.554123 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
23:53:01.556409 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
23:53:01.557395 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
23:53:01.557932 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
23:53:01.558503 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
23:53:01.559192 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
23:53:01.559712 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
23:53:01.560245 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
23:53:01.561967 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
23:53:01.567093 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
23:53:01.567987 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
23:53:01.569249 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
23:53:01.570082 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
23:53:01.571025 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
23:53:01.571606 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
23:53:01.578086 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
23:53:01.580824 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
23:53:01.581727 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
23:53:01.584392 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
23:53:01.585060 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
23:53:01.586701 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
23:53:01.590828 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
23:53:01.591674 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
23:53:01.594078 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
23:53:01.596408 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
23:53:01.597027 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
23:53:01.597646 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
23:53:01.599266 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
23:53:01.601011 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
23:53:01.601792 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
23:53:01.616942 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
23:53:01.617828 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
23:53:01.626190 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
23:53:01.637019 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
23:53:01.649111 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
23:53:01.659650 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
23:53:01.667187 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
23:53:01.692650 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
23:53:01.705209 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
23:53:01.711266 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
23:53:01.721090 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
23:53:01.732617 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
23:53:01.746069 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
23:53:01.758780 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
23:53:01.797527 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
23:53:01.798341 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
23:53:01.813725 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
23:53:01.814624 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
23:53:01.823179 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
23:53:01.835180 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
23:53:01.847354 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
23:53:01.859581 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
23:53:01.863008 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
23:53:01.864191 [debug] [MainThread]: Parsing macros/staging/stage.sql
23:53:01.874264 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
23:53:01.883427 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
23:53:01.889582 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
23:53:01.892158 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
23:53:01.904303 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
23:53:01.907376 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
23:53:01.909304 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
23:53:01.913969 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
23:53:01.916921 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
23:53:01.918358 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
23:53:01.920425 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
23:53:01.920790 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
23:53:01.924899 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
23:53:01.933478 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
23:53:01.948391 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
23:53:01.953212 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
23:53:01.954190 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
23:53:01.979021 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
23:53:01.981286 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
23:53:01.986330 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
23:53:01.993527 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
23:53:01.996694 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
23:53:02.000224 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
23:53:02.007167 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
23:53:02.017744 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
23:53:02.021883 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
23:53:02.027010 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
23:53:02.031719 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
23:53:02.032759 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
23:53:02.034323 [debug] [MainThread]: Parsing macros/supporting/hash.sql
23:53:02.055516 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
23:53:02.060694 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
23:53:02.062686 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
23:53:02.850486 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
23:53:02.860466 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
23:53:02.862778 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
23:53:02.865279 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:53:02.980993 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:53:03.103244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4767dcb-790c-4775-a7eb-f7c39f50f3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107615fd0>]}
23:53:03.118872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4767dcb-790c-4775-a7eb-f7c39f50f3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107559370>]}
23:53:03.119181 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:53:03.120369 [info ] [MainThread]: 
23:53:03.120878 [debug] [MainThread]: Acquiring new databricks connection "master"
23:53:03.121970 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:53:03.131941 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:53:03.132211 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:53:03.132391 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:53:03.132555 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:53:04.032973 [debug] [ThreadPool]: SQL status: OK in 0.9 seconds
23:53:04.392282 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:53:04.392565 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:53:04.392766 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:53:04.688177 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:53:04.688812 [info ] [MainThread]: 
23:53:04.696740 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
23:53:04.697250 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
23:53:04.700012 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
23:53:04.700315 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
23:53:04.704207 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
23:53:04.717574 [debug] [Thread-1  ]: finished collecting timing info
23:53:04.718009 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
23:53:04.718368 [debug] [Thread-1  ]: finished collecting timing info
23:53:04.719042 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
23:53:04.720378 [debug] [MainThread]: Connection 'master' was properly closed.
23:53:04.720677 [debug] [MainThread]: Connection 'model.test_dbx.raw_customer' was properly closed.
23:53:04.774708 [info ] [MainThread]: Done.
23:53:04.775245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107647c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076505e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107650790>]}


============================== 2022-03-22 23:53:38.924263 | 56b1a63b-ab9a-4c20-af83-ac045e10f7f2 ==============================
23:53:38.924263 [info ] [MainThread]: Running with dbt=1.0.3
23:53:38.924965 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:53:38.925303 [debug] [MainThread]: Tracking: tracking
23:53:38.945072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10449b4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044a7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044a7310>]}
23:53:39.297350 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
23:53:39.297833 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:53:39.311794 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:53:39.441115 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:53:39.441561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10479da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104667d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104667a60>]}


============================== 2022-03-22 23:53:54.087055 | e806921a-dd7b-40a6-a4f7-f31e494636c6 ==============================
23:53:54.087055 [info ] [MainThread]: Running with dbt=1.0.3
23:53:54.087796 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:53:54.088168 [debug] [MainThread]: Tracking: tracking
23:53:54.122570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f2b400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f3baf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f3b040>]}
23:53:54.452533 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
23:53:54.452807 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
23:53:54.473316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e806921a-dd7b-40a6-a4f7-f31e494636c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109242ee0>]}
23:53:54.490433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e806921a-dd7b-40a6-a4f7-f31e494636c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fa310>]}
23:53:54.490801 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:53:54.492170 [info ] [MainThread]: 
23:53:54.492714 [debug] [MainThread]: Acquiring new databricks connection "master"
23:53:54.493800 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:53:54.505014 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:53:54.505314 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:53:54.505505 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:53:54.505683 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:53:55.242245 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
23:53:55.601165 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:53:55.601419 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:53:55.601599 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:53:55.789518 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:53:55.790130 [info ] [MainThread]: 
23:53:55.797158 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
23:53:55.797972 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
23:53:55.800733 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
23:53:55.801022 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
23:53:55.840271 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:53:55.840542 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
23:53:55.840737 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:53:55.840903 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:53:56.137410 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:53:56.137838 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#529, data_type#530, comment#531]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#529, data_type#530, comment#531]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

23:53:56.138359 [debug] [Thread-1  ]: Databricks adapter: Error while running:
macro get_columns_in_relation
23:53:56.138637 [debug] [Thread-1  ]: Databricks adapter: Runtime Error
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
  'DescribeRelation true, [col_name#529, data_type#530, comment#531]
  +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:53:56.141103 [debug] [Thread-1  ]: finished collecting timing info
23:53:56.141430 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
23:53:56.141667 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:53:56.141901 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
23:53:56.336482 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Runtime Error
    Query execution failed.
    Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
    'DescribeRelation true, [col_name#529, data_type#530, comment#531]
    +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:53:56.337154 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
23:53:56.337905 [debug] [MainThread]: Connection 'master' was properly closed.
23:53:56.338403 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
23:53:56.338906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fa220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090fa3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092f6f10>]}


============================== 2022-03-22 23:54:59.394919 | e3e76ceb-53e2-4dcd-bba8-5cd991593a40 ==============================
23:54:59.394919 [info ] [MainThread]: Running with dbt=1.0.3
23:54:59.395839 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:54:59.396170 [debug] [MainThread]: Tracking: tracking
23:54:59.433012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10adf0d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10adf4fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10adf4130>]}
23:54:59.802816 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
23:54:59.803312 [debug] [MainThread]: Partial parsing: deleted file: test_dbx://models/raw_stage/raw_customer.sql
23:54:59.803571 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:54:59.818045 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:54:59.944611 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:54:59.945051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0f0940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0de460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afbcbb0>]}


============================== 2022-03-22 23:55:41.925583 | 50960f1d-3948-4db5-b8ff-1bc586a18639 ==============================
23:55:41.925583 [info ] [MainThread]: Running with dbt=1.0.3
23:55:41.926635 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:55:41.926966 [debug] [MainThread]: Tracking: tracking
23:55:41.948539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a710640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a71c0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a71c070>]}
23:55:42.384693 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
23:55:42.385191 [debug] [MainThread]: Partial parsing: deleted file: test_dbx://models/raw_stage/raw_customer.sql
23:55:42.385460 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:55:42.400121 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:55:42.547028 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:55:42.547481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa128e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8dfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8df850>]}


============================== 2022-03-22 23:56:36.336074 | d42aa09b-2f7a-4826-94af-5cde5fa2b570 ==============================
23:56:36.336074 [info ] [MainThread]: Running with dbt=1.0.3
23:56:36.336984 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:56:36.337255 [debug] [MainThread]: Tracking: tracking
23:56:36.377583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10677d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106789760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106789220>]}
23:56:36.728037 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
23:56:36.728497 [debug] [MainThread]: Partial parsing: deleted file: test_dbx://models/raw_stage/raw_customer.sql
23:56:36.728762 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:56:36.743457 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:56:36.870362 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:56:36.870795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a7f940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a6d460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10694bbb0>]}


============================== 2022-03-22 23:57:36.059246 | be556e36-084a-4eb4-b062-d2bc2a45678c ==============================
23:57:36.059246 [info ] [MainThread]: Running with dbt=1.0.3
23:57:36.060442 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=None, exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:57:36.060927 [debug] [MainThread]: Tracking: tracking
23:57:36.081782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe624f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe70fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe70310>]}
23:57:36.513703 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
23:57:36.514245 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:57:36.528911 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:57:36.680647 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:57:36.681119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110168a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11002ed00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11002ea60>]}


============================== 2022-03-22 23:57:49.856453 | c5a2a336-914f-41c4-9762-3dd4f482f061 ==============================
23:57:49.856453 [info ] [MainThread]: Running with dbt=1.0.3
23:57:49.857558 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['raw_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:57:49.857998 [debug] [MainThread]: Tracking: tracking
23:57:49.879877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a154f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a1efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a1e310>]}
23:57:50.171784 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
23:57:50.172298 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
23:57:50.187298 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
23:57:50.360626 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
23:57:50.361168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e31a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cfcd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cfca60>]}


============================== 2022-03-22 23:58:08.026404 | 672db2ed-d2b7-4791-aea2-156848fd394f ==============================
23:58:08.026404 [info ] [MainThread]: Running with dbt=1.0.3
23:58:08.027188 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['raw_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:58:08.027491 [debug] [MainThread]: Tracking: tracking
23:58:08.061421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103dd94f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103de5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103de5310>]}
23:58:08.376414 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
23:58:08.376712 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
23:58:08.401729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '672db2ed-d2b7-4791-aea2-156848fd394f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040ecf10>]}
23:58:08.422311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '672db2ed-d2b7-4791-aea2-156848fd394f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa5340>]}
23:58:08.422702 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:58:08.424222 [info ] [MainThread]: 
23:58:08.424796 [debug] [MainThread]: Acquiring new databricks connection "master"
23:58:08.426127 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:58:08.439681 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:58:08.439991 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:58:08.440194 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:58:08.440392 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:58:09.278059 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
23:58:09.576648 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:58:09.576929 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:58:09.577115 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:58:09.828260 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:58:09.828907 [info ] [MainThread]: 
23:58:09.852311 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
23:58:09.852907 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
23:58:09.855776 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
23:58:09.856050 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
23:58:09.859636 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
23:58:09.863036 [debug] [Thread-1  ]: finished collecting timing info
23:58:09.863341 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
23:58:09.863570 [debug] [Thread-1  ]: finished collecting timing info
23:58:09.864018 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
23:58:09.865024 [debug] [MainThread]: Connection 'master' was properly closed.
23:58:09.865260 [debug] [MainThread]: Connection 'model.test_dbx.raw_customer' was properly closed.
23:58:09.900316 [info ] [MainThread]: Done.
23:58:09.900906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa5670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa52b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa54c0>]}


============================== 2022-03-22 23:58:25.465740 | b8bbc96e-dcf5-42dc-8ac7-ccc4ae773f62 ==============================
23:58:25.465740 [info ] [MainThread]: Running with dbt=1.0.3
23:58:25.466664 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
23:58:25.467019 [debug] [MainThread]: Tracking: tracking
23:58:25.500033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10743cdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10744b070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10744b0a0>]}
23:58:25.784122 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
23:58:25.784404 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
23:58:25.801462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b8bbc96e-dcf5-42dc-8ac7-ccc4ae773f62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10774c0d0>]}
23:58:25.817279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b8bbc96e-dcf5-42dc-8ac7-ccc4ae773f62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10760c1f0>]}
23:58:25.817630 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:58:25.818994 [info ] [MainThread]: 
23:58:25.819536 [debug] [MainThread]: Acquiring new databricks connection "master"
23:58:25.820876 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:58:25.831764 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:58:25.832032 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:58:25.832214 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:58:25.832386 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:58:26.687048 [debug] [ThreadPool]: SQL status: OK in 0.85 seconds
23:58:26.971196 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:58:26.971546 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:58:26.971784 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:58:27.162464 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:58:27.163072 [info ] [MainThread]: 
23:58:27.169957 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
23:58:27.170743 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
23:58:27.173358 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
23:58:27.173615 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
23:58:27.215598 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:58:27.215888 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
23:58:27.216075 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:58:27.216250 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:58:27.528905 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
23:58:27.529333 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#582, data_type#583, comment#584]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
'DescribeRelation true, [col_name#582, data_type#583, comment#584]
+- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:116)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

23:58:27.529879 [debug] [Thread-1  ]: Databricks adapter: Error while running:
macro get_columns_in_relation
23:58:27.530170 [debug] [Thread-1  ]: Databricks adapter: Runtime Error
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
  'DescribeRelation true, [col_name#582, data_type#583, comment#584]
  +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:58:27.547752 [debug] [Thread-1  ]: finished collecting timing info
23:58:27.548198 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
23:58:27.548485 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:58:27.548753 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
23:58:27.738896 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Runtime Error
    Query execution failed.
    Error message: org.apache.spark.sql.AnalysisException: Table or view not found for 'DESCRIBE TABLE': test_dbt.raw_customer; line 3 pos 6;
    'DescribeRelation true, [col_name#582, data_type#583, comment#584]
    +- 'UnresolvedTableOrView [test_dbt, raw_customer], DESCRIBE TABLE, true
23:58:27.739503 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
23:58:27.740238 [debug] [MainThread]: Connection 'master' was properly closed.
23:58:27.740559 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
23:58:27.740964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10760c3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076150a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107806f10>]}


============================== 2022-03-23 00:00:09.106962 | 42b73241-c38d-4969-8303-fcd14c4046ba ==============================
00:00:09.106962 [info ] [MainThread]: Running with dbt=1.0.3
00:00:09.108234 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
00:00:09.108609 [debug] [MainThread]: Tracking: tracking
00:00:09.130433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bfbc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bfc070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bfc8e0>]}
00:00:09.485014 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:00:09.485574 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:00:09.500668 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:00:09.627963 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:00:09.628397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ef8a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ee6520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dc5c70>]}


============================== 2022-03-23 00:03:10.157117 | 73ea762e-d1e8-4b5d-a1b3-8c3837376520 ==============================
00:03:10.157117 [info ] [MainThread]: Running with dbt=1.0.3
00:03:10.158421 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:03:10.158745 [debug] [MainThread]: Tracking: tracking
00:03:10.178263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d12070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d251c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d25550>]}
00:03:10.532404 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 2 files changed.
00:03:10.532797 [debug] [MainThread]: Partial parsing: deleted file: test_dbx://models/example/my_second_dbt_model.sql
00:03:10.532978 [debug] [MainThread]: Partial parsing: deleted file: test_dbx://models/example/my_first_dbt_model.sql
00:03:10.533336 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:03:10.533623 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/raw_stage/raw_customer.sql
00:03:10.547488 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:03:10.671717 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:03:10.673397 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
00:03:10.694834 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:03:10.702572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73ea762e-d1e8-4b5d-a1b3-8c3837376520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104100ac0>]}
00:03:10.719346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73ea762e-d1e8-4b5d-a1b3-8c3837376520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fb94c0>]}
00:03:10.719741 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:03:10.721221 [info ] [MainThread]: 
00:03:10.721882 [debug] [MainThread]: Acquiring new databricks connection "master"
00:03:10.723002 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:03:10.733271 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:03:10.733560 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:03:10.733748 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:03:11.410429 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
00:03:11.780814 [debug] [ThreadPool]: On list_schemas: Close
00:03:11.992396 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:03:12.002092 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:03:12.002343 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:03:12.002537 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:03:12.002723 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:03:12.691730 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
00:03:13.080590 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:03:13.081020 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:03:13.081252 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:03:13.294390 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:03:13.294805 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:03:13.295359 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:03:13.295767 [info ] [MainThread]: 
00:03:13.302154 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:03:13.302658 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:03:13.303741 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:03:13.306576 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:03:13.306870 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:03:13.310764 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:03:13.311766 [debug] [Thread-1  ]: finished collecting timing info
00:03:13.312005 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:03:13.338430 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:03:13.344525 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:03:13.344867 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:03:13.345154 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:03:13.345429 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:03:14.635930 [debug] [Thread-1  ]: SQL status: OK in 1.29 seconds
00:03:14.637881 [debug] [Thread-1  ]: finished collecting timing info
00:03:14.638245 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:03:14.638495 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:03:14.638719 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:03:14.836280 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '73ea762e-d1e8-4b5d-a1b3-8c3837376520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104187790>]}
00:03:14.836987 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.53s]
00:03:14.837565 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:03:14.838406 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:03:14.838833 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:03:14.839460 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:03:14.842553 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:03:14.842868 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:03:14.858413 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:03:14.858706 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:03:14.858898 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
00:03:14.859079 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:03:15.208276 [debug] [Thread-1  ]: SQL status: OK in 0.35 seconds
00:03:15.745097 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:03:15.745334 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */

      describe extended test_dbt.raw_customer
  
00:03:15.974220 [debug] [Thread-1  ]: SQL status: OK in 0.23 seconds
00:03:16.344044 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:03:16.345104 [debug] [Thread-1  ]: finished collecting timing info
00:03:16.345331 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:03:16.349015 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:03:16.350078 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:03:16.350286 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM staging

00:03:16.537359 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM staging

00:03:16.537655 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM staging

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM staging

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:03:16.538088 [debug] [Thread-1  ]: finished collecting timing info
00:03:16.538312 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:03:16.538493 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:03:16.538668 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:03:16.738781 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  WITH staging AS (
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
  )
  
  SELECT 
    *,
    TO_DATE(cast(from_utc_timestamp(
          to_utc_timestamp(
      current_timestamp::
      timestamp
  
  , 'UTC'),
          '	Etc/GMT'
          ) as date)) AS LOAD_DATE
  FROM staging
00:03:16.739484 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '73ea762e-d1e8-4b5d-a1b3-8c3837376520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10427a0d0>]}
00:03:16.740079 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 1.90s]
00:03:16.740722 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:03:16.742126 [debug] [MainThread]: Acquiring new databricks connection "master"
00:03:16.742417 [debug] [MainThread]: On master: ROLLBACK
00:03:16.742647 [debug] [MainThread]: Opening a new connection, currently in state init
00:03:16.853841 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:03:16.854224 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:03:16.854479 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:03:16.854775 [debug] [MainThread]: On master: ROLLBACK
00:03:16.855039 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:03:16.855289 [debug] [MainThread]: On master: Close
00:03:17.032880 [info ] [MainThread]: 
00:03:17.033322 [info ] [MainThread]: Finished running 2 view models in 6.31s.
00:03:17.033649 [debug] [MainThread]: Connection 'master' was properly closed.
00:03:17.033884 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:03:17.050369 [info ] [MainThread]: 
00:03:17.050822 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:03:17.051244 [info ] [MainThread]: 
00:03:17.051643 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:03:17.051985 [error] [MainThread]:   Query execution failed.
00:03:17.052343 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:03:17.052648 [error] [MainThread]:   mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
00:03:17.052947 [error] [MainThread]:   
00:03:17.053316 [error] [MainThread]:   == SQL ==
00:03:17.053677 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:03:17.054147 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:03:17.054691 [error] [MainThread]:     
00:03:17.055175 [error] [MainThread]:     as
00:03:17.055644 [error] [MainThread]:       
00:03:17.056107 [error] [MainThread]:   
00:03:17.056429 [error] [MainThread]:   WITH staging AS (
00:03:17.056722 [error] [MainThread]:   -- Generated by dbtvault.
00:03:17.056982 [error] [MainThread]:   
00:03:17.057233 [error] [MainThread]:   
00:03:17.057496 [error] [MainThread]:   
00:03:17.057746 [error] [MainThread]:   WITH source_data AS (
00:03:17.057998 [error] [MainThread]:   
00:03:17.058251 [error] [MainThread]:       SELECT
00:03:17.058502 [error] [MainThread]:   
00:03:17.058750 [error] [MainThread]:       "CUSTOMER_ID",
00:03:17.059085 [error] [MainThread]:       "FIRST_NAME",
00:03:17.059387 [error] [MainThread]:       "LAST_NAME",
00:03:17.059656 [error] [MainThread]:       "EMAIL"
00:03:17.059921 [error] [MainThread]:   
00:03:17.060198 [error] [MainThread]:       FROM test_dbt.raw_customer
00:03:17.060457 [error] [MainThread]:   ),
00:03:17.060744 [error] [MainThread]:   
00:03:17.061003 [error] [MainThread]:   derived_columns AS (
00:03:17.061256 [error] [MainThread]:   
00:03:17.061521 [error] [MainThread]:       SELECT
00:03:17.061763 [error] [MainThread]:   
00:03:17.062008 [error] [MainThread]:       "CUSTOMER_ID",
00:03:17.062251 [error] [MainThread]:       "FIRST_NAME",
00:03:17.062494 [error] [MainThread]:       "LAST_NAME",
00:03:17.062735 [error] [MainThread]:       "EMAIL",
00:03:17.063000 [error] [MainThread]:       'SAP' AS "SOURCE"
00:03:17.063245 [error] [MainThread]:   -------------^^^
00:03:17.063488 [error] [MainThread]:   
00:03:17.063729 [error] [MainThread]:       FROM source_data
00:03:17.063975 [error] [MainThread]:   ),
00:03:17.064228 [error] [MainThread]:   
00:03:17.064494 [error] [MainThread]:   hashed_columns AS (
00:03:17.064737 [error] [MainThread]:   
00:03:17.064981 [error] [MainThread]:       SELECT
00:03:17.065227 [error] [MainThread]:   
00:03:17.065502 [error] [MainThread]:       "CUSTOMER_ID",
00:03:17.065805 [error] [MainThread]:       "FIRST_NAME",
00:03:17.066220 [error] [MainThread]:       "LAST_NAME",
00:03:17.066486 [error] [MainThread]:       "EMAIL",
00:03:17.066897 [error] [MainThread]:       "SOURCE",
00:03:17.067327 [error] [MainThread]:   
00:03:17.067772 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:03:17.068068 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:03:17.068370 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:03:17.068653 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:03:17.069186 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:03:17.069536 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:03:17.069963 [error] [MainThread]:   
00:03:17.070243 [error] [MainThread]:       FROM derived_columns
00:03:17.070517 [error] [MainThread]:   ),
00:03:17.070773 [error] [MainThread]:   
00:03:17.071025 [error] [MainThread]:   columns_to_select AS (
00:03:17.071418 [error] [MainThread]:   
00:03:17.071761 [error] [MainThread]:       SELECT
00:03:17.072051 [error] [MainThread]:   
00:03:17.072545 [error] [MainThread]:       "CUSTOMER_ID",
00:03:17.072872 [error] [MainThread]:       "FIRST_NAME",
00:03:17.073151 [error] [MainThread]:       "LAST_NAME",
00:03:17.073419 [error] [MainThread]:       "EMAIL",
00:03:17.073685 [error] [MainThread]:       "SOURCE",
00:03:17.074099 [error] [MainThread]:       "CUSTOMER_HK",
00:03:17.074392 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:03:17.074759 [error] [MainThread]:   
00:03:17.075156 [error] [MainThread]:       FROM hashed_columns
00:03:17.075500 [error] [MainThread]:   )
00:03:17.075766 [error] [MainThread]:   
00:03:17.076050 [error] [MainThread]:   SELECT * FROM columns_to_select
00:03:17.076339 [error] [MainThread]:   )
00:03:17.076599 [error] [MainThread]:   
00:03:17.076843 [error] [MainThread]:   SELECT 
00:03:17.077114 [error] [MainThread]:     *,
00:03:17.077363 [error] [MainThread]:     TO_DATE(cast(from_utc_timestamp(
00:03:17.077610 [error] [MainThread]:           to_utc_timestamp(
00:03:17.077932 [error] [MainThread]:       current_timestamp::
00:03:17.078195 [error] [MainThread]:       timestamp
00:03:17.078457 [error] [MainThread]:   
00:03:17.078706 [error] [MainThread]:   , 'UTC'),
00:03:17.078954 [error] [MainThread]:           '	Etc/GMT'
00:03:17.079230 [error] [MainThread]:           ) as date)) AS LOAD_DATE
00:03:17.079494 [error] [MainThread]:   FROM staging
00:03:17.079760 [info ] [MainThread]: 
00:03:17.080030 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:03:17.080389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040f39a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ee90a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104271580>]}


============================== 2022-03-23 00:05:57.350645 | 54a5454d-3955-4db4-ac02-f92251d85ec7 ==============================
00:05:57.350645 [info ] [MainThread]: Running with dbt=1.0.3
00:05:57.351396 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:05:57.351756 [debug] [MainThread]: Tracking: tracking
00:05:57.373407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10583f070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058458b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058451c0>]}
00:05:57.781381 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:05:57.781872 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:05:57.796475 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:05:57.922417 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:05:57.943877 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:05:57.951888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54a5454d-3955-4db4-ac02-f92251d85ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c7ff40>]}
00:05:57.966976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54a5454d-3955-4db4-ac02-f92251d85ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a1ad00>]}
00:05:57.967443 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:05:57.969190 [info ] [MainThread]: 
00:05:57.969872 [debug] [MainThread]: Acquiring new databricks connection "master"
00:05:57.971172 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:05:57.981412 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:05:57.981647 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:05:57.981833 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:05:58.650309 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
00:05:59.179453 [debug] [ThreadPool]: On list_schemas: Close
00:05:59.463990 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:05:59.474448 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:05:59.474687 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:05:59.474878 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:05:59.475056 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:06:00.212655 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
00:06:00.653972 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:06:00.654366 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:06:00.654594 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:06:00.839018 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:06:00.839402 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:06:00.839890 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:06:00.840244 [info ] [MainThread]: 
00:06:00.847630 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:06:00.848057 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:06:00.848889 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:06:00.851657 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:06:00.852016 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:06:00.856295 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:06:00.858151 [debug] [Thread-1  ]: finished collecting timing info
00:06:00.858430 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:06:00.883803 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:06:00.894751 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:06:00.895014 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:06:00.895202 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:06:00.895381 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:06:02.473236 [debug] [Thread-1  ]: SQL status: OK in 1.58 seconds
00:06:02.475155 [debug] [Thread-1  ]: finished collecting timing info
00:06:02.475543 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:06:02.475777 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:06:02.476000 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:06:02.686826 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54a5454d-3955-4db4-ac02-f92251d85ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c52640>]}
00:06:02.687554 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.84s]
00:06:02.688142 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:06:02.688899 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:06:02.689289 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:06:02.689927 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:06:02.693299 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:06:02.693621 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:06:02.734537 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:06:02.744839 [debug] [Thread-1  ]: finished collecting timing info
00:06:02.745111 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:06:02.748879 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:06:02.751465 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:06:02.751692 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:06:02.751891 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:06:02.752069 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:06:03.090433 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:06:03.090922 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:06:03.091572 [debug] [Thread-1  ]: finished collecting timing info
00:06:03.091899 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:06:03.092177 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:06:03.092442 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:06:03.318878 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  SELECT 
    *,
    TO_DATE(cast(from_utc_timestamp(
          to_utc_timestamp(
      current_timestamp::
      timestamp
  
  , 'UTC'),
          '	Etc/GMT'
          ) as date)) AS LOAD_DATE
  FROM (
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
  )
00:06:03.319547 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54a5454d-3955-4db4-ac02-f92251d85ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105da8b80>]}
00:06:03.320134 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.63s]
00:06:03.320726 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:06:03.322191 [debug] [MainThread]: Acquiring new databricks connection "master"
00:06:03.322487 [debug] [MainThread]: On master: ROLLBACK
00:06:03.322714 [debug] [MainThread]: Opening a new connection, currently in state init
00:06:03.459505 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:06:03.459946 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:06:03.460236 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:06:03.460535 [debug] [MainThread]: On master: ROLLBACK
00:06:03.460808 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:06:03.461075 [debug] [MainThread]: On master: Close
00:06:03.558670 [info ] [MainThread]: 
00:06:03.559278 [info ] [MainThread]: Finished running 2 view models in 5.59s.
00:06:03.559758 [debug] [MainThread]: Connection 'master' was properly closed.
00:06:03.560040 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:06:03.595208 [info ] [MainThread]: 
00:06:03.595721 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:06:03.596188 [info ] [MainThread]: 
00:06:03.596620 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:06:03.597022 [error] [MainThread]:   Query execution failed.
00:06:03.597405 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:06:03.597756 [error] [MainThread]:   mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
00:06:03.598125 [error] [MainThread]:   
00:06:03.598402 [error] [MainThread]:   == SQL ==
00:06:03.598694 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:06:03.599002 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:06:03.599309 [error] [MainThread]:     
00:06:03.599621 [error] [MainThread]:     as
00:06:03.599986 [error] [MainThread]:       
00:06:03.600295 [error] [MainThread]:   
00:06:03.600643 [error] [MainThread]:   SELECT 
00:06:03.600948 [error] [MainThread]:     *,
00:06:03.601274 [error] [MainThread]:     TO_DATE(cast(from_utc_timestamp(
00:06:03.601634 [error] [MainThread]:           to_utc_timestamp(
00:06:03.601966 [error] [MainThread]:       current_timestamp::
00:06:03.602358 [error] [MainThread]:       timestamp
00:06:03.602988 [error] [MainThread]:   
00:06:03.603375 [error] [MainThread]:   , 'UTC'),
00:06:03.603839 [error] [MainThread]:           '	Etc/GMT'
00:06:03.604237 [error] [MainThread]:           ) as date)) AS LOAD_DATE
00:06:03.604551 [error] [MainThread]:   FROM (
00:06:03.604919 [error] [MainThread]:   -- Generated by dbtvault.
00:06:03.605345 [error] [MainThread]:   
00:06:03.605912 [error] [MainThread]:   
00:06:03.606344 [error] [MainThread]:   
00:06:03.606756 [error] [MainThread]:   WITH source_data AS (
00:06:03.607125 [error] [MainThread]:   
00:06:03.607454 [error] [MainThread]:       SELECT
00:06:03.607834 [error] [MainThread]:   
00:06:03.608240 [error] [MainThread]:       "CUSTOMER_ID",
00:06:03.608543 [error] [MainThread]:       "FIRST_NAME",
00:06:03.608854 [error] [MainThread]:       "LAST_NAME",
00:06:03.609172 [error] [MainThread]:       "EMAIL"
00:06:03.609621 [error] [MainThread]:   
00:06:03.610069 [error] [MainThread]:       FROM test_dbt.raw_customer
00:06:03.610798 [error] [MainThread]:   ),
00:06:03.611291 [error] [MainThread]:   
00:06:03.611599 [error] [MainThread]:   derived_columns AS (
00:06:03.611885 [error] [MainThread]:   
00:06:03.612161 [error] [MainThread]:       SELECT
00:06:03.612432 [error] [MainThread]:   
00:06:03.612700 [error] [MainThread]:       "CUSTOMER_ID",
00:06:03.612994 [error] [MainThread]:       "FIRST_NAME",
00:06:03.613270 [error] [MainThread]:       "LAST_NAME",
00:06:03.613538 [error] [MainThread]:       "EMAIL",
00:06:03.613856 [error] [MainThread]:       'SAP' AS "SOURCE"
00:06:03.614245 [error] [MainThread]:   -------------^^^
00:06:03.614685 [error] [MainThread]:   
00:06:03.615134 [error] [MainThread]:       FROM source_data
00:06:03.615582 [error] [MainThread]:   ),
00:06:03.616033 [error] [MainThread]:   
00:06:03.616485 [error] [MainThread]:   hashed_columns AS (
00:06:03.617016 [error] [MainThread]:   
00:06:03.617494 [error] [MainThread]:       SELECT
00:06:03.617941 [error] [MainThread]:   
00:06:03.618307 [error] [MainThread]:       "CUSTOMER_ID",
00:06:03.618587 [error] [MainThread]:       "FIRST_NAME",
00:06:03.619019 [error] [MainThread]:       "LAST_NAME",
00:06:03.619541 [error] [MainThread]:       "EMAIL",
00:06:03.620053 [error] [MainThread]:       "SOURCE",
00:06:03.621083 [error] [MainThread]:   
00:06:03.621573 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:06:03.622116 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:06:03.622719 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:06:03.623223 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:06:03.623745 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:06:03.624206 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:06:03.624663 [error] [MainThread]:   
00:06:03.625106 [error] [MainThread]:       FROM derived_columns
00:06:03.625554 [error] [MainThread]:   ),
00:06:03.626002 [error] [MainThread]:   
00:06:03.626449 [error] [MainThread]:   columns_to_select AS (
00:06:03.626901 [error] [MainThread]:   
00:06:03.627298 [error] [MainThread]:       SELECT
00:06:03.627768 [error] [MainThread]:   
00:06:03.628229 [error] [MainThread]:       "CUSTOMER_ID",
00:06:03.628681 [error] [MainThread]:       "FIRST_NAME",
00:06:03.629132 [error] [MainThread]:       "LAST_NAME",
00:06:03.629595 [error] [MainThread]:       "EMAIL",
00:06:03.630163 [error] [MainThread]:       "SOURCE",
00:06:03.630714 [error] [MainThread]:       "CUSTOMER_HK",
00:06:03.631640 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:06:03.632103 [error] [MainThread]:   
00:06:03.632562 [error] [MainThread]:       FROM hashed_columns
00:06:03.633053 [error] [MainThread]:   )
00:06:03.633564 [error] [MainThread]:   
00:06:03.634032 [error] [MainThread]:   SELECT * FROM columns_to_select
00:06:03.634426 [error] [MainThread]:   )
00:06:03.634798 [info ] [MainThread]: 
00:06:03.635132 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:06:03.635618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c74340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c742e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105da6a60>]}


============================== 2022-03-23 00:07:26.500744 | edbd38e4-076e-49d6-8e34-ee03b6c4e729 ==============================
00:07:26.500744 [info ] [MainThread]: Running with dbt=1.0.3
00:07:26.501554 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:07:26.501950 [debug] [MainThread]: Tracking: tracking
00:07:26.525430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121ba5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121c8760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121c8fd0>]}
00:07:26.921258 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:07:26.921773 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:07:26.937614 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:07:27.071814 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:07:27.093385 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:07:27.101680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'edbd38e4-076e-49d6-8e34-ee03b6c4e729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125fff10>]}
00:07:27.116815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'edbd38e4-076e-49d6-8e34-ee03b6c4e729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112397cd0>]}
00:07:27.117190 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:07:27.118931 [info ] [MainThread]: 
00:07:27.119619 [debug] [MainThread]: Acquiring new databricks connection "master"
00:07:27.121006 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:07:27.131597 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:07:27.131851 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:07:27.132033 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:07:27.558368 [debug] [ThreadPool]: SQL status: OK in 0.43 seconds
00:07:27.854061 [debug] [ThreadPool]: On list_schemas: Close
00:07:28.046746 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:07:28.056331 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:07:28.056599 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:07:28.056788 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:07:28.056963 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:07:28.795599 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
00:07:29.184743 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:07:29.185154 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:07:29.185392 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:07:29.375424 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:07:29.375836 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:07:29.376409 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:07:29.376917 [info ] [MainThread]: 
00:07:29.397063 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:07:29.397524 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:07:29.398374 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:07:29.400960 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:07:29.401216 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:07:29.405642 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:07:29.406680 [debug] [Thread-1  ]: finished collecting timing info
00:07:29.406924 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:07:29.431886 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:07:29.442195 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:07:29.442467 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:07:29.442653 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:07:29.442826 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:07:30.629443 [debug] [Thread-1  ]: SQL status: OK in 1.19 seconds
00:07:30.631439 [debug] [Thread-1  ]: finished collecting timing info
00:07:30.631794 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:07:30.632095 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:07:30.632310 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:07:30.853165 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'edbd38e4-076e-49d6-8e34-ee03b6c4e729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125d0610>]}
00:07:30.853881 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.46s]
00:07:30.854473 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:07:30.855309 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:07:30.855862 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:07:30.856581 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:07:30.859631 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:07:30.859908 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:07:30.901571 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:07:30.902451 [debug] [Thread-1  ]: finished collecting timing info
00:07:30.902681 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:07:30.906491 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:07:30.907532 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:07:30.907733 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:07:30.907911 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SAP" AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:07:30.908089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:07:31.230463 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SAP" AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:07:31.230878 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SAP" AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SAP" AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:07:31.231409 [debug] [Thread-1  ]: finished collecting timing info
00:07:31.231673 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:07:31.231895 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:07:31.232103 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:07:31.465770 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  SELECT 
    *,
    TO_DATE(cast(from_utc_timestamp(
          to_utc_timestamp(
      current_timestamp::
      timestamp
  
  , 'UTC'),
          '	Etc/GMT'
          ) as date)) AS LOAD_DATE
  FROM (
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SAP" AS "SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
  )
00:07:31.466456 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'edbd38e4-076e-49d6-8e34-ee03b6c4e729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11259caf0>]}
00:07:31.466966 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.61s]
00:07:31.467449 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:07:31.468853 [debug] [MainThread]: Acquiring new databricks connection "master"
00:07:31.469145 [debug] [MainThread]: On master: ROLLBACK
00:07:31.469387 [debug] [MainThread]: Opening a new connection, currently in state init
00:07:31.592512 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:07:31.592946 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:07:31.593236 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:07:31.593537 [debug] [MainThread]: On master: ROLLBACK
00:07:31.593809 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:07:31.594090 [debug] [MainThread]: On master: Close
00:07:31.697099 [info ] [MainThread]: 
00:07:31.697713 [info ] [MainThread]: Finished running 2 view models in 4.58s.
00:07:31.698193 [debug] [MainThread]: Connection 'master' was properly closed.
00:07:31.698469 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:07:31.719979 [info ] [MainThread]: 
00:07:31.720437 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:07:31.720927 [info ] [MainThread]: 
00:07:31.739563 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:07:31.740296 [error] [MainThread]:   Query execution failed.
00:07:31.740698 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:07:31.741206 [error] [MainThread]:   mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
00:07:31.741640 [error] [MainThread]:   
00:07:31.741964 [error] [MainThread]:   == SQL ==
00:07:31.742247 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:07:31.742528 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:07:31.742799 [error] [MainThread]:     
00:07:31.743095 [error] [MainThread]:     as
00:07:31.743388 [error] [MainThread]:       
00:07:31.743681 [error] [MainThread]:   
00:07:31.743948 [error] [MainThread]:   SELECT 
00:07:31.744218 [error] [MainThread]:     *,
00:07:31.744486 [error] [MainThread]:     TO_DATE(cast(from_utc_timestamp(
00:07:31.744752 [error] [MainThread]:           to_utc_timestamp(
00:07:31.745030 [error] [MainThread]:       current_timestamp::
00:07:31.745474 [error] [MainThread]:       timestamp
00:07:31.745892 [error] [MainThread]:   
00:07:31.746186 [error] [MainThread]:   , 'UTC'),
00:07:31.746467 [error] [MainThread]:           '	Etc/GMT'
00:07:31.746759 [error] [MainThread]:           ) as date)) AS LOAD_DATE
00:07:31.747047 [error] [MainThread]:   FROM (
00:07:31.747320 [error] [MainThread]:   -- Generated by dbtvault.
00:07:31.747618 [error] [MainThread]:   
00:07:31.747892 [error] [MainThread]:   
00:07:31.748180 [error] [MainThread]:   
00:07:31.748452 [error] [MainThread]:   WITH source_data AS (
00:07:31.748747 [error] [MainThread]:   
00:07:31.749022 [error] [MainThread]:       SELECT
00:07:31.749295 [error] [MainThread]:   
00:07:31.749665 [error] [MainThread]:       "CUSTOMER_ID",
00:07:31.750029 [error] [MainThread]:       "FIRST_NAME",
00:07:31.750413 [error] [MainThread]:       "LAST_NAME",
00:07:31.750757 [error] [MainThread]:       "EMAIL"
00:07:31.751126 [error] [MainThread]:   
00:07:31.751481 [error] [MainThread]:       FROM test_dbt.raw_customer
00:07:31.751807 [error] [MainThread]:   ),
00:07:31.752157 [error] [MainThread]:   
00:07:31.752543 [error] [MainThread]:   derived_columns AS (
00:07:31.752939 [error] [MainThread]:   
00:07:31.753283 [error] [MainThread]:       SELECT
00:07:31.753638 [error] [MainThread]:   
00:07:31.754085 [error] [MainThread]:       "CUSTOMER_ID",
00:07:31.754585 [error] [MainThread]:       "FIRST_NAME",
00:07:31.754952 [error] [MainThread]:       "LAST_NAME",
00:07:31.755318 [error] [MainThread]:       "EMAIL",
00:07:31.755767 [error] [MainThread]:       "SAP" AS "SOURCE"
00:07:31.756266 [error] [MainThread]:   -------------^^^
00:07:31.756663 [error] [MainThread]:   
00:07:31.756956 [error] [MainThread]:       FROM source_data
00:07:31.757242 [error] [MainThread]:   ),
00:07:31.757530 [error] [MainThread]:   
00:07:31.757825 [error] [MainThread]:   hashed_columns AS (
00:07:31.758120 [error] [MainThread]:   
00:07:31.758391 [error] [MainThread]:       SELECT
00:07:31.758675 [error] [MainThread]:   
00:07:31.759049 [error] [MainThread]:       "CUSTOMER_ID",
00:07:31.759343 [error] [MainThread]:       "FIRST_NAME",
00:07:31.759625 [error] [MainThread]:       "LAST_NAME",
00:07:31.759899 [error] [MainThread]:       "EMAIL",
00:07:31.760198 [error] [MainThread]:       "SOURCE",
00:07:31.760477 [error] [MainThread]:   
00:07:31.760750 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:07:31.761053 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:07:31.761326 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:07:31.761625 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:07:31.761908 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:07:31.762175 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:07:31.762483 [error] [MainThread]:   
00:07:31.762799 [error] [MainThread]:       FROM derived_columns
00:07:31.763066 [error] [MainThread]:   ),
00:07:31.763329 [error] [MainThread]:   
00:07:31.763597 [error] [MainThread]:   columns_to_select AS (
00:07:31.763855 [error] [MainThread]:   
00:07:31.764121 [error] [MainThread]:       SELECT
00:07:31.764395 [error] [MainThread]:   
00:07:31.764660 [error] [MainThread]:       "CUSTOMER_ID",
00:07:31.764919 [error] [MainThread]:       "FIRST_NAME",
00:07:31.765233 [error] [MainThread]:       "LAST_NAME",
00:07:31.765564 [error] [MainThread]:       "EMAIL",
00:07:31.765856 [error] [MainThread]:       "SOURCE",
00:07:31.766164 [error] [MainThread]:       "CUSTOMER_HK",
00:07:31.766487 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:07:31.766773 [error] [MainThread]:   
00:07:31.767172 [error] [MainThread]:       FROM hashed_columns
00:07:31.767499 [error] [MainThread]:   )
00:07:31.767778 [error] [MainThread]:   
00:07:31.768197 [error] [MainThread]:   SELECT * FROM columns_to_select
00:07:31.768504 [error] [MainThread]:   )
00:07:31.768863 [info ] [MainThread]: 
00:07:31.769187 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:07:31.769658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112397610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112480dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112631b80>]}


============================== 2022-03-23 00:09:42.149386 | 6ea987fd-3a70-43c4-9d8e-7343b9f20f90 ==============================
00:09:42.149386 [info ] [MainThread]: Running with dbt=1.0.3
00:09:42.150126 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:09:42.150425 [debug] [MainThread]: Tracking: tracking
00:09:42.174156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1f1d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1e0580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1e0040>]}
00:09:42.567590 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:09:42.568130 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:09:42.583495 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:09:42.717089 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:09:42.742753 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:09:42.750759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6ea987fd-3a70-43c4-9d8e-7343b9f20f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a62bee0>]}
00:09:42.767692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6ea987fd-3a70-43c4-9d8e-7343b9f20f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3bed30>]}
00:09:42.768188 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:09:42.769795 [info ] [MainThread]: 
00:09:42.770436 [debug] [MainThread]: Acquiring new databricks connection "master"
00:09:42.771652 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:09:42.782762 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:09:42.783011 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:09:42.783202 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:09:43.487514 [debug] [ThreadPool]: SQL status: OK in 0.7 seconds
00:09:43.867854 [debug] [ThreadPool]: On list_schemas: Close
00:09:44.064392 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:09:44.075155 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:09:44.075425 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:09:44.075618 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:09:44.075795 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:09:44.987820 [debug] [ThreadPool]: SQL status: OK in 0.91 seconds
00:09:45.288124 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:09:45.288535 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:09:45.288761 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:09:45.500317 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:09:45.500649 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:09:45.501095 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:09:45.501494 [info ] [MainThread]: 
00:09:45.508310 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:09:45.508737 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:09:45.509587 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:09:45.512484 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:09:45.512770 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:09:45.516707 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:09:45.518862 [debug] [Thread-1  ]: finished collecting timing info
00:09:45.519115 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:09:45.546035 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:09:45.547096 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:09:45.547304 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:09:45.547489 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:09:45.547665 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:09:46.691744 [debug] [Thread-1  ]: SQL status: OK in 1.14 seconds
00:09:46.693656 [debug] [Thread-1  ]: finished collecting timing info
00:09:46.694014 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:09:46.694243 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:09:46.694459 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:09:46.954699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ea987fd-3a70-43c4-9d8e-7343b9f20f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5fb5e0>]}
00:09:46.955458 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.45s]
00:09:46.956061 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:09:46.956824 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:09:46.957278 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:09:46.958002 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:09:46.961007 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:09:46.961304 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:09:47.002458 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:09:47.003437 [debug] [Thread-1  ]: finished collecting timing info
00:09:47.003693 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:09:47.007967 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:09:47.008904 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:09:47.009113 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:09:47.009294 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
-- )

-- SELECT 
--   *,
--   TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
-- FROM staging

00:09:47.009468 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:09:47.398371 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
-- )

-- SELECT 
--   *,
--   TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
-- FROM staging

00:09:47.398632 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
-- )

-- SELECT 
--   *,
--   TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
-- FROM staging

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- WITH staging AS (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
-- )

-- SELECT 
--   *,
--   TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
-- FROM staging

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:09:47.399026 [debug] [Thread-1  ]: finished collecting timing info
00:09:47.399223 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:09:47.399385 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:09:47.399541 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:09:47.595025 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- WITH staging AS (
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
  -- )
  
  -- SELECT 
  --   *,
  --   TO_DATE(cast(from_utc_timestamp(
          to_utc_timestamp(
      current_timestamp::
      timestamp
  
  , 'UTC'),
          '	Etc/GMT'
          ) as date)) AS LOAD_DATE
  -- FROM staging
00:09:47.595708 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ea987fd-3a70-43c4-9d8e-7343b9f20f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a621be0>]}
00:09:47.596242 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.64s]
00:09:47.596719 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:09:47.598342 [debug] [MainThread]: Acquiring new databricks connection "master"
00:09:47.598655 [debug] [MainThread]: On master: ROLLBACK
00:09:47.598885 [debug] [MainThread]: Opening a new connection, currently in state init
00:09:47.724925 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:09:47.725374 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:09:47.725666 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:09:47.725965 [debug] [MainThread]: On master: ROLLBACK
00:09:47.726237 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:09:47.726499 [debug] [MainThread]: On master: Close
00:09:47.843483 [info ] [MainThread]: 
00:09:47.843990 [info ] [MainThread]: Finished running 2 view models in 5.07s.
00:09:47.844385 [debug] [MainThread]: Connection 'master' was properly closed.
00:09:47.844648 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:09:47.864311 [info ] [MainThread]: 
00:09:47.864707 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:09:47.865061 [info ] [MainThread]: 
00:09:47.865389 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:09:47.865736 [error] [MainThread]:   Query execution failed.
00:09:47.866057 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:09:47.866354 [error] [MainThread]:   mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
00:09:47.866674 [error] [MainThread]:   
00:09:47.867041 [error] [MainThread]:   == SQL ==
00:09:47.867425 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:09:47.867838 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:09:47.868159 [error] [MainThread]:     
00:09:47.868489 [error] [MainThread]:     as
00:09:47.868822 [error] [MainThread]:       
00:09:47.869191 [error] [MainThread]:   
00:09:47.869494 [error] [MainThread]:   -- WITH staging AS (
00:09:47.869809 [error] [MainThread]:   -- Generated by dbtvault.
00:09:47.870182 [error] [MainThread]:   
00:09:47.870545 [error] [MainThread]:   
00:09:47.871034 [error] [MainThread]:   
00:09:47.871349 [error] [MainThread]:   WITH source_data AS (
00:09:47.871774 [error] [MainThread]:   
00:09:47.872084 [error] [MainThread]:       SELECT
00:09:47.872550 [error] [MainThread]:   
00:09:47.873024 [error] [MainThread]:       "CUSTOMER_ID",
00:09:47.873375 [error] [MainThread]:       "FIRST_NAME",
00:09:47.873847 [error] [MainThread]:       "LAST_NAME",
00:09:47.874228 [error] [MainThread]:       "EMAIL"
00:09:47.874535 [error] [MainThread]:   
00:09:47.874888 [error] [MainThread]:       FROM test_dbt.raw_customer
00:09:47.875190 [error] [MainThread]:   ),
00:09:47.875624 [error] [MainThread]:   
00:09:47.875928 [error] [MainThread]:   derived_columns AS (
00:09:47.876212 [error] [MainThread]:   
00:09:47.876488 [error] [MainThread]:       SELECT
00:09:47.876761 [error] [MainThread]:   
00:09:47.877037 [error] [MainThread]:       "CUSTOMER_ID",
00:09:47.877339 [error] [MainThread]:       "FIRST_NAME",
00:09:47.877616 [error] [MainThread]:       "LAST_NAME",
00:09:47.877917 [error] [MainThread]:       "EMAIL",
00:09:47.878269 [error] [MainThread]:       'SAP' AS "SOURCE"
00:09:47.878568 [error] [MainThread]:   -------------^^^
00:09:47.878855 [error] [MainThread]:   
00:09:47.879133 [error] [MainThread]:       FROM source_data
00:09:47.879418 [error] [MainThread]:   ),
00:09:47.879732 [error] [MainThread]:   
00:09:47.880046 [error] [MainThread]:   hashed_columns AS (
00:09:47.880428 [error] [MainThread]:   
00:09:47.880766 [error] [MainThread]:       SELECT
00:09:47.881058 [error] [MainThread]:   
00:09:47.881335 [error] [MainThread]:       "CUSTOMER_ID",
00:09:47.881612 [error] [MainThread]:       "FIRST_NAME",
00:09:47.881898 [error] [MainThread]:       "LAST_NAME",
00:09:47.882190 [error] [MainThread]:       "EMAIL",
00:09:47.882465 [error] [MainThread]:       "SOURCE",
00:09:47.882743 [error] [MainThread]:   
00:09:47.883014 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:09:47.883317 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:09:47.883590 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:09:47.883959 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:09:47.884283 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:09:47.884727 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:09:47.885049 [error] [MainThread]:   
00:09:47.885368 [error] [MainThread]:       FROM derived_columns
00:09:47.885668 [error] [MainThread]:   ),
00:09:47.885945 [error] [MainThread]:   
00:09:47.886217 [error] [MainThread]:   columns_to_select AS (
00:09:47.886507 [error] [MainThread]:   
00:09:47.886794 [error] [MainThread]:       SELECT
00:09:47.887095 [error] [MainThread]:   
00:09:47.887411 [error] [MainThread]:       "CUSTOMER_ID",
00:09:47.887740 [error] [MainThread]:       "FIRST_NAME",
00:09:47.888094 [error] [MainThread]:       "LAST_NAME",
00:09:47.888549 [error] [MainThread]:       "EMAIL",
00:09:47.888928 [error] [MainThread]:       "SOURCE",
00:09:47.889277 [error] [MainThread]:       "CUSTOMER_HK",
00:09:47.889706 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:09:47.890124 [error] [MainThread]:   
00:09:47.890516 [error] [MainThread]:       FROM hashed_columns
00:09:47.890853 [error] [MainThread]:   )
00:09:47.891152 [error] [MainThread]:   
00:09:47.891552 [error] [MainThread]:   SELECT * FROM columns_to_select
00:09:47.891906 [error] [MainThread]:   -- )
00:09:47.892243 [error] [MainThread]:   
00:09:47.892534 [error] [MainThread]:   -- SELECT 
00:09:47.892814 [error] [MainThread]:   --   *,
00:09:47.893160 [error] [MainThread]:   --   TO_DATE(cast(from_utc_timestamp(
00:09:47.893533 [error] [MainThread]:           to_utc_timestamp(
00:09:47.893828 [error] [MainThread]:       current_timestamp::
00:09:47.894119 [error] [MainThread]:       timestamp
00:09:47.894399 [error] [MainThread]:   
00:09:47.894694 [error] [MainThread]:   , 'UTC'),
00:09:47.894984 [error] [MainThread]:           '	Etc/GMT'
00:09:47.895272 [error] [MainThread]:           ) as date)) AS LOAD_DATE
00:09:47.895551 [error] [MainThread]:   -- FROM staging
00:09:47.895833 [info ] [MainThread]: 
00:09:47.896118 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:09:47.896507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a63eaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a63efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a61f880>]}


============================== 2022-03-23 00:10:39.052305 | 498d43d7-704f-4d0a-b2f3-bdcd638cae2f ==============================
00:10:39.052305 [info ] [MainThread]: Running with dbt=1.0.3
00:10:39.053061 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:10:39.053573 [debug] [MainThread]: Tracking: tracking
00:10:39.076282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10725d070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10726c1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10726c2b0>]}
00:10:39.428412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:10:39.428903 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:10:39.444105 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:10:39.568323 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:10:39.592970 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:10:39.600797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '498d43d7-704f-4d0a-b2f3-bdcd638cae2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10760a0d0>]}
00:10:39.617332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '498d43d7-704f-4d0a-b2f3-bdcd638cae2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107433d00>]}
00:10:39.617728 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:10:39.619318 [info ] [MainThread]: 
00:10:39.620019 [debug] [MainThread]: Acquiring new databricks connection "master"
00:10:39.621421 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:10:39.633196 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:10:39.633461 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:10:39.633650 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:10:39.996655 [debug] [ThreadPool]: SQL status: OK in 0.36 seconds
00:10:40.326298 [debug] [ThreadPool]: On list_schemas: Close
00:10:40.503716 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:10:40.513462 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:10:40.513711 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:10:40.513891 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:10:40.514063 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:10:41.205529 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
00:10:41.509678 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:10:41.510014 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:10:41.510247 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:10:41.702187 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:10:41.702515 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:10:41.702998 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:10:41.703431 [info ] [MainThread]: 
00:10:41.710506 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:10:41.710958 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:10:41.712087 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:10:41.715007 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:10:41.715315 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:10:41.719563 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:10:41.720751 [debug] [Thread-1  ]: finished collecting timing info
00:10:41.721181 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:10:41.747967 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:10:41.749111 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:10:41.749336 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:10:41.749549 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:10:41.749741 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:10:42.914953 [debug] [Thread-1  ]: SQL status: OK in 1.17 seconds
00:10:42.931639 [debug] [Thread-1  ]: finished collecting timing info
00:10:42.932061 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:10:42.932354 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:10:42.932634 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:10:43.125354 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '498d43d7-704f-4d0a-b2f3-bdcd638cae2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10767bd30>]}
00:10:43.125787 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.41s]
00:10:43.126145 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:10:43.126641 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:10:43.127113 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:10:43.127596 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:10:43.129924 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:10:43.130159 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:10:43.165544 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:10:43.166453 [debug] [Thread-1  ]: finished collecting timing info
00:10:43.166658 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:10:43.170416 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:10:43.171645 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:10:43.171915 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:10:43.172148 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    


-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:10:43.172306 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:10:43.472963 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    


-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:10:43.473294 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    


-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    


-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:10:43.473810 [debug] [Thread-1  ]: finished collecting timing info
00:10:43.474069 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:10:43.474283 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:10:43.474491 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:10:43.653552 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
00:10:43.654135 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '498d43d7-704f-4d0a-b2f3-bdcd638cae2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10768ad30>]}
00:10:43.654605 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.53s]
00:10:43.655072 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:10:43.656505 [debug] [MainThread]: Acquiring new databricks connection "master"
00:10:43.656807 [debug] [MainThread]: On master: ROLLBACK
00:10:43.657032 [debug] [MainThread]: Opening a new connection, currently in state init
00:10:43.775447 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:10:43.775818 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:10:43.776048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:10:43.776279 [debug] [MainThread]: On master: ROLLBACK
00:10:43.776487 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:10:43.776692 [debug] [MainThread]: On master: Close
00:10:43.879498 [info ] [MainThread]: 
00:10:43.880107 [info ] [MainThread]: Finished running 2 view models in 4.26s.
00:10:43.880594 [debug] [MainThread]: Connection 'master' was properly closed.
00:10:43.880812 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:10:43.914712 [info ] [MainThread]: 
00:10:43.915267 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:10:43.915784 [info ] [MainThread]: 
00:10:43.916236 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:10:43.916653 [error] [MainThread]:   Query execution failed.
00:10:43.916999 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:10:43.917532 [error] [MainThread]:   mismatched input '"SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 13)
00:10:43.917889 [error] [MainThread]:   
00:10:43.918411 [error] [MainThread]:   == SQL ==
00:10:43.918766 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:10:43.919183 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:10:43.919520 [error] [MainThread]:     
00:10:43.919845 [error] [MainThread]:     as
00:10:43.920141 [error] [MainThread]:       
00:10:43.920441 [error] [MainThread]:   
00:10:43.920770 [error] [MainThread]:   
00:10:43.921230 [error] [MainThread]:   -- Generated by dbtvault.
00:10:43.921642 [error] [MainThread]:   
00:10:43.922057 [error] [MainThread]:   
00:10:43.922439 [error] [MainThread]:   
00:10:43.922790 [error] [MainThread]:   WITH source_data AS (
00:10:43.923228 [error] [MainThread]:   
00:10:43.923679 [error] [MainThread]:       SELECT
00:10:43.924027 [error] [MainThread]:   
00:10:43.924307 [error] [MainThread]:       "CUSTOMER_ID",
00:10:43.924625 [error] [MainThread]:       "FIRST_NAME",
00:10:43.924895 [error] [MainThread]:       "LAST_NAME",
00:10:43.925185 [error] [MainThread]:       "EMAIL"
00:10:43.925451 [error] [MainThread]:   
00:10:43.925729 [error] [MainThread]:       FROM test_dbt.raw_customer
00:10:43.926036 [error] [MainThread]:   ),
00:10:43.926303 [error] [MainThread]:   
00:10:43.926604 [error] [MainThread]:   derived_columns AS (
00:10:43.926909 [error] [MainThread]:   
00:10:43.927238 [error] [MainThread]:       SELECT
00:10:43.927501 [error] [MainThread]:   
00:10:43.927760 [error] [MainThread]:       "CUSTOMER_ID",
00:10:43.928015 [error] [MainThread]:       "FIRST_NAME",
00:10:43.928314 [error] [MainThread]:       "LAST_NAME",
00:10:43.928685 [error] [MainThread]:       "EMAIL",
00:10:43.928975 [error] [MainThread]:       'SAP' AS "SOURCE"
00:10:43.929260 [error] [MainThread]:   -------------^^^
00:10:43.929638 [error] [MainThread]:   
00:10:43.929981 [error] [MainThread]:       FROM source_data
00:10:43.930261 [error] [MainThread]:   ),
00:10:43.930533 [error] [MainThread]:   
00:10:43.930803 [error] [MainThread]:   hashed_columns AS (
00:10:43.931053 [error] [MainThread]:   
00:10:43.931385 [error] [MainThread]:       SELECT
00:10:43.931688 [error] [MainThread]:   
00:10:43.931972 [error] [MainThread]:       "CUSTOMER_ID",
00:10:43.932247 [error] [MainThread]:       "FIRST_NAME",
00:10:43.932534 [error] [MainThread]:       "LAST_NAME",
00:10:43.932870 [error] [MainThread]:       "EMAIL",
00:10:43.933162 [error] [MainThread]:       "SOURCE",
00:10:43.933418 [error] [MainThread]:   
00:10:43.933688 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:10:43.933936 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:10:43.934202 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:10:43.934465 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:10:43.934723 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:10:43.934978 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:10:43.935217 [error] [MainThread]:   
00:10:43.935589 [error] [MainThread]:       FROM derived_columns
00:10:43.935858 [error] [MainThread]:   ),
00:10:43.936158 [error] [MainThread]:   
00:10:43.936414 [error] [MainThread]:   columns_to_select AS (
00:10:43.936663 [error] [MainThread]:   
00:10:43.936938 [error] [MainThread]:       SELECT
00:10:43.937188 [error] [MainThread]:   
00:10:43.937478 [error] [MainThread]:       "CUSTOMER_ID",
00:10:43.937914 [error] [MainThread]:       "FIRST_NAME",
00:10:43.938446 [error] [MainThread]:       "LAST_NAME",
00:10:43.938743 [error] [MainThread]:       "EMAIL",
00:10:43.939203 [error] [MainThread]:       "SOURCE",
00:10:43.939635 [error] [MainThread]:       "CUSTOMER_HK",
00:10:43.939949 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:10:43.940224 [error] [MainThread]:   
00:10:43.940631 [error] [MainThread]:       FROM hashed_columns
00:10:43.941023 [error] [MainThread]:   )
00:10:43.941447 [error] [MainThread]:   
00:10:43.941789 [error] [MainThread]:   SELECT * FROM columns_to_select
00:10:43.942186 [info ] [MainThread]: 
00:10:43.942600 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:10:43.943138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076bc730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076bccd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107649f10>]}


============================== 2022-03-23 00:12:32.877141 | b1ae3f19-3440-4b01-b4d1-4a320195ccfa ==============================
00:12:32.877141 [info ] [MainThread]: Running with dbt=1.0.3
00:12:32.878221 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:12:32.878491 [debug] [MainThread]: Tracking: tracking
00:12:32.912565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db34f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dba220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dba670>]}
00:12:33.269931 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:12:33.270494 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:12:33.286473 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:12:33.412679 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:12:33.421024 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:12:33.426764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b1ae3f19-3440-4b01-b4d1-4a320195ccfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1f0f10>]}
00:12:33.458696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b1ae3f19-3440-4b01-b4d1-4a320195ccfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f84cd0>]}
00:12:33.459294 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:12:33.461373 [info ] [MainThread]: 
00:12:33.462046 [debug] [MainThread]: Acquiring new databricks connection "master"
00:12:33.463306 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:12:33.474589 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:12:33.474890 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:12:33.475088 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:12:33.941572 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
00:12:34.233993 [debug] [ThreadPool]: On list_schemas: Close
00:12:34.464032 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:12:34.474402 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:12:34.474656 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:12:34.474851 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:12:34.475036 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:12:35.211178 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
00:12:35.501488 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:12:35.501903 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:12:35.502133 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:12:35.680565 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:12:35.680957 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:12:35.681507 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:12:35.682004 [info ] [MainThread]: 
00:12:35.688330 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:12:35.688971 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:12:35.689547 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:12:35.692268 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:12:35.692516 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:12:35.696365 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:12:35.707468 [debug] [Thread-1  ]: finished collecting timing info
00:12:35.707791 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:12:35.732776 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:12:35.733962 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:12:35.734196 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:12:35.734379 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:12:35.734551 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:12:37.158588 [debug] [Thread-1  ]: SQL status: OK in 1.42 seconds
00:12:37.159942 [debug] [Thread-1  ]: finished collecting timing info
00:12:37.160193 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:12:37.160375 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:12:37.160552 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:12:37.367309 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b1ae3f19-3440-4b01-b4d1-4a320195ccfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f84520>]}
00:12:37.368079 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.68s]
00:12:37.368696 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:12:37.369557 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:12:37.369922 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:12:37.370512 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:12:37.373524 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:12:37.373802 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:12:37.413199 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:12:37.414118 [debug] [Thread-1  ]: finished collecting timing info
00:12:37.414328 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:12:37.417788 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:12:37.419432 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:12:37.419765 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:12:37.420009 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:12:37.420206 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:12:37.709697 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

00:12:37.710045 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

SELECT 
  *,
  TO_DATE(cast(from_utc_timestamp(
        to_utc_timestamp(
    current_timestamp::
    timestamp

, 'UTC'),
        '	Etc/GMT'
        ) as date)) AS LOAD_DATE
FROM (
-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select
)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:12:37.710572 [debug] [Thread-1  ]: finished collecting timing info
00:12:37.710838 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:12:37.711063 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:12:37.711280 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:12:37.898733 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  SELECT 
    *,
    TO_DATE(cast(from_utc_timestamp(
          to_utc_timestamp(
      current_timestamp::
      timestamp
  
  , 'UTC'),
          '	Etc/GMT'
          ) as date)) AS LOAD_DATE
  FROM (
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "RECORD_SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
  )
00:12:37.899418 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b1ae3f19-3440-4b01-b4d1-4a320195ccfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1aeb50>]}
00:12:37.899994 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.53s]
00:12:37.900584 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:12:37.902131 [debug] [MainThread]: Acquiring new databricks connection "master"
00:12:37.902449 [debug] [MainThread]: On master: ROLLBACK
00:12:37.902690 [debug] [MainThread]: Opening a new connection, currently in state init
00:12:38.028842 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:12:38.029289 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:12:38.029575 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:12:38.029879 [debug] [MainThread]: On master: ROLLBACK
00:12:38.030147 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:12:38.030410 [debug] [MainThread]: On master: Close
00:12:38.122624 [info ] [MainThread]: 
00:12:38.123046 [info ] [MainThread]: Finished running 2 view models in 4.66s.
00:12:38.123375 [debug] [MainThread]: Connection 'master' was properly closed.
00:12:38.123561 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:12:38.157351 [info ] [MainThread]: 
00:12:38.157742 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:12:38.158085 [info ] [MainThread]: 
00:12:38.158414 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:12:38.158728 [error] [MainThread]:   Query execution failed.
00:12:38.159070 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:12:38.159344 [error] [MainThread]:   mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 13)
00:12:38.159797 [error] [MainThread]:   
00:12:38.160177 [error] [MainThread]:   == SQL ==
00:12:38.160466 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:12:38.160893 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:12:38.161204 [error] [MainThread]:     
00:12:38.161480 [error] [MainThread]:     as
00:12:38.161750 [error] [MainThread]:       
00:12:38.162028 [error] [MainThread]:   
00:12:38.162326 [error] [MainThread]:   SELECT 
00:12:38.162593 [error] [MainThread]:     *,
00:12:38.162855 [error] [MainThread]:     TO_DATE(cast(from_utc_timestamp(
00:12:38.163225 [error] [MainThread]:           to_utc_timestamp(
00:12:38.163600 [error] [MainThread]:       current_timestamp::
00:12:38.163881 [error] [MainThread]:       timestamp
00:12:38.164217 [error] [MainThread]:   
00:12:38.164643 [error] [MainThread]:   , 'UTC'),
00:12:38.164944 [error] [MainThread]:           '	Etc/GMT'
00:12:38.165222 [error] [MainThread]:           ) as date)) AS LOAD_DATE
00:12:38.165491 [error] [MainThread]:   FROM (
00:12:38.165785 [error] [MainThread]:   -- Generated by dbtvault.
00:12:38.166160 [error] [MainThread]:   
00:12:38.166458 [error] [MainThread]:   
00:12:38.166727 [error] [MainThread]:   
00:12:38.167009 [error] [MainThread]:   WITH source_data AS (
00:12:38.167286 [error] [MainThread]:   
00:12:38.167560 [error] [MainThread]:       SELECT
00:12:38.167998 [error] [MainThread]:   
00:12:38.168359 [error] [MainThread]:       "CUSTOMER_ID",
00:12:38.168658 [error] [MainThread]:       "FIRST_NAME",
00:12:38.168974 [error] [MainThread]:       "LAST_NAME",
00:12:38.169256 [error] [MainThread]:       "EMAIL"
00:12:38.169616 [error] [MainThread]:   
00:12:38.170057 [error] [MainThread]:       FROM test_dbt.raw_customer
00:12:38.170411 [error] [MainThread]:   ),
00:12:38.170710 [error] [MainThread]:   
00:12:38.170996 [error] [MainThread]:   derived_columns AS (
00:12:38.171348 [error] [MainThread]:   
00:12:38.171724 [error] [MainThread]:       SELECT
00:12:38.172067 [error] [MainThread]:   
00:12:38.172476 [error] [MainThread]:       "CUSTOMER_ID",
00:12:38.172959 [error] [MainThread]:       "FIRST_NAME",
00:12:38.173436 [error] [MainThread]:       "LAST_NAME",
00:12:38.173960 [error] [MainThread]:       "EMAIL",
00:12:38.174433 [error] [MainThread]:       'SAP' AS "RECORD_SOURCE"
00:12:38.174753 [error] [MainThread]:   -------------^^^
00:12:38.175054 [error] [MainThread]:   
00:12:38.175356 [error] [MainThread]:       FROM source_data
00:12:38.175648 [error] [MainThread]:   ),
00:12:38.175983 [error] [MainThread]:   
00:12:38.176306 [error] [MainThread]:   hashed_columns AS (
00:12:38.176582 [error] [MainThread]:   
00:12:38.176889 [error] [MainThread]:       SELECT
00:12:38.177171 [error] [MainThread]:   
00:12:38.177522 [error] [MainThread]:       "CUSTOMER_ID",
00:12:38.177868 [error] [MainThread]:       "FIRST_NAME",
00:12:38.178194 [error] [MainThread]:       "LAST_NAME",
00:12:38.178509 [error] [MainThread]:       "EMAIL",
00:12:38.178828 [error] [MainThread]:       "RECORD_SOURCE",
00:12:38.179111 [error] [MainThread]:   
00:12:38.179396 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:12:38.179721 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:12:38.179996 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:12:38.180306 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:12:38.180575 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:12:38.180865 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:12:38.181157 [error] [MainThread]:   
00:12:38.181498 [error] [MainThread]:       FROM derived_columns
00:12:38.181771 [error] [MainThread]:   ),
00:12:38.182053 [error] [MainThread]:   
00:12:38.182372 [error] [MainThread]:   columns_to_select AS (
00:12:38.182657 [error] [MainThread]:   
00:12:38.182961 [error] [MainThread]:       SELECT
00:12:38.183253 [error] [MainThread]:   
00:12:38.183583 [error] [MainThread]:       "CUSTOMER_ID",
00:12:38.183852 [error] [MainThread]:       "FIRST_NAME",
00:12:38.184198 [error] [MainThread]:       "LAST_NAME",
00:12:38.184572 [error] [MainThread]:       "EMAIL",
00:12:38.184869 [error] [MainThread]:       "RECORD_SOURCE",
00:12:38.185118 [error] [MainThread]:       "CUSTOMER_HK",
00:12:38.185358 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:12:38.185682 [error] [MainThread]:   
00:12:38.185940 [error] [MainThread]:       FROM hashed_columns
00:12:38.186216 [error] [MainThread]:   )
00:12:38.186546 [error] [MainThread]:   
00:12:38.186819 [error] [MainThread]:   SELECT * FROM columns_to_select
00:12:38.187250 [error] [MainThread]:   )
00:12:38.187692 [info ] [MainThread]: 
00:12:38.188009 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:12:38.188483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a324c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a324cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1ab220>]}


============================== 2022-03-23 00:15:00.680524 | 562b7878-0896-4e9f-9a7e-17cde3d9051c ==============================
00:15:00.680524 [info ] [MainThread]: Running with dbt=1.0.3
00:15:00.681460 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
00:15:00.681768 [debug] [MainThread]: Tracking: tracking
00:15:00.721527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a79070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a85220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a852b0>]}
00:15:01.084396 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:15:01.084890 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:15:01.099942 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:15:01.223625 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:15:01.232409 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:15:01.237732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '562b7878-0896-4e9f-9a7e-17cde3d9051c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e210d0>]}
00:15:01.259095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '562b7878-0896-4e9f-9a7e-17cde3d9051c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c4cd00>]}
00:15:01.259463 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:15:01.260761 [info ] [MainThread]: 
00:15:01.261310 [debug] [MainThread]: Acquiring new databricks connection "master"
00:15:01.262139 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:15:01.273952 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:15:01.274235 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:15:01.274427 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:15:01.274605 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:15:02.343650 [debug] [ThreadPool]: SQL status: OK in 1.07 seconds
00:15:02.704158 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:15:02.704517 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:15:02.704765 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:15:02.879740 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:15:02.880375 [info ] [MainThread]: 
00:15:02.886622 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:15:02.887133 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:15:02.889727 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:15:02.889976 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:15:02.925792 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:15:02.926801 [debug] [Thread-1  ]: finished collecting timing info
00:15:02.927014 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:15:02.927219 [debug] [Thread-1  ]: finished collecting timing info
00:15:02.927598 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:15:02.928502 [debug] [MainThread]: Connection 'master' was properly closed.
00:15:02.928699 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:15:02.957248 [info ] [MainThread]: Done.
00:15:02.957771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e54cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e54d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e547c0>]}


============================== 2022-03-23 00:15:24.369864 | 4774bfe7-4914-42a2-8539-07cc2809a802 ==============================
00:15:24.369864 [info ] [MainThread]: Running with dbt=1.0.3
00:15:24.370547 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:15:24.370865 [debug] [MainThread]: Tracking: tracking
00:15:24.389013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c804f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c8e760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c8e220>]}
00:15:24.722799 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
00:15:24.723065 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
00:15:24.723531 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:15:24.731667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4774bfe7-4914-42a2-8539-07cc2809a802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f83f40>]}
00:15:24.759561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4774bfe7-4914-42a2-8539-07cc2809a802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e4f2e0>]}
00:15:24.760016 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:15:24.761724 [info ] [MainThread]: 
00:15:24.762302 [debug] [MainThread]: Acquiring new databricks connection "master"
00:15:24.763369 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:15:24.775124 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:15:24.775426 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:15:24.775630 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:15:25.203077 [debug] [ThreadPool]: SQL status: OK in 0.43 seconds
00:15:25.489803 [debug] [ThreadPool]: On list_schemas: Close
00:15:25.686402 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:15:25.697011 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:15:25.697256 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:15:25.697453 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:15:25.697634 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:15:26.411490 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
00:15:26.719320 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:15:26.719746 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:15:26.719975 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:15:26.896669 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:15:26.896880 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:15:26.897170 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:15:26.897420 [info ] [MainThread]: 
00:15:26.908481 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:15:26.909041 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:15:26.909641 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:15:26.911950 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:15:26.912205 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:15:26.916138 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:15:26.918335 [debug] [Thread-1  ]: finished collecting timing info
00:15:26.918679 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:15:26.941369 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:15:26.956059 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:15:26.956433 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:15:26.956719 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:15:26.956989 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:15:27.981920 [debug] [Thread-1  ]: SQL status: OK in 1.02 seconds
00:15:27.983912 [debug] [Thread-1  ]: finished collecting timing info
00:15:27.984210 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:15:27.984446 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:15:27.984673 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:15:28.160559 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4774bfe7-4914-42a2-8539-07cc2809a802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050df760>]}
00:15:28.161257 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.25s]
00:15:28.161851 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:15:28.162721 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:15:28.163160 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:15:28.163805 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:15:28.166863 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:15:28.167164 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:15:28.298355 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:15:28.305652 [debug] [Thread-1  ]: finished collecting timing info
00:15:28.305915 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:15:28.310132 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:15:28.311126 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:15:28.311329 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:15:28.311510 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:15:28.311688 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:15:28.599117 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:15:28.599548 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SOURCE"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SOURCE",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:15:28.600133 [debug] [Thread-1  ]: finished collecting timing info
00:15:28.600395 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:15:28.600613 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:15:28.600820 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:15:28.789172 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "RECORD_SOURCE"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SOURCE",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SOURCE",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
00:15:28.789538 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4774bfe7-4914-42a2-8539-07cc2809a802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050ff7f0>]}
00:15:28.789852 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.63s]
00:15:28.790153 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:15:28.791091 [debug] [MainThread]: Acquiring new databricks connection "master"
00:15:28.791275 [debug] [MainThread]: On master: ROLLBACK
00:15:28.791451 [debug] [MainThread]: Opening a new connection, currently in state init
00:15:28.905327 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:15:28.905624 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:15:28.905804 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:15:28.905982 [debug] [MainThread]: On master: ROLLBACK
00:15:28.906162 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:15:28.906324 [debug] [MainThread]: On master: Close
00:15:29.069501 [info ] [MainThread]: 
00:15:29.070118 [info ] [MainThread]: Finished running 2 view models in 4.31s.
00:15:29.070627 [debug] [MainThread]: Connection 'master' was properly closed.
00:15:29.070984 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:15:29.106425 [info ] [MainThread]: 
00:15:29.125259 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:15:29.128237 [info ] [MainThread]: 
00:15:29.128814 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:15:29.129315 [error] [MainThread]:   Query execution failed.
00:15:29.129754 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:15:29.130237 [error] [MainThread]:   mismatched input '"RECORD_SOURCE"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)
00:15:29.130642 [error] [MainThread]:   
00:15:29.131028 [error] [MainThread]:   == SQL ==
00:15:29.131412 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:15:29.131774 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:15:29.132112 [error] [MainThread]:     
00:15:29.132449 [error] [MainThread]:     as
00:15:29.132792 [error] [MainThread]:       
00:15:29.133113 [error] [MainThread]:   
00:15:29.133455 [error] [MainThread]:   -- Generated by dbtvault.
00:15:29.133718 [error] [MainThread]:   
00:15:29.134076 [error] [MainThread]:   
00:15:29.134363 [error] [MainThread]:   
00:15:29.134641 [error] [MainThread]:   WITH source_data AS (
00:15:29.134930 [error] [MainThread]:   
00:15:29.135226 [error] [MainThread]:       SELECT
00:15:29.135675 [error] [MainThread]:   
00:15:29.135961 [error] [MainThread]:       "CUSTOMER_ID",
00:15:29.136236 [error] [MainThread]:       "FIRST_NAME",
00:15:29.136521 [error] [MainThread]:       "LAST_NAME",
00:15:29.136802 [error] [MainThread]:       "EMAIL"
00:15:29.137171 [error] [MainThread]:   
00:15:29.137608 [error] [MainThread]:       FROM test_dbt.raw_customer
00:15:29.137933 [error] [MainThread]:   ),
00:15:29.138244 [error] [MainThread]:   
00:15:29.138629 [error] [MainThread]:   derived_columns AS (
00:15:29.138949 [error] [MainThread]:   
00:15:29.139285 [error] [MainThread]:       SELECT
00:15:29.139604 [error] [MainThread]:   
00:15:29.139969 [error] [MainThread]:       "CUSTOMER_ID",
00:15:29.140279 [error] [MainThread]:       "FIRST_NAME",
00:15:29.140549 [error] [MainThread]:       "LAST_NAME",
00:15:29.140833 [error] [MainThread]:       "EMAIL",
00:15:29.141133 [error] [MainThread]:       'SAP' AS "RECORD_SOURCE"
00:15:29.141430 [error] [MainThread]:   -------------^^^
00:15:29.141691 [error] [MainThread]:   
00:15:29.141957 [error] [MainThread]:       FROM source_data
00:15:29.142215 [error] [MainThread]:   ),
00:15:29.142474 [error] [MainThread]:   
00:15:29.142753 [error] [MainThread]:   hashed_columns AS (
00:15:29.143027 [error] [MainThread]:   
00:15:29.143308 [error] [MainThread]:       SELECT
00:15:29.143568 [error] [MainThread]:   
00:15:29.143844 [error] [MainThread]:       "CUSTOMER_ID",
00:15:29.144115 [error] [MainThread]:       "FIRST_NAME",
00:15:29.144379 [error] [MainThread]:       "LAST_NAME",
00:15:29.144640 [error] [MainThread]:       "EMAIL",
00:15:29.144899 [error] [MainThread]:       "RECORD_SOURCE",
00:15:29.145162 [error] [MainThread]:   
00:15:29.145462 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:15:29.145731 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:15:29.145995 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:15:29.146260 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:15:29.146518 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:15:29.146801 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:15:29.147066 [error] [MainThread]:   
00:15:29.147505 [error] [MainThread]:       FROM derived_columns
00:15:29.147837 [error] [MainThread]:   ),
00:15:29.148133 [error] [MainThread]:   
00:15:29.148462 [error] [MainThread]:   columns_to_select AS (
00:15:29.148746 [error] [MainThread]:   
00:15:29.149021 [error] [MainThread]:       SELECT
00:15:29.149308 [error] [MainThread]:   
00:15:29.149574 [error] [MainThread]:       "CUSTOMER_ID",
00:15:29.149850 [error] [MainThread]:       "FIRST_NAME",
00:15:29.150130 [error] [MainThread]:       "LAST_NAME",
00:15:29.150395 [error] [MainThread]:       "EMAIL",
00:15:29.150693 [error] [MainThread]:       "RECORD_SOURCE",
00:15:29.151015 [error] [MainThread]:       "CUSTOMER_HK",
00:15:29.151305 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:15:29.151675 [error] [MainThread]:   
00:15:29.151975 [error] [MainThread]:       FROM hashed_columns
00:15:29.152289 [error] [MainThread]:   )
00:15:29.152578 [error] [MainThread]:   
00:15:29.152890 [error] [MainThread]:   SELECT * FROM columns_to_select
00:15:29.153258 [info ] [MainThread]: 
00:15:29.153598 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:15:29.154040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050838e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105083df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10510d790>]}


============================== 2022-03-23 00:16:02.173698 | 9551ea69-c6e3-4512-b2e4-5183732750b7 ==============================
00:16:02.173698 [info ] [MainThread]: Running with dbt=1.0.3
00:16:02.174410 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:16:02.174690 [debug] [MainThread]: Tracking: tracking
00:16:02.191424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5fe640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6091c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6095b0>]}
00:16:02.522581 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:16:02.523082 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:16:02.538207 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:16:02.674769 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:16:02.685679 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:16:02.691904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9551ea69-c6e3-4512-b2e4-5183732750b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9a70d0>]}
00:16:02.707777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9551ea69-c6e3-4512-b2e4-5183732750b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7d5c70>]}
00:16:02.708154 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:16:02.709567 [info ] [MainThread]: 
00:16:02.710090 [debug] [MainThread]: Acquiring new databricks connection "master"
00:16:02.711057 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:16:02.721816 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:16:02.722106 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:16:02.722297 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:16:03.202301 [debug] [ThreadPool]: SQL status: OK in 0.48 seconds
00:16:03.513073 [debug] [ThreadPool]: On list_schemas: Close
00:16:03.694657 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:16:03.705128 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:16:03.705370 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:16:03.705564 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:16:03.705747 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:16:04.707214 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
00:16:05.016087 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:16:05.016506 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:16:05.016735 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:16:05.194237 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:16:05.194633 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:16:05.195173 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:16:05.195670 [info ] [MainThread]: 
00:16:05.208824 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:16:05.209496 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:16:05.210047 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:16:05.212601 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:16:05.212857 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:16:05.216818 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:16:05.217742 [debug] [Thread-1  ]: finished collecting timing info
00:16:05.217996 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:16:05.242445 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:16:05.255455 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:16:05.255894 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:16:05.256212 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:16:05.256431 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:16:06.243298 [debug] [Thread-1  ]: SQL status: OK in 0.99 seconds
00:16:06.245304 [debug] [Thread-1  ]: finished collecting timing info
00:16:06.245672 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:16:06.245901 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:16:06.246122 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:16:06.441097 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9551ea69-c6e3-4512-b2e4-5183732750b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa17ca0>]}
00:16:06.441791 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.23s]
00:16:06.442371 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:16:06.443085 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:16:06.443467 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:16:06.444230 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:16:06.447194 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:16:06.447420 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:16:06.484939 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:16:06.486067 [debug] [Thread-1  ]: finished collecting timing info
00:16:06.486286 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:16:06.490129 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:16:06.505775 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:16:06.506150 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:16:06.506430 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SRC"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:16:06.506699 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:16:06.811853 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SRC"

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:16:06.812332 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SRC"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SRC"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"RECORD_SRC"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    'SAP' AS "RECORD_SRC"
-------------^^^

    FROM source_data
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "RECORD_SRC",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:16:06.812978 [debug] [Thread-1  ]: finished collecting timing info
00:16:06.813309 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:16:06.813593 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:16:06.813858 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:16:07.008232 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"RECORD_SRC"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      'SAP' AS "RECORD_SRC"
  -------------^^^
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SRC",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "RECORD_SRC",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
00:16:07.008909 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9551ea69-c6e3-4512-b2e4-5183732750b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab64ca0>]}
00:16:07.009479 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.56s]
00:16:07.010069 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:16:07.011698 [debug] [MainThread]: Acquiring new databricks connection "master"
00:16:07.011985 [debug] [MainThread]: On master: ROLLBACK
00:16:07.012211 [debug] [MainThread]: Opening a new connection, currently in state init
00:16:07.125354 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:16:07.125789 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:16:07.126071 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:16:07.126363 [debug] [MainThread]: On master: ROLLBACK
00:16:07.126629 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:16:07.126890 [debug] [MainThread]: On master: Close
00:16:07.222708 [info ] [MainThread]: 
00:16:07.223332 [info ] [MainThread]: Finished running 2 view models in 4.51s.
00:16:07.223813 [debug] [MainThread]: Connection 'master' was properly closed.
00:16:07.224125 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:16:07.258009 [info ] [MainThread]: 
00:16:07.258578 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:16:07.259041 [info ] [MainThread]: 
00:16:07.259487 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:16:07.259883 [error] [MainThread]:   Query execution failed.
00:16:07.260256 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:16:07.260623 [error] [MainThread]:   mismatched input '"RECORD_SRC"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 31, pos 13)
00:16:07.261050 [error] [MainThread]:   
00:16:07.261623 [error] [MainThread]:   == SQL ==
00:16:07.261940 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:16:07.262235 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:16:07.262546 [error] [MainThread]:     
00:16:07.262829 [error] [MainThread]:     as
00:16:07.263113 [error] [MainThread]:       
00:16:07.263393 [error] [MainThread]:   
00:16:07.263708 [error] [MainThread]:   -- Generated by dbtvault.
00:16:07.263984 [error] [MainThread]:   
00:16:07.264259 [error] [MainThread]:   
00:16:07.264535 [error] [MainThread]:   
00:16:07.264809 [error] [MainThread]:   WITH source_data AS (
00:16:07.265117 [error] [MainThread]:   
00:16:07.265442 [error] [MainThread]:       SELECT
00:16:07.265736 [error] [MainThread]:   
00:16:07.266005 [error] [MainThread]:       "CUSTOMER_ID",
00:16:07.266287 [error] [MainThread]:       "FIRST_NAME",
00:16:07.266578 [error] [MainThread]:       "LAST_NAME",
00:16:07.266850 [error] [MainThread]:       "EMAIL"
00:16:07.267149 [error] [MainThread]:   
00:16:07.267433 [error] [MainThread]:       FROM test_dbt.raw_customer
00:16:07.267701 [error] [MainThread]:   ),
00:16:07.267966 [error] [MainThread]:   
00:16:07.268235 [error] [MainThread]:   derived_columns AS (
00:16:07.268504 [error] [MainThread]:   
00:16:07.268767 [error] [MainThread]:       SELECT
00:16:07.269031 [error] [MainThread]:   
00:16:07.269340 [error] [MainThread]:       "CUSTOMER_ID",
00:16:07.269614 [error] [MainThread]:       "FIRST_NAME",
00:16:07.269884 [error] [MainThread]:       "LAST_NAME",
00:16:07.270162 [error] [MainThread]:       "EMAIL",
00:16:07.270484 [error] [MainThread]:       'SAP' AS "RECORD_SRC"
00:16:07.270772 [error] [MainThread]:   -------------^^^
00:16:07.271078 [error] [MainThread]:   
00:16:07.271364 [error] [MainThread]:       FROM source_data
00:16:07.271631 [error] [MainThread]:   ),
00:16:07.271909 [error] [MainThread]:   
00:16:07.272181 [error] [MainThread]:   hashed_columns AS (
00:16:07.272460 [error] [MainThread]:   
00:16:07.272752 [error] [MainThread]:       SELECT
00:16:07.273076 [error] [MainThread]:   
00:16:07.273377 [error] [MainThread]:       "CUSTOMER_ID",
00:16:07.273852 [error] [MainThread]:       "FIRST_NAME",
00:16:07.274269 [error] [MainThread]:       "LAST_NAME",
00:16:07.274605 [error] [MainThread]:       "EMAIL",
00:16:07.274888 [error] [MainThread]:       "RECORD_SRC",
00:16:07.275308 [error] [MainThread]:   
00:16:07.275662 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:16:07.275983 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:16:07.276268 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:16:07.276566 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:16:07.276937 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:16:07.277225 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:16:07.277500 [error] [MainThread]:   
00:16:07.277776 [error] [MainThread]:       FROM derived_columns
00:16:07.278045 [error] [MainThread]:   ),
00:16:07.278327 [error] [MainThread]:   
00:16:07.278591 [error] [MainThread]:   columns_to_select AS (
00:16:07.278856 [error] [MainThread]:   
00:16:07.279122 [error] [MainThread]:       SELECT
00:16:07.279388 [error] [MainThread]:   
00:16:07.279657 [error] [MainThread]:       "CUSTOMER_ID",
00:16:07.279925 [error] [MainThread]:       "FIRST_NAME",
00:16:07.280194 [error] [MainThread]:       "LAST_NAME",
00:16:07.280469 [error] [MainThread]:       "EMAIL",
00:16:07.280748 [error] [MainThread]:       "RECORD_SRC",
00:16:07.281022 [error] [MainThread]:       "CUSTOMER_HK",
00:16:07.281285 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:16:07.281553 [error] [MainThread]:   
00:16:07.281812 [error] [MainThread]:       FROM hashed_columns
00:16:07.282103 [error] [MainThread]:   )
00:16:07.282368 [error] [MainThread]:   
00:16:07.282662 [error] [MainThread]:   SELECT * FROM columns_to_select
00:16:07.282949 [info ] [MainThread]: 
00:16:07.283240 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:16:07.283662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa56580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa56220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9e7eb0>]}


============================== 2022-03-23 00:20:39.811233 | 48f5de6d-8379-40e9-bddf-e7dbdbdaac70 ==============================
00:20:39.811233 [info ] [MainThread]: Running with dbt=1.0.3
00:20:39.812666 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:20:39.812968 [debug] [MainThread]: Tracking: tracking
00:20:39.838198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066d12e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066aab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066aaf70>]}
00:20:40.218170 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:20:40.218746 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:20:40.234204 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:20:40.365211 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:20:40.375112 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:20:40.380961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48f5de6d-8379-40e9-bddf-e7dbdbdaac70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a6e0d0>]}
00:20:40.406362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48f5de6d-8379-40e9-bddf-e7dbdbdaac70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106897d30>]}
00:20:40.406839 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:20:40.408612 [info ] [MainThread]: 
00:20:40.409305 [debug] [MainThread]: Acquiring new databricks connection "master"
00:20:40.410340 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:20:40.421347 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:20:40.421689 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:20:40.421902 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:20:41.226356 [debug] [ThreadPool]: SQL status: OK in 0.8 seconds
00:20:41.646157 [debug] [ThreadPool]: On list_schemas: Close
00:20:41.856178 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:20:41.866815 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:20:41.867066 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:20:41.867256 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:20:41.867435 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:20:42.515742 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
00:20:42.819057 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:20:42.819395 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:20:42.819626 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:20:43.017322 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:20:43.017731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:20:43.018169 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:20:43.018564 [info ] [MainThread]: 
00:20:43.025126 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:20:43.025545 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:20:43.026346 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:20:43.028958 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:20:43.029205 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:20:43.033030 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:20:43.034003 [debug] [Thread-1  ]: finished collecting timing info
00:20:43.034233 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:20:43.059647 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:20:43.061034 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:20:43.061317 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:20:43.061535 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:20:43.061731 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:20:44.092547 [debug] [Thread-1  ]: SQL status: OK in 1.03 seconds
00:20:44.094597 [debug] [Thread-1  ]: finished collecting timing info
00:20:44.094956 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:20:44.095234 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:20:44.095453 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:20:44.286845 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48f5de6d-8379-40e9-bddf-e7dbdbdaac70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c04d60>]}
00:20:44.287559 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.26s]
00:20:44.288157 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:20:44.288970 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:20:44.289350 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:20:44.289956 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:20:44.293127 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:20:44.293452 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:20:44.325435 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:20:44.326256 [debug] [Thread-1  ]: finished collecting timing info
00:20:44.326486 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:20:44.330123 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:20:44.330936 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:20:44.331140 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:20:44.331335 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM source_data
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:20:44.331523 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:20:44.642478 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM source_data
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:20:44.642919 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"CUSTOMER_HK"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 97)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
-------------------------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM source_data
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '"CUSTOMER_HK"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 97)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL"

    FROM test_dbt.raw_customer
),

hashed_columns AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
-------------------------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"

    FROM source_data
),

columns_to_select AS (

    SELECT

    "CUSTOMER_ID",
    "FIRST_NAME",
    "LAST_NAME",
    "EMAIL",
    "CUSTOMER_HK",
    "CUSTOMER_HASHDIFF"

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:20:44.643561 [debug] [Thread-1  ]: finished collecting timing info
00:20:44.643898 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:20:44.644179 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:20:44.644446 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:20:44.841820 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input '"CUSTOMER_HK"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 97)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL"
  
      FROM test_dbt.raw_customer
  ),
  
  hashed_columns AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
  -------------------------------------------------------------------------------------------------^^^
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
  
      FROM source_data
  ),
  
  columns_to_select AS (
  
      SELECT
  
      "CUSTOMER_ID",
      "FIRST_NAME",
      "LAST_NAME",
      "EMAIL",
      "CUSTOMER_HK",
      "CUSTOMER_HASHDIFF"
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
00:20:44.842472 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48f5de6d-8379-40e9-bddf-e7dbdbdaac70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b87400>]}
00:20:44.843057 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.55s]
00:20:44.843676 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:20:44.845018 [debug] [MainThread]: Acquiring new databricks connection "master"
00:20:44.845314 [debug] [MainThread]: On master: ROLLBACK
00:20:44.845542 [debug] [MainThread]: Opening a new connection, currently in state init
00:20:44.955571 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:20:44.956015 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:20:44.956301 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:20:44.956599 [debug] [MainThread]: On master: ROLLBACK
00:20:44.956868 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:20:44.957132 [debug] [MainThread]: On master: Close
00:20:45.047930 [info ] [MainThread]: 
00:20:45.048559 [info ] [MainThread]: Finished running 2 view models in 4.64s.
00:20:45.049048 [debug] [MainThread]: Connection 'master' was properly closed.
00:20:45.049349 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:20:45.068872 [info ] [MainThread]: 
00:20:45.069265 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:20:45.069630 [info ] [MainThread]: 
00:20:45.069992 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:20:45.070377 [error] [MainThread]:   Query execution failed.
00:20:45.070684 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:20:45.071007 [error] [MainThread]:   mismatched input '"CUSTOMER_HK"' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 32, pos 97)
00:20:45.071339 [error] [MainThread]:   
00:20:45.071655 [error] [MainThread]:   == SQL ==
00:20:45.071989 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:20:45.072451 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:20:45.072816 [error] [MainThread]:     
00:20:45.073231 [error] [MainThread]:     as
00:20:45.073569 [error] [MainThread]:       
00:20:45.074022 [error] [MainThread]:   
00:20:45.074375 [error] [MainThread]:   -- Generated by dbtvault.
00:20:45.074910 [error] [MainThread]:   
00:20:45.075407 [error] [MainThread]:   
00:20:45.075831 [error] [MainThread]:   
00:20:45.076154 [error] [MainThread]:   WITH source_data AS (
00:20:45.076466 [error] [MainThread]:   
00:20:45.076758 [error] [MainThread]:       SELECT
00:20:45.077075 [error] [MainThread]:   
00:20:45.077436 [error] [MainThread]:       "CUSTOMER_ID",
00:20:45.077768 [error] [MainThread]:       "FIRST_NAME",
00:20:45.078057 [error] [MainThread]:       "LAST_NAME",
00:20:45.078349 [error] [MainThread]:       "EMAIL"
00:20:45.078702 [error] [MainThread]:   
00:20:45.078996 [error] [MainThread]:       FROM test_dbt.raw_customer
00:20:45.079297 [error] [MainThread]:   ),
00:20:45.079574 [error] [MainThread]:   
00:20:45.079852 [error] [MainThread]:   hashed_columns AS (
00:20:45.080165 [error] [MainThread]:   
00:20:45.080464 [error] [MainThread]:       SELECT
00:20:45.080758 [error] [MainThread]:   
00:20:45.081047 [error] [MainThread]:       "CUSTOMER_ID",
00:20:45.081322 [error] [MainThread]:       "FIRST_NAME",
00:20:45.081589 [error] [MainThread]:       "LAST_NAME",
00:20:45.081867 [error] [MainThread]:       "EMAIL",
00:20:45.082180 [error] [MainThread]:   
00:20:45.082609 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST("CUSTOMER_ID" AS VARCHAR))), ''))) AS BINARY(16)) AS "CUSTOMER_HK",
00:20:45.082913 [error] [MainThread]:   -------------------------------------------------------------------------------------------------^^^
00:20:45.083219 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:20:45.083509 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("EMAIL" AS VARCHAR))), ''), '^^'),
00:20:45.083790 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("FIRST_NAME" AS VARCHAR))), ''), '^^'),
00:20:45.084066 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST("LAST_NAME" AS VARCHAR))), ''), '^^')
00:20:45.084362 [error] [MainThread]:       )) AS BINARY(16)) AS "CUSTOMER_HASHDIFF"
00:20:45.084640 [error] [MainThread]:   
00:20:45.084925 [error] [MainThread]:       FROM source_data
00:20:45.085218 [error] [MainThread]:   ),
00:20:45.085489 [error] [MainThread]:   
00:20:45.085752 [error] [MainThread]:   columns_to_select AS (
00:20:45.086012 [error] [MainThread]:   
00:20:45.086441 [error] [MainThread]:       SELECT
00:20:45.086741 [error] [MainThread]:   
00:20:45.087016 [error] [MainThread]:       "CUSTOMER_ID",
00:20:45.087335 [error] [MainThread]:       "FIRST_NAME",
00:20:45.087621 [error] [MainThread]:       "LAST_NAME",
00:20:45.087933 [error] [MainThread]:       "EMAIL",
00:20:45.088338 [error] [MainThread]:       "CUSTOMER_HK",
00:20:45.088724 [error] [MainThread]:       "CUSTOMER_HASHDIFF"
00:20:45.089185 [error] [MainThread]:   
00:20:45.089603 [error] [MainThread]:       FROM hashed_columns
00:20:45.090150 [error] [MainThread]:   )
00:20:45.090596 [error] [MainThread]:   
00:20:45.090915 [error] [MainThread]:   SELECT * FROM columns_to_select
00:20:45.091215 [info ] [MainThread]: 
00:20:45.091528 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:20:45.091920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c35040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106acdcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b85970>]}


============================== 2022-03-23 00:24:15.260150 | 417c309f-1a79-4b5e-ad00-29f4ac6ad416 ==============================
00:24:15.260150 [info ] [MainThread]: Running with dbt=1.0.3
00:24:15.261851 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
00:24:15.262328 [debug] [MainThread]: Tracking: tracking
00:24:15.305535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105bde640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105beb0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105beb1c0>]}
00:24:15.690045 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
00:24:15.690575 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
00:24:15.707617 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:24:15.824365 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:24:15.851290 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.test_dbx.example

00:24:15.859570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '417c309f-1a79-4b5e-ad00-29f4ac6ad416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f860d0>]}
00:24:15.876631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '417c309f-1a79-4b5e-ad00-29f4ac6ad416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105db6c70>]}
00:24:15.877022 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:24:15.878343 [info ] [MainThread]: 
00:24:15.879172 [debug] [MainThread]: Acquiring new databricks connection "master"
00:24:15.880584 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:24:15.893044 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:24:15.893367 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:24:15.893571 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:24:15.893765 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:24:16.840813 [debug] [ThreadPool]: SQL status: OK in 0.95 seconds
00:24:17.196196 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:24:17.196485 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:24:17.196676 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:24:17.376772 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:24:17.377207 [info ] [MainThread]: 
00:24:17.382672 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:24:17.383151 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:24:17.385719 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:24:17.385952 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:24:17.415771 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:24:17.416961 [debug] [Thread-1  ]: finished collecting timing info
00:24:17.417186 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:24:17.417400 [debug] [Thread-1  ]: finished collecting timing info
00:24:17.417789 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:24:17.418667 [debug] [MainThread]: Connection 'master' was properly closed.
00:24:17.418878 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:24:17.449639 [info ] [MainThread]: Done.
00:24:17.450202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10603a490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10603a4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10603a430>]}


============================== 2022-03-23 00:33:04.657575 | 2d629360-8e4a-4394-9f8b-e44a1aededdd ==============================
00:33:04.657575 [info ] [MainThread]: Running with dbt=1.0.3
00:33:04.658826 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
00:33:04.659219 [debug] [MainThread]: Tracking: tracking
00:33:04.700616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a8d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10558d5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10558d760>]}
00:33:04.817188 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
00:33:04.825461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2d629360-8e4a-4394-9f8b-e44a1aededdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105581400>]}
00:33:05.084969 [debug] [MainThread]: Parsing macros/adapters.sql
00:33:05.111040 [debug] [MainThread]: Parsing macros/materializations/seed.sql
00:33:05.115340 [debug] [MainThread]: Parsing macros/materializations/view.sql
00:33:05.115988 [debug] [MainThread]: Parsing macros/materializations/table.sql
00:33:05.119163 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
00:33:05.136731 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
00:33:05.141474 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
00:33:05.145222 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
00:33:05.150942 [debug] [MainThread]: Parsing macros/adapters.sql
00:33:05.188169 [debug] [MainThread]: Parsing macros/materializations/seed.sql
00:33:05.196797 [debug] [MainThread]: Parsing macros/materializations/view.sql
00:33:05.197340 [debug] [MainThread]: Parsing macros/materializations/table.sql
00:33:05.200018 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
00:33:05.223547 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
00:33:05.228600 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
00:33:05.234387 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
00:33:05.239919 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
00:33:05.243601 [debug] [MainThread]: Parsing macros/materializations/configs.sql
00:33:05.245747 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
00:33:05.247258 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
00:33:05.262895 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
00:33:05.273248 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
00:33:05.284011 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
00:33:05.288295 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
00:33:05.289898 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
00:33:05.291528 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
00:33:05.295562 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
00:33:05.306035 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
00:33:05.307524 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
00:33:05.316741 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
00:33:05.331001 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
00:33:05.337749 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
00:33:05.340342 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
00:33:05.346935 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
00:33:05.348125 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
00:33:05.350544 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
00:33:05.352631 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
00:33:05.358109 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
00:33:05.373093 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
00:33:05.374442 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
00:33:05.376696 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
00:33:05.378181 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
00:33:05.378996 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
00:33:05.379516 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
00:33:05.380164 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
00:33:05.381409 [debug] [MainThread]: Parsing macros/etc/statement.sql
00:33:05.385380 [debug] [MainThread]: Parsing macros/etc/datetime.sql
00:33:05.393005 [debug] [MainThread]: Parsing macros/adapters/schema.sql
00:33:05.394986 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
00:33:05.397455 [debug] [MainThread]: Parsing macros/adapters/relation.sql
00:33:05.406213 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
00:33:05.408776 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
00:33:05.412698 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
00:33:05.419254 [debug] [MainThread]: Parsing macros/adapters/columns.sql
00:33:05.427946 [debug] [MainThread]: Parsing tests/generic/builtin.sql
00:33:05.430671 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
00:33:05.431635 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
00:33:05.432758 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
00:33:05.433766 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
00:33:05.439250 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
00:33:05.440259 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
00:33:05.441415 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
00:33:05.444023 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
00:33:05.444989 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
00:33:05.446910 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
00:33:05.449084 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
00:33:05.457846 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
00:33:05.459514 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
00:33:05.460797 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
00:33:05.462090 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
00:33:05.463789 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
00:33:05.465055 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
00:33:05.466437 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
00:33:05.467284 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
00:33:05.470400 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
00:33:05.475037 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
00:33:05.476432 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
00:33:05.479571 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
00:33:05.481185 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
00:33:05.482501 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
00:33:05.484175 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
00:33:05.507230 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
00:33:05.509189 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
00:33:05.511482 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
00:33:05.512885 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
00:33:05.513889 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
00:33:05.514978 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
00:33:05.516204 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
00:33:05.517883 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
00:33:05.519829 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
00:33:05.521271 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
00:33:05.523404 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
00:33:05.525334 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
00:33:05.526472 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
00:33:05.528647 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
00:33:05.530661 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
00:33:05.532119 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
00:33:05.533236 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
00:33:05.535716 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
00:33:05.537455 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
00:33:05.539149 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
00:33:05.541081 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
00:33:05.543560 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
00:33:05.544831 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
00:33:05.547963 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
00:33:05.555842 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
00:33:05.559604 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
00:33:05.561026 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
00:33:05.564058 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
00:33:05.567874 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
00:33:05.570859 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
00:33:05.572331 [debug] [MainThread]: Parsing macros/sql/star.sql
00:33:05.576392 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
00:33:05.583339 [debug] [MainThread]: Parsing macros/sql/union.sql
00:33:05.592399 [debug] [MainThread]: Parsing macros/sql/groupby.sql
00:33:05.593592 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
00:33:05.596916 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
00:33:05.598411 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
00:33:05.599908 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
00:33:05.605665 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
00:33:05.610359 [debug] [MainThread]: Parsing macros/sql/pivot.sql
00:33:05.614074 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
00:33:05.616159 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
00:33:05.617369 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
00:33:05.622595 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
00:33:05.644871 [debug] [MainThread]: Parsing macros/get_base_dates.sql
00:33:05.649740 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
00:33:05.653588 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
00:33:05.655224 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
00:33:05.655838 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
00:33:05.656370 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
00:33:05.657046 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
00:33:05.657581 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
00:33:05.660504 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
00:33:05.662061 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
00:33:05.662831 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
00:33:05.665340 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
00:33:05.667640 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
00:33:05.668629 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
00:33:05.669166 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
00:33:05.669733 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
00:33:05.670423 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
00:33:05.670945 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
00:33:05.671478 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
00:33:05.673203 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
00:33:05.678320 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
00:33:05.679212 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
00:33:05.680509 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
00:33:05.681363 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
00:33:05.682337 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
00:33:05.682939 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
00:33:05.691090 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
00:33:05.693242 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
00:33:05.694150 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
00:33:05.696999 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
00:33:05.697698 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
00:33:05.699383 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
00:33:05.703543 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
00:33:05.704416 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
00:33:05.706899 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
00:33:05.709105 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
00:33:05.709749 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
00:33:05.710389 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
00:33:05.712070 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
00:33:05.713891 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
00:33:05.714701 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
00:33:05.731045 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
00:33:05.732075 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
00:33:05.740794 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
00:33:05.751457 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
00:33:05.764037 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
00:33:05.775536 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
00:33:05.783272 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
00:33:05.807137 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
00:33:05.821103 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
00:33:05.827525 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
00:33:05.837401 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
00:33:05.849218 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
00:33:05.863248 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
00:33:05.876314 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
00:33:05.915089 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
00:33:05.915893 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
00:33:05.931673 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
00:33:05.932588 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
00:33:05.941263 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
00:33:05.953612 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
00:33:05.965969 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
00:33:05.978497 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
00:33:05.982075 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
00:33:05.983224 [debug] [MainThread]: Parsing macros/staging/stage.sql
00:33:05.993184 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
00:33:06.001856 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
00:33:06.007482 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
00:33:06.010473 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
00:33:06.023182 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
00:33:06.026339 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
00:33:06.028292 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
00:33:06.033734 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
00:33:06.036715 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
00:33:06.038161 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
00:33:06.040234 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
00:33:06.040601 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
00:33:06.044759 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
00:33:06.053499 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
00:33:06.068462 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
00:33:06.073321 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
00:33:06.074307 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
00:33:06.097759 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
00:33:06.099922 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
00:33:06.104745 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
00:33:06.110453 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
00:33:06.113551 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
00:33:06.117030 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
00:33:06.123776 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
00:33:06.134172 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
00:33:06.138295 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
00:33:06.143288 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
00:33:06.148001 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
00:33:06.149024 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
00:33:06.150555 [debug] [MainThread]: Parsing macros/supporting/hash.sql
00:33:06.170700 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
00:33:06.175746 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
00:33:06.177699 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
00:33:06.972658 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
00:33:06.982069 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
00:33:07.085709 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
00:33:07.203099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2d629360-8e4a-4394-9f8b-e44a1aededdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058b30d0>]}
00:33:07.217729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2d629360-8e4a-4394-9f8b-e44a1aededdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10567e4f0>]}
00:33:07.218137 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
00:33:07.219469 [info ] [MainThread]: 
00:33:07.219979 [debug] [MainThread]: Acquiring new databricks connection "master"
00:33:07.220885 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
00:33:07.231268 [debug] [ThreadPool]: Using databricks connection "list_schemas"
00:33:07.231534 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
00:33:07.231705 [debug] [ThreadPool]: Opening a new connection, currently in state init
00:33:07.827379 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
00:33:08.159083 [debug] [ThreadPool]: On list_schemas: Close
00:33:08.371478 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
00:33:08.412416 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
00:33:08.412712 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
00:33:08.412908 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
00:33:08.413090 [debug] [ThreadPool]: Opening a new connection, currently in state closed
00:33:09.066977 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
00:33:09.352281 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
00:33:09.352678 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
00:33:09.352951 [debug] [ThreadPool]: On list_None_test_dbt: Close
00:33:09.532889 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:33:09.533278 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:33:09.533814 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
00:33:09.534316 [info ] [MainThread]: 
00:33:09.550507 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
00:33:09.550905 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
00:33:09.551475 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
00:33:09.554052 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
00:33:09.554288 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
00:33:09.558646 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
00:33:09.559743 [debug] [Thread-1  ]: finished collecting timing info
00:33:09.559993 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
00:33:09.585794 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
00:33:09.596289 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:33:09.596556 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
00:33:09.596752 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

00:33:09.596934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:33:10.670030 [debug] [Thread-1  ]: SQL status: OK in 1.07 seconds
00:33:10.671933 [debug] [Thread-1  ]: finished collecting timing info
00:33:10.672280 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
00:33:10.672549 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:33:10.672764 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
00:33:10.852718 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d629360-8e4a-4394-9f8b-e44a1aededdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105876b80>]}
00:33:10.853414 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.30s]
00:33:10.853999 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
00:33:10.854956 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
00:33:10.855332 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
00:33:10.855936 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
00:33:10.858829 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
00:33:10.859072 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
00:33:10.900631 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
00:33:10.901575 [debug] [Thread-1  ]: finished collecting timing info
00:33:10.901793 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
00:33:10.905726 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
00:33:10.906602 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
00:33:10.906827 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
00:33:10.907028 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    'SAP' AS `RECORD_SRC`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,
    `CUSTOMER_HK`,
    `CUSTOMER_HASHDIFF`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:33:10.907278 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
00:33:11.239517 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    'SAP' AS `RECORD_SRC`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,
    `CUSTOMER_HK`,
    `CUSTOMER_HASHDIFF`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

00:33:11.239966 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
DataType binary(16) is not supported.(line 46, pos 82)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    'SAP' AS `RECORD_SRC`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
----------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,
    `CUSTOMER_HK`,
    `CUSTOMER_HASHDIFF`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
DataType binary(16) is not supported.(line 46, pos 82)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    'SAP' AS `RECORD_SRC`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
----------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CUSTOMER_ID`,
    `FIRST_NAME`,
    `LAST_NAME`,
    `EMAIL`,
    `RECORD_SRC`,
    `CUSTOMER_HK`,
    `CUSTOMER_HASHDIFF`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPrimitiveDataType$1(AstBuilder.scala:2511)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPrimitiveDataType(AstBuilder.scala:2486)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPrimitiveDataType(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext.accept(SqlBaseParser.java:20748)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCast$1(AstBuilder.scala:1725)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCast(AstBuilder.scala:1724)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCast(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$CastContext.accept(SqlBaseParser.java:18038)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:1644)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:17636)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1373)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1521)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1520)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:17064)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:1602)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:16989)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1373)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpression$1(AstBuilder.scala:1407)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:1406)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext.accept(SqlBaseParser.java:16507)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpressionSeq$2(AstBuilder.scala:657)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpressionSeq(AstBuilder.scala:657)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:749)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:742)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:649)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:637)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:12729)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault(SqlBaseBaseVisitor.java:1161)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:12234)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:1147)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:12003)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNoWithQuery$1(AstBuilder.scala:191)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNoWithQuery(AstBuilder.scala:191)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNoWithQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NoWithQueryContext.accept(SqlBaseParser.java:11565)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:121)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:120)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:8243)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedQuery$1(AstBuilder.scala:200)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedQuery(AstBuilder.scala:199)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withCTE$1(AstBuilder.scala:135)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.withCTE(AstBuilder.scala:134)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$2(AstBuilder.scala:124)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$EnhancedLogicalPlan$.optionalMap$extension(ParserUtils.scala:231)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:124)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:120)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:8243)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCreateView$1(AstBuilder.scala:4259)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCreateView(AstBuilder.scala:4200)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCreateView(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateViewContext.accept(SqlBaseParser.java:2707)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:81)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:81)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:86)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

00:33:11.240667 [debug] [Thread-1  ]: finished collecting timing info
00:33:11.241084 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
00:33:11.241488 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
00:33:11.241816 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
00:33:11.439075 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  DataType binary(16) is not supported.(line 46, pos 82)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      `CUSTOMER_ID`,
      `FIRST_NAME`,
      `LAST_NAME`,
      `EMAIL`
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      `CUSTOMER_ID`,
      `FIRST_NAME`,
      `LAST_NAME`,
      `EMAIL`,
      'SAP' AS `RECORD_SRC`
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      `CUSTOMER_ID`,
      `FIRST_NAME`,
      `LAST_NAME`,
      `EMAIL`,
      `RECORD_SRC`,
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
  ----------------------------------------------------------------------------------^^^
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      `CUSTOMER_ID`,
      `FIRST_NAME`,
      `LAST_NAME`,
      `EMAIL`,
      `RECORD_SRC`,
      `CUSTOMER_HK`,
      `CUSTOMER_HASHDIFF`
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
00:33:11.439718 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d629360-8e4a-4394-9f8b-e44a1aededdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057fc670>]}
00:33:11.440289 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.58s]
00:33:11.440865 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
00:33:11.442271 [debug] [MainThread]: Acquiring new databricks connection "master"
00:33:11.442550 [debug] [MainThread]: On master: ROLLBACK
00:33:11.442769 [debug] [MainThread]: Opening a new connection, currently in state init
00:33:11.559802 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:33:11.560235 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
00:33:11.560515 [debug] [MainThread]: Spark adapter: NotImplemented: commit
00:33:11.560807 [debug] [MainThread]: On master: ROLLBACK
00:33:11.561069 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
00:33:11.561345 [debug] [MainThread]: On master: Close
00:33:11.663035 [info ] [MainThread]: 
00:33:11.663664 [info ] [MainThread]: Finished running 2 view models in 4.44s.
00:33:11.664155 [debug] [MainThread]: Connection 'master' was properly closed.
00:33:11.664442 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
00:33:11.720451 [info ] [MainThread]: 
00:33:11.721083 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
00:33:11.721666 [info ] [MainThread]: 
00:33:11.722157 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
00:33:11.722645 [error] [MainThread]:   Query execution failed.
00:33:11.723087 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
00:33:11.723635 [error] [MainThread]:   DataType binary(16) is not supported.(line 46, pos 82)
00:33:11.724175 [error] [MainThread]:   
00:33:11.724676 [error] [MainThread]:   == SQL ==
00:33:11.725166 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
00:33:11.725654 [error] [MainThread]:   create or replace view test_dbt.stg_customer
00:33:11.726110 [error] [MainThread]:     
00:33:11.726525 [error] [MainThread]:     as
00:33:11.726938 [error] [MainThread]:       
00:33:11.727349 [error] [MainThread]:   
00:33:11.727763 [error] [MainThread]:   -- Generated by dbtvault.
00:33:11.728176 [error] [MainThread]:   
00:33:11.728606 [error] [MainThread]:   
00:33:11.729020 [error] [MainThread]:   
00:33:11.729458 [error] [MainThread]:   WITH source_data AS (
00:33:11.729891 [error] [MainThread]:   
00:33:11.730308 [error] [MainThread]:       SELECT
00:33:11.730723 [error] [MainThread]:   
00:33:11.731131 [error] [MainThread]:       `CUSTOMER_ID`,
00:33:11.731539 [error] [MainThread]:       `FIRST_NAME`,
00:33:11.731950 [error] [MainThread]:       `LAST_NAME`,
00:33:11.732362 [error] [MainThread]:       `EMAIL`
00:33:11.732770 [error] [MainThread]:   
00:33:11.733194 [error] [MainThread]:       FROM test_dbt.raw_customer
00:33:11.733639 [error] [MainThread]:   ),
00:33:11.734080 [error] [MainThread]:   
00:33:11.734485 [error] [MainThread]:   derived_columns AS (
00:33:11.734896 [error] [MainThread]:   
00:33:11.735306 [error] [MainThread]:       SELECT
00:33:11.735729 [error] [MainThread]:   
00:33:11.736150 [error] [MainThread]:       `CUSTOMER_ID`,
00:33:11.736564 [error] [MainThread]:       `FIRST_NAME`,
00:33:11.736986 [error] [MainThread]:       `LAST_NAME`,
00:33:11.737442 [error] [MainThread]:       `EMAIL`,
00:33:11.737889 [error] [MainThread]:       'SAP' AS `RECORD_SRC`
00:33:11.738288 [error] [MainThread]:   
00:33:11.738710 [error] [MainThread]:       FROM source_data
00:33:11.739133 [error] [MainThread]:   ),
00:33:11.739545 [error] [MainThread]:   
00:33:11.739957 [error] [MainThread]:   hashed_columns AS (
00:33:11.740340 [error] [MainThread]:   
00:33:11.741971 [error] [MainThread]:       SELECT
00:33:11.742379 [error] [MainThread]:   
00:33:11.743569 [error] [MainThread]:       `CUSTOMER_ID`,
00:33:11.765753 [error] [MainThread]:       `FIRST_NAME`,
00:33:11.767071 [error] [MainThread]:       `LAST_NAME`,
00:33:11.767414 [error] [MainThread]:       `EMAIL`,
00:33:11.767742 [error] [MainThread]:       `RECORD_SRC`,
00:33:11.768021 [error] [MainThread]:   
00:33:11.768296 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(`CUSTOMER_ID` AS VARCHAR))), ''))) AS BINARY(16)) AS `CUSTOMER_HK`,
00:33:11.768605 [error] [MainThread]:   ----------------------------------------------------------------------------------^^^
00:33:11.768890 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
00:33:11.769177 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(`EMAIL` AS VARCHAR))), ''), '^^'),
00:33:11.769459 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(`FIRST_NAME` AS VARCHAR))), ''), '^^'),
00:33:11.769750 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(`LAST_NAME` AS VARCHAR))), ''), '^^')
00:33:11.770015 [error] [MainThread]:       )) AS BINARY(16)) AS `CUSTOMER_HASHDIFF`
00:33:11.770356 [error] [MainThread]:   
00:33:11.770715 [error] [MainThread]:       FROM derived_columns
00:33:11.771139 [error] [MainThread]:   ),
00:33:11.771523 [error] [MainThread]:   
00:33:11.771884 [error] [MainThread]:   columns_to_select AS (
00:33:11.772178 [error] [MainThread]:   
00:33:11.772453 [error] [MainThread]:       SELECT
00:33:11.772767 [error] [MainThread]:   
00:33:11.773043 [error] [MainThread]:       `CUSTOMER_ID`,
00:33:11.773339 [error] [MainThread]:       `FIRST_NAME`,
00:33:11.773953 [error] [MainThread]:       `LAST_NAME`,
00:33:11.774403 [error] [MainThread]:       `EMAIL`,
00:33:11.774739 [error] [MainThread]:       `RECORD_SRC`,
00:33:11.775152 [error] [MainThread]:       `CUSTOMER_HK`,
00:33:11.775524 [error] [MainThread]:       `CUSTOMER_HASHDIFF`
00:33:11.775856 [error] [MainThread]:   
00:33:11.776136 [error] [MainThread]:       FROM hashed_columns
00:33:11.776426 [error] [MainThread]:   )
00:33:11.776732 [error] [MainThread]:   
00:33:11.777006 [error] [MainThread]:   SELECT * FROM columns_to_select
00:33:11.777285 [info ] [MainThread]: 
00:33:11.777565 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
00:33:11.777953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105650400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058b3670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057f5e20>]}


============================== 2022-03-23 01:00:25.652498 | 4107294a-daf5-4177-9a10-f6b31bd6dbe8 ==============================
01:00:25.652498 [info ] [MainThread]: Running with dbt=1.0.3
01:00:25.653534 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=None, exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
01:00:25.653846 [debug] [MainThread]: Tracking: tracking
01:00:25.697456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f448250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f4587c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f4586a0>]}
01:00:25.785919 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
01:00:25.786401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4107294a-daf5-4177-9a10-f6b31bd6dbe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f42ae50>]}
01:00:26.027513 [debug] [MainThread]: Parsing macros/adapters.sql
01:00:26.056322 [debug] [MainThread]: Parsing macros/materializations/seed.sql
01:00:26.060626 [debug] [MainThread]: Parsing macros/materializations/view.sql
01:00:26.061270 [debug] [MainThread]: Parsing macros/materializations/table.sql
01:00:26.064476 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
01:00:26.082444 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
01:00:26.087104 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
01:00:26.090826 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
01:00:26.096136 [debug] [MainThread]: Parsing macros/adapters.sql
01:00:26.132734 [debug] [MainThread]: Parsing macros/materializations/seed.sql
01:00:26.141433 [debug] [MainThread]: Parsing macros/materializations/view.sql
01:00:26.142050 [debug] [MainThread]: Parsing macros/materializations/table.sql
01:00:26.144694 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
01:00:26.168833 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
01:00:26.176517 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
01:00:26.182435 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
01:00:26.187766 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
01:00:26.191507 [debug] [MainThread]: Parsing macros/materializations/configs.sql
01:00:26.195159 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
01:00:26.197882 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
01:00:26.223308 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
01:00:26.235702 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
01:00:26.247768 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
01:00:26.252347 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
01:00:26.254116 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
01:00:26.256936 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
01:00:26.261283 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
01:00:26.273881 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
01:00:26.275472 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
01:00:26.284782 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
01:00:26.299436 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
01:00:26.306312 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
01:00:26.308982 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
01:00:26.315574 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
01:00:26.316792 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
01:00:26.319270 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
01:00:26.321404 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
01:00:26.326973 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
01:00:26.342199 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
01:00:26.343578 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
01:00:26.345821 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
01:00:26.347270 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
01:00:26.348095 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
01:00:26.348625 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
01:00:26.349285 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
01:00:26.350549 [debug] [MainThread]: Parsing macros/etc/statement.sql
01:00:26.354592 [debug] [MainThread]: Parsing macros/etc/datetime.sql
01:00:26.362320 [debug] [MainThread]: Parsing macros/adapters/schema.sql
01:00:26.364346 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
01:00:26.366837 [debug] [MainThread]: Parsing macros/adapters/relation.sql
01:00:26.376356 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
01:00:26.379113 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
01:00:26.383222 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
01:00:26.390031 [debug] [MainThread]: Parsing macros/adapters/columns.sql
01:00:26.398944 [debug] [MainThread]: Parsing tests/generic/builtin.sql
01:00:26.401685 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
01:00:26.402665 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
01:00:26.403815 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
01:00:26.404848 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
01:00:26.410336 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
01:00:26.411359 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
01:00:26.412543 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
01:00:26.415240 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
01:00:26.416241 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
01:00:26.417951 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
01:00:26.420123 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
01:00:26.429051 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
01:00:26.430750 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
01:00:26.432072 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
01:00:26.433463 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
01:00:26.435022 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
01:00:26.436326 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
01:00:26.437752 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
01:00:26.438638 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
01:00:26.441810 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
01:00:26.446663 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
01:00:26.448110 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
01:00:26.451389 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
01:00:26.453117 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
01:00:26.454619 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
01:00:26.456358 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
01:00:26.478865 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
01:00:26.480656 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
01:00:26.483002 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
01:00:26.484627 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
01:00:26.485660 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
01:00:26.486764 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
01:00:26.487788 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
01:00:26.488870 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
01:00:26.490391 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
01:00:26.491918 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
01:00:26.493986 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
01:00:26.495525 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
01:00:26.496624 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
01:00:26.498803 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
01:00:26.500654 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
01:00:26.501919 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
01:00:26.503084 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
01:00:26.505882 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
01:00:26.507627 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
01:00:26.509315 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
01:00:26.511242 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
01:00:26.513718 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
01:00:26.515083 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
01:00:26.518172 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
01:00:26.526047 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
01:00:26.529677 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
01:00:26.531097 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
01:00:26.534080 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
01:00:26.537865 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
01:00:26.540852 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
01:00:26.542373 [debug] [MainThread]: Parsing macros/sql/star.sql
01:00:26.546419 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
01:00:26.553566 [debug] [MainThread]: Parsing macros/sql/union.sql
01:00:26.562778 [debug] [MainThread]: Parsing macros/sql/groupby.sql
01:00:26.563969 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
01:00:26.567057 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
01:00:26.568493 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
01:00:26.569947 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
01:00:26.575488 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
01:00:26.580064 [debug] [MainThread]: Parsing macros/sql/pivot.sql
01:00:26.583708 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
01:00:26.585730 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
01:00:26.586863 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
01:00:26.592370 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
01:00:26.615309 [debug] [MainThread]: Parsing macros/get_base_dates.sql
01:00:26.620138 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
01:00:26.623973 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
01:00:26.625568 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
01:00:26.626157 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
01:00:26.626804 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
01:00:26.627461 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
01:00:26.627981 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
01:00:26.630832 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
01:00:26.632354 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
01:00:26.633101 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
01:00:26.635538 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
01:00:26.637823 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
01:00:26.638777 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
01:00:26.639297 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
01:00:26.639853 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
01:00:26.640518 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
01:00:26.641025 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
01:00:26.641545 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
01:00:26.643284 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
01:00:26.648219 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
01:00:26.649088 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
01:00:26.650394 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
01:00:26.651227 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
01:00:26.652174 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
01:00:26.652762 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
01:00:26.659280 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
01:00:26.661276 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
01:00:26.662139 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
01:00:26.664772 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
01:00:26.665506 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
01:00:26.667139 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
01:00:26.671231 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
01:00:26.672058 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
01:00:26.674433 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
01:00:26.676556 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
01:00:26.677167 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
01:00:26.677778 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
01:00:26.679380 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
01:00:26.681037 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
01:00:26.681967 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
01:00:26.697051 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
01:00:26.697927 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
01:00:26.706165 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
01:00:26.716560 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
01:00:26.728723 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
01:00:26.739358 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
01:00:26.747024 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
01:00:26.769835 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
01:00:26.783320 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
01:00:26.789750 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
01:00:26.799654 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
01:00:26.811650 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
01:00:26.825431 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
01:00:26.838140 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
01:00:26.876178 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
01:00:26.876997 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
01:00:26.893030 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
01:00:26.894185 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
01:00:26.904698 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
01:00:26.918720 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
01:00:26.931294 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
01:00:26.943974 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
01:00:26.947847 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
01:00:26.949602 [debug] [MainThread]: Parsing macros/staging/stage.sql
01:00:26.964146 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
01:00:26.975805 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
01:00:26.981573 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
01:00:26.984124 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
01:00:26.997502 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
01:00:27.001020 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
01:00:27.003844 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
01:00:27.009580 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
01:00:27.012746 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
01:00:27.014304 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
01:00:27.016508 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
01:00:27.016901 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
01:00:27.021309 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
01:00:27.030119 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
01:00:27.045671 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
01:00:27.051151 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
01:00:27.052308 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
01:00:27.076915 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
01:00:27.079288 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
01:00:27.084354 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
01:00:27.090341 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
01:00:27.093610 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
01:00:27.097274 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
01:00:27.104484 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
01:00:27.115342 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
01:00:27.119609 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
01:00:27.124885 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
01:00:27.129764 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
01:00:27.130856 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
01:00:27.132476 [debug] [MainThread]: Parsing macros/supporting/hash.sql
01:00:27.153599 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
01:00:27.158823 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
01:00:27.160879 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
01:00:27.949668 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
01:00:27.959022 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
01:00:28.063688 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
01:00:28.161929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4107294a-daf5-4177-9a10-f6b31bd6dbe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f5460d0>]}
01:00:28.174752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4107294a-daf5-4177-9a10-f6b31bd6dbe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f4526a0>]}
01:00:28.175110 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:00:28.176360 [info ] [MainThread]: 
01:00:28.176853 [debug] [MainThread]: Acquiring new databricks connection "master"
01:00:28.177726 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
01:00:28.187729 [debug] [ThreadPool]: Using databricks connection "list_schemas"
01:00:28.188004 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
01:00:28.188185 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:00:29.124161 [debug] [ThreadPool]: SQL status: OK in 0.94 seconds
01:00:29.712868 [debug] [ThreadPool]: On list_schemas: Close
01:00:29.986713 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:00:30.028268 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:00:30.028559 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:00:30.028757 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:00:30.028944 [debug] [ThreadPool]: Opening a new connection, currently in state closed
01:00:30.876288 [debug] [ThreadPool]: SQL status: OK in 0.85 seconds
01:00:31.353128 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:00:31.353467 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:00:31.353706 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:00:31.596619 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:00:31.597022 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:00:31.597564 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:00:31.598066 [info ] [MainThread]: 
01:00:31.605857 [debug] [Thread-1  ]: Began running node model.test_dbx.raw_customer
01:00:31.606269 [info ] [Thread-1  ]: 1 of 2 START view model test_dbt.raw_customer................................... [RUN]
01:00:31.606836 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.raw_customer"
01:00:31.609492 [debug] [Thread-1  ]: Began compiling node model.test_dbx.raw_customer
01:00:31.609730 [debug] [Thread-1  ]: Compiling model.test_dbx.raw_customer
01:00:31.613643 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.raw_customer"
01:00:31.614650 [debug] [Thread-1  ]: finished collecting timing info
01:00:31.614879 [debug] [Thread-1  ]: Began executing node model.test_dbx.raw_customer
01:00:31.640242 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.raw_customer"
01:00:31.642544 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
01:00:31.642868 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.raw_customer"
01:00:31.643065 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.raw_customer"} */
create or replace view test_dbt.raw_customer
  
  as
    
SELECT
    customer_id AS CUSTOMER_ID,
    first_name AS FIRST_NAME,
    last_name AS LAST_NAME,
    email AS EMAIL 
FROM 
    test_dbt.customer

01:00:31.643246 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
01:00:32.961971 [debug] [Thread-1  ]: SQL status: OK in 1.32 seconds
01:00:32.963929 [debug] [Thread-1  ]: finished collecting timing info
01:00:32.964286 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: ROLLBACK
01:00:32.964576 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
01:00:32.964816 [debug] [Thread-1  ]: On model.test_dbx.raw_customer: Close
01:00:33.262949 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4107294a-daf5-4177-9a10-f6b31bd6dbe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e734970>]}
01:00:33.263665 [info ] [Thread-1  ]: 1 of 2 OK created view model test_dbt.raw_customer.............................. [[32mOK[0m in 1.66s]
01:00:33.264255 [debug] [Thread-1  ]: Finished running node model.test_dbx.raw_customer
01:00:33.265047 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
01:00:33.265423 [info ] [Thread-1  ]: 2 of 2 START view model test_dbt.stg_customer................................... [RUN]
01:00:33.266193 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
01:00:33.269311 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
01:00:33.269562 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
01:00:33.308803 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
01:00:33.309774 [debug] [Thread-1  ]: finished collecting timing info
01:00:33.309992 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
01:00:33.313738 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.stg_customer"
01:00:33.314605 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
01:00:33.314800 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.stg_customer"
01:00:33.314974 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    'SAP' AS RECORD_SRC

    FROM source_data
),

hashed_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS CUSTOMER_HASHDIFF

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,
    CUSTOMER_HK,
    CUSTOMER_HASHDIFF

    FROM hashed_columns
)

SELECT * FROM columns_to_select

01:00:33.315142 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
01:00:33.643802 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    'SAP' AS RECORD_SRC

    FROM source_data
),

hashed_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS CUSTOMER_HASHDIFF

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,
    CUSTOMER_HK,
    CUSTOMER_HASHDIFF

    FROM hashed_columns
)

SELECT * FROM columns_to_select

01:00:33.644189 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
DataType binary(16) is not supported.(line 46, pos 80)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    'SAP' AS RECORD_SRC

    FROM source_data
),

hashed_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
--------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS CUSTOMER_HASHDIFF

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,
    CUSTOMER_HK,
    CUSTOMER_HASHDIFF

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
DataType binary(16) is not supported.(line 46, pos 80)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
create or replace view test_dbt.stg_customer
  
  as
    

-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL

    FROM test_dbt.raw_customer
),

derived_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    'SAP' AS RECORD_SRC

    FROM source_data
),

hashed_columns AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,

    CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
--------------------------------------------------------------------------------^^^
    CAST(MD5_BINARY(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
        IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
    )) AS BINARY(16)) AS CUSTOMER_HASHDIFF

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    CUSTOMER_ID,
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    RECORD_SRC,
    CUSTOMER_HK,
    CUSTOMER_HASHDIFF

    FROM hashed_columns
)

SELECT * FROM columns_to_select

	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPrimitiveDataType$1(AstBuilder.scala:2511)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPrimitiveDataType(AstBuilder.scala:2486)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPrimitiveDataType(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext.accept(SqlBaseParser.java:20748)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCast$1(AstBuilder.scala:1725)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCast(AstBuilder.scala:1724)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCast(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$CastContext.accept(SqlBaseParser.java:18038)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:1644)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:17636)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1373)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1521)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1520)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:17064)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:1602)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:16989)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1373)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpression$1(AstBuilder.scala:1407)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:1406)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext.accept(SqlBaseParser.java:16507)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpressionSeq$2(AstBuilder.scala:657)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpressionSeq(AstBuilder.scala:657)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:749)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:742)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:649)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:637)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:12729)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault(SqlBaseBaseVisitor.java:1161)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:12234)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:74)
	at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:1147)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:12003)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNoWithQuery$1(AstBuilder.scala:191)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNoWithQuery(AstBuilder.scala:191)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNoWithQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NoWithQueryContext.accept(SqlBaseParser.java:11565)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:121)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:120)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:8243)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedQuery$1(AstBuilder.scala:200)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedQuery(AstBuilder.scala:199)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withCTE$1(AstBuilder.scala:135)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.withCTE(AstBuilder.scala:134)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$2(AstBuilder.scala:124)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$EnhancedLogicalPlan$.optionalMap$extension(ParserUtils.scala:231)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:124)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:120)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:8243)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:64)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:115)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCreateView$1(AstBuilder.scala:4259)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCreateView(AstBuilder.scala:4200)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitCreateView(AstBuilder.scala:60)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateViewContext.accept(SqlBaseParser.java:2707)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:81)
	at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:124)
	at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:81)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:86)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

01:00:33.644703 [debug] [Thread-1  ]: finished collecting timing info
01:00:33.644971 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: ROLLBACK
01:00:33.645197 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
01:00:33.645413 [debug] [Thread-1  ]: On model.test_dbx.stg_customer: Close
01:00:33.827444 [debug] [Thread-1  ]: Runtime Error in model stg_customer (models/stage/stg_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  DataType binary(16) is not supported.(line 46, pos 80)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
  create or replace view test_dbt.stg_customer
    
    as
      
  
  -- Generated by dbtvault.
  
  
  
  WITH source_data AS (
  
      SELECT
  
      CUSTOMER_ID,
      FIRST_NAME,
      LAST_NAME,
      EMAIL
  
      FROM test_dbt.raw_customer
  ),
  
  derived_columns AS (
  
      SELECT
  
      CUSTOMER_ID,
      FIRST_NAME,
      LAST_NAME,
      EMAIL,
      'SAP' AS RECORD_SRC
  
      FROM source_data
  ),
  
  hashed_columns AS (
  
      SELECT
  
      CUSTOMER_ID,
      FIRST_NAME,
      LAST_NAME,
      EMAIL,
      RECORD_SRC,
  
      CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
  --------------------------------------------------------------------------------^^^
      CAST(MD5_BINARY(CONCAT_WS('||',
          IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
          IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
      )) AS BINARY(16)) AS CUSTOMER_HASHDIFF
  
      FROM derived_columns
  ),
  
  columns_to_select AS (
  
      SELECT
  
      CUSTOMER_ID,
      FIRST_NAME,
      LAST_NAME,
      EMAIL,
      RECORD_SRC,
      CUSTOMER_HK,
      CUSTOMER_HASHDIFF
  
      FROM hashed_columns
  )
  
  SELECT * FROM columns_to_select
01:00:33.828023 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4107294a-daf5-4177-9a10-f6b31bd6dbe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f67c580>]}
01:00:33.828500 [error] [Thread-1  ]: 2 of 2 ERROR creating view model test_dbt.stg_customer.......................... [[31mERROR[0m in 0.56s]
01:00:33.828978 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
01:00:33.830408 [debug] [MainThread]: Acquiring new databricks connection "master"
01:00:33.830708 [debug] [MainThread]: On master: ROLLBACK
01:00:33.830935 [debug] [MainThread]: Opening a new connection, currently in state init
01:00:33.947421 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:00:33.947883 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:00:33.948174 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:00:33.948474 [debug] [MainThread]: On master: ROLLBACK
01:00:33.948746 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:00:33.949016 [debug] [MainThread]: On master: Close
01:00:34.043135 [info ] [MainThread]: 
01:00:34.043685 [info ] [MainThread]: Finished running 2 view models in 5.87s.
01:00:34.044069 [debug] [MainThread]: Connection 'master' was properly closed.
01:00:34.044292 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
01:00:34.062053 [info ] [MainThread]: 
01:00:34.062434 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
01:00:34.062769 [info ] [MainThread]: 
01:00:34.063110 [error] [MainThread]: [33mRuntime Error in model stg_customer (models/stage/stg_customer.sql)[0m
01:00:34.063441 [error] [MainThread]:   Query execution failed.
01:00:34.063725 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
01:00:34.064013 [error] [MainThread]:   DataType binary(16) is not supported.(line 46, pos 80)
01:00:34.064306 [error] [MainThread]:   
01:00:34.064610 [error] [MainThread]:   == SQL ==
01:00:34.064886 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.stg_customer"} */
01:00:34.065160 [error] [MainThread]:   create or replace view test_dbt.stg_customer
01:00:34.065448 [error] [MainThread]:     
01:00:34.065734 [error] [MainThread]:     as
01:00:34.066017 [error] [MainThread]:       
01:00:34.066303 [error] [MainThread]:   
01:00:34.066596 [error] [MainThread]:   -- Generated by dbtvault.
01:00:34.066970 [error] [MainThread]:   
01:00:34.067300 [error] [MainThread]:   
01:00:34.067637 [error] [MainThread]:   
01:00:34.067984 [error] [MainThread]:   WITH source_data AS (
01:00:34.068487 [error] [MainThread]:   
01:00:34.068949 [error] [MainThread]:       SELECT
01:00:34.069263 [error] [MainThread]:   
01:00:34.069569 [error] [MainThread]:       CUSTOMER_ID,
01:00:34.069869 [error] [MainThread]:       FIRST_NAME,
01:00:34.070162 [error] [MainThread]:       LAST_NAME,
01:00:34.070682 [error] [MainThread]:       EMAIL
01:00:34.071034 [error] [MainThread]:   
01:00:34.071337 [error] [MainThread]:       FROM test_dbt.raw_customer
01:00:34.071705 [error] [MainThread]:   ),
01:00:34.072049 [error] [MainThread]:   
01:00:34.072416 [error] [MainThread]:   derived_columns AS (
01:00:34.072705 [error] [MainThread]:   
01:00:34.072999 [error] [MainThread]:       SELECT
01:00:34.073269 [error] [MainThread]:   
01:00:34.073654 [error] [MainThread]:       CUSTOMER_ID,
01:00:34.073935 [error] [MainThread]:       FIRST_NAME,
01:00:34.074234 [error] [MainThread]:       LAST_NAME,
01:00:34.074518 [error] [MainThread]:       EMAIL,
01:00:34.074834 [error] [MainThread]:       'SAP' AS RECORD_SRC
01:00:34.075125 [error] [MainThread]:   
01:00:34.075402 [error] [MainThread]:       FROM source_data
01:00:34.075678 [error] [MainThread]:   ),
01:00:34.075932 [error] [MainThread]:   
01:00:34.076192 [error] [MainThread]:   hashed_columns AS (
01:00:34.076460 [error] [MainThread]:   
01:00:34.076757 [error] [MainThread]:       SELECT
01:00:34.077021 [error] [MainThread]:   
01:00:34.077275 [error] [MainThread]:       CUSTOMER_ID,
01:00:34.077528 [error] [MainThread]:       FIRST_NAME,
01:00:34.077793 [error] [MainThread]:       LAST_NAME,
01:00:34.078087 [error] [MainThread]:       EMAIL,
01:00:34.078356 [error] [MainThread]:       RECORD_SRC,
01:00:34.078622 [error] [MainThread]:   
01:00:34.078885 [error] [MainThread]:       CAST((MD5_BINARY(NULLIF(UPPER(TRIM(CAST(CUSTOMER_ID AS VARCHAR))), ''))) AS BINARY(16)) AS CUSTOMER_HK,
01:00:34.079222 [error] [MainThread]:   --------------------------------------------------------------------------------^^^
01:00:34.079483 [error] [MainThread]:       CAST(MD5_BINARY(CONCAT_WS('||',
01:00:34.079732 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(EMAIL AS VARCHAR))), ''), '^^'),
01:00:34.079978 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(FIRST_NAME AS VARCHAR))), ''), '^^'),
01:00:34.080219 [error] [MainThread]:           IFNULL(NULLIF(UPPER(TRIM(CAST(LAST_NAME AS VARCHAR))), ''), '^^')
01:00:34.080480 [error] [MainThread]:       )) AS BINARY(16)) AS CUSTOMER_HASHDIFF
01:00:34.080725 [error] [MainThread]:   
01:00:34.080963 [error] [MainThread]:       FROM derived_columns
01:00:34.081201 [error] [MainThread]:   ),
01:00:34.081436 [error] [MainThread]:   
01:00:34.081681 [error] [MainThread]:   columns_to_select AS (
01:00:34.081909 [error] [MainThread]:   
01:00:34.082138 [error] [MainThread]:       SELECT
01:00:34.082367 [error] [MainThread]:   
01:00:34.082593 [error] [MainThread]:       CUSTOMER_ID,
01:00:34.082820 [error] [MainThread]:       FIRST_NAME,
01:00:34.083049 [error] [MainThread]:       LAST_NAME,
01:00:34.083275 [error] [MainThread]:       EMAIL,
01:00:34.083513 [error] [MainThread]:       RECORD_SRC,
01:00:34.083791 [error] [MainThread]:       CUSTOMER_HK,
01:00:34.084056 [error] [MainThread]:       CUSTOMER_HASHDIFF
01:00:34.084466 [error] [MainThread]:   
01:00:34.084776 [error] [MainThread]:       FROM hashed_columns
01:00:34.085183 [error] [MainThread]:   )
01:00:34.085643 [error] [MainThread]:   
01:00:34.085983 [error] [MainThread]:   SELECT * FROM columns_to_select
01:00:34.086304 [info ] [MainThread]: 
01:00:34.086588 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
01:00:34.086980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f4fee80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f546670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6a2fd0>]}


============================== 2022-03-23 01:49:26.723230 | 12daaf51-7549-478b-abe4-7f2bee6a4fd5 ==============================
01:49:26.723230 [info ] [MainThread]: Running with dbt=1.0.3
01:49:26.724052 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
01:49:26.724376 [debug] [MainThread]: Tracking: tracking
01:49:26.777540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b3efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11193db20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b4cbe0>]}
01:49:26.929886 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
01:49:26.930328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '12daaf51-7549-478b-abe4-7f2bee6a4fd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b4c2b0>]}
01:49:29.061181 [debug] [MainThread]: Parsing macros/adapters.sql
01:49:29.109398 [debug] [MainThread]: Parsing macros/materializations/seed.sql
01:49:29.114102 [debug] [MainThread]: Parsing macros/materializations/view.sql
01:49:29.114872 [debug] [MainThread]: Parsing macros/materializations/table.sql
01:49:29.118317 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
01:49:29.138215 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
01:49:29.143765 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
01:49:29.147942 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
01:49:29.153301 [debug] [MainThread]: Parsing macros/adapters.sql
01:49:29.189648 [debug] [MainThread]: Parsing macros/materializations/seed.sql
01:49:29.198242 [debug] [MainThread]: Parsing macros/materializations/view.sql
01:49:29.198766 [debug] [MainThread]: Parsing macros/materializations/table.sql
01:49:29.201401 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
01:49:29.223471 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
01:49:29.228066 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
01:49:29.233647 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
01:49:29.238868 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
01:49:29.242343 [debug] [MainThread]: Parsing macros/materializations/configs.sql
01:49:29.244429 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
01:49:29.245899 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
01:49:29.261058 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
01:49:29.271186 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
01:49:29.281635 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
01:49:29.285606 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
01:49:29.287238 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
01:49:29.288828 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
01:49:29.292913 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
01:49:29.302860 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
01:49:29.304176 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
01:49:29.313486 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
01:49:29.327679 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
01:49:29.334334 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
01:49:29.336990 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
01:49:29.343440 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
01:49:29.344605 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
01:49:29.346956 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
01:49:29.349070 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
01:49:29.354525 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
01:49:29.369287 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
01:49:29.370602 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
01:49:29.372761 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
01:49:29.374146 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
01:49:29.374977 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
01:49:29.375483 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
01:49:29.376109 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
01:49:29.377321 [debug] [MainThread]: Parsing macros/etc/statement.sql
01:49:29.381200 [debug] [MainThread]: Parsing macros/etc/datetime.sql
01:49:29.388662 [debug] [MainThread]: Parsing macros/adapters/schema.sql
01:49:29.390583 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
01:49:29.393353 [debug] [MainThread]: Parsing macros/adapters/relation.sql
01:49:29.402027 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
01:49:29.404575 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
01:49:29.408564 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
01:49:29.415106 [debug] [MainThread]: Parsing macros/adapters/columns.sql
01:49:29.423906 [debug] [MainThread]: Parsing tests/generic/builtin.sql
01:49:29.426815 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
01:49:29.427777 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
01:49:29.428892 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
01:49:29.429957 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
01:49:29.435321 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
01:49:29.436319 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
01:49:29.437461 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
01:49:29.439951 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
01:49:29.440959 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
01:49:29.442592 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
01:49:29.444698 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
01:49:29.453464 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
01:49:29.455095 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
01:49:29.456350 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
01:49:29.457623 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
01:49:29.459123 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
01:49:29.460372 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
01:49:29.461746 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
01:49:29.462660 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
01:49:29.465663 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
01:49:29.470287 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
01:49:29.471678 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
01:49:29.474761 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
01:49:29.476370 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
01:49:29.477682 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
01:49:29.479361 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
01:49:29.501582 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
01:49:29.503357 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
01:49:29.505626 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
01:49:29.507008 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
01:49:29.508149 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
01:49:29.509230 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
01:49:29.510232 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
01:49:29.511285 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
01:49:29.512839 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
01:49:29.514363 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
01:49:29.516378 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
01:49:29.517912 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
01:49:29.519013 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
01:49:29.521212 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
01:49:29.523108 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
01:49:29.524379 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
01:49:29.525631 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
01:49:29.528145 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
01:49:29.529899 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
01:49:29.531582 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
01:49:29.533502 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
01:49:29.535983 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
01:49:29.537245 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
01:49:29.540329 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
01:49:29.548855 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
01:49:29.552571 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
01:49:29.554015 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
01:49:29.557126 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
01:49:29.560936 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
01:49:29.563991 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
01:49:29.565579 [debug] [MainThread]: Parsing macros/sql/star.sql
01:49:29.569652 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
01:49:29.576614 [debug] [MainThread]: Parsing macros/sql/union.sql
01:49:29.585744 [debug] [MainThread]: Parsing macros/sql/groupby.sql
01:49:29.586949 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
01:49:29.589984 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
01:49:29.591595 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
01:49:29.593792 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
01:49:29.601998 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
01:49:29.607852 [debug] [MainThread]: Parsing macros/sql/pivot.sql
01:49:29.612876 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
01:49:29.615016 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
01:49:29.616179 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
01:49:29.621384 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
01:49:29.645903 [debug] [MainThread]: Parsing macros/get_base_dates.sql
01:49:29.651016 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
01:49:29.655111 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
01:49:29.656923 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
01:49:29.657590 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
01:49:29.658147 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
01:49:29.658858 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
01:49:29.659414 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
01:49:29.662427 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
01:49:29.664033 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
01:49:29.664846 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
01:49:29.667456 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
01:49:29.669837 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
01:49:29.670855 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
01:49:29.671455 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
01:49:29.672846 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
01:49:29.673760 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
01:49:29.674347 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
01:49:29.674910 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
01:49:29.676762 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
01:49:29.682636 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
01:49:29.683634 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
01:49:29.685074 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
01:49:29.685965 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
01:49:29.686972 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
01:49:29.687602 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
01:49:29.694564 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
01:49:29.696687 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
01:49:29.697603 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
01:49:29.700386 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
01:49:29.701085 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
01:49:29.702814 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
01:49:29.707055 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
01:49:29.707939 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
01:49:29.710439 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
01:49:29.712674 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
01:49:29.713320 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
01:49:29.713963 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
01:49:29.715650 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
01:49:29.717391 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
01:49:29.718206 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
01:49:29.733947 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
01:49:29.734914 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
01:49:29.743574 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
01:49:29.754387 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
01:49:29.767644 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
01:49:29.778831 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
01:49:29.787108 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
01:49:29.811021 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
01:49:29.824386 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
01:49:29.830957 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
01:49:29.841186 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
01:49:29.853365 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
01:49:29.867914 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
01:49:29.881354 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
01:49:29.921055 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
01:49:29.921961 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
01:49:29.938027 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
01:49:29.938964 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
01:49:29.947644 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
01:49:29.959664 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
01:49:29.972481 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
01:49:29.985211 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
01:49:29.988731 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
01:49:29.989859 [debug] [MainThread]: Parsing macros/staging/stage.sql
01:49:29.999917 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
01:49:30.008431 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
01:49:30.014158 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
01:49:30.016511 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
01:49:30.028796 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
01:49:30.031942 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
01:49:30.033939 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
01:49:30.038646 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
01:49:30.041588 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
01:49:30.043128 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
01:49:30.045278 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
01:49:30.045661 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
01:49:30.049814 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
01:49:30.058897 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
01:49:30.073693 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
01:49:30.078590 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
01:49:30.079555 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
01:49:30.103404 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
01:49:30.105523 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
01:49:30.110487 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
01:49:30.116188 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
01:49:30.119217 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
01:49:30.122655 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
01:49:30.129364 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
01:49:30.139437 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
01:49:30.143657 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
01:49:30.148606 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
01:49:30.153239 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
01:49:30.154263 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
01:49:30.155802 [debug] [MainThread]: Parsing macros/supporting/hash.sql
01:49:30.176177 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
01:49:30.181215 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
01:49:30.183145 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
01:49:30.992575 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
01:49:31.002098 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
01:49:31.036324 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
01:49:31.037339 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
01:49:31.122667 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
01:49:31.249599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '12daaf51-7549-478b-abe4-7f2bee6a4fd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e6c0d0>]}
01:49:31.359522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '12daaf51-7549-478b-abe4-7f2bee6a4fd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b4f760>]}
01:49:31.359902 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:49:31.361249 [info ] [MainThread]: 
01:49:31.361856 [debug] [MainThread]: Acquiring new databricks connection "master"
01:49:31.362912 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:49:31.406335 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:49:31.406657 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:49:31.406863 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:49:31.407062 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:49:32.788452 [debug] [ThreadPool]: SQL status: OK in 1.38 seconds
01:49:33.114360 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:49:33.114668 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:49:33.114895 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:49:33.345592 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:49:33.346111 [info ] [MainThread]: 
01:49:33.399624 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
01:49:33.400191 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
01:49:33.403045 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
01:49:33.403312 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
01:49:33.412204 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
01:49:33.413484 [debug] [Thread-1  ]: finished collecting timing info
01:49:33.413709 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
01:49:33.413927 [debug] [Thread-1  ]: finished collecting timing info
01:49:33.414328 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
01:49:33.415241 [debug] [MainThread]: Connection 'master' was properly closed.
01:49:33.415454 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
01:49:33.451422 [info ] [MainThread]: Done.
01:49:33.452064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111dad220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111dad550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111dadf40>]}


============================== 2022-03-23 01:50:29.965098 | f9f4606b-a9a7-49df-913a-421efbd7801a ==============================
01:50:29.965098 [info ] [MainThread]: Running with dbt=1.0.3
01:50:29.965770 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
01:50:29.966008 [debug] [MainThread]: Tracking: tracking
01:50:29.996036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cac1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cbc190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cbc490>]}
01:50:30.317938 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
01:50:30.318223 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
01:50:30.326260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9f4606b-a9a7-49df-913a-421efbd7801a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107facaf0>]}
01:50:30.345933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9f4606b-a9a7-49df-913a-421efbd7801a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e7b580>]}
01:50:30.346307 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:50:30.347606 [info ] [MainThread]: 
01:50:30.348177 [debug] [MainThread]: Acquiring new databricks connection "master"
01:50:30.349087 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
01:50:30.360302 [debug] [ThreadPool]: Using databricks connection "list_schemas"
01:50:30.360576 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
01:50:30.360768 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:50:30.864278 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
01:50:31.148047 [debug] [ThreadPool]: On list_schemas: Close
01:50:31.317500 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:50:31.327120 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:50:31.327361 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:50:31.327540 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:50:31.327706 [debug] [ThreadPool]: Opening a new connection, currently in state closed
01:50:32.038435 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
01:50:32.320764 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:50:32.321087 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:50:32.321322 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:50:32.485978 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:50:32.486387 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:50:32.486960 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:50:32.487475 [info ] [MainThread]: 
01:50:32.496610 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
01:50:32.497020 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
01:50:32.497602 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
01:50:32.500277 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
01:50:32.500548 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
01:50:32.546456 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
01:50:32.548366 [debug] [Thread-1  ]: finished collecting timing info
01:50:32.548591 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
01:50:32.608484 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
01:50:32.609962 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
01:50:32.610286 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
01:50:32.610566 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      SELECT '1'
01:50:32.610815 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
01:50:40.043824 [debug] [Thread-1  ]: SQL status: OK in 7.43 seconds
01:50:40.050543 [debug] [Thread-1  ]: finished collecting timing info
01:50:40.050818 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
01:50:40.051018 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
01:50:40.051203 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
01:50:40.235263 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9f4606b-a9a7-49df-913a-421efbd7801a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10807d2b0>]}
01:50:40.235992 [info ] [Thread-1  ]: 1 of 1 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 7.74s]
01:50:40.236613 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
01:50:40.238051 [debug] [MainThread]: Acquiring new databricks connection "master"
01:50:40.238334 [debug] [MainThread]: On master: ROLLBACK
01:50:40.238557 [debug] [MainThread]: Opening a new connection, currently in state init
01:50:40.338943 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:50:40.339340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:50:40.339566 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:50:40.339804 [debug] [MainThread]: On master: ROLLBACK
01:50:40.340020 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:50:40.340229 [debug] [MainThread]: On master: Close
01:50:40.429917 [info ] [MainThread]: 
01:50:40.430529 [info ] [MainThread]: Finished running 1 incremental model in 10.08s.
01:50:40.431004 [debug] [MainThread]: Connection 'master' was properly closed.
01:50:40.431320 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
01:50:40.451770 [info ] [MainThread]: 
01:50:40.452136 [info ] [MainThread]: [32mCompleted successfully[0m
01:50:40.452478 [info ] [MainThread]: 
01:50:40.452778 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
01:50:40.453231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e7b520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10810ddf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fcd610>]}


============================== 2022-03-23 01:53:28.003161 | a9bd5118-09e6-4d2a-9871-419960f95ec9 ==============================
01:53:28.003161 [info ] [MainThread]: Running with dbt=1.0.3
01:53:28.003963 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
01:53:28.004287 [debug] [MainThread]: Tracking: tracking
01:53:28.022030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10573a880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10573a2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10573a970>]}
01:53:28.317831 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
01:53:28.318320 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
01:53:28.333321 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
01:53:28.476893 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
01:53:28.501350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a9bd5118-09e6-4d2a-9871-419960f95ec9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ad30d0>]}
01:53:28.515447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a9bd5118-09e6-4d2a-9871-419960f95ec9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059080a0>]}
01:53:28.515785 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:53:28.517020 [info ] [MainThread]: 
01:53:28.517529 [debug] [MainThread]: Acquiring new databricks connection "master"
01:53:28.518401 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
01:53:28.528909 [debug] [ThreadPool]: Using databricks connection "list_schemas"
01:53:28.529199 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
01:53:28.529384 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:53:29.096802 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
01:53:29.485705 [debug] [ThreadPool]: On list_schemas: Close
01:53:29.675018 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:53:29.684817 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:53:29.685051 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:53:29.685297 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:53:29.685514 [debug] [ThreadPool]: Opening a new connection, currently in state closed
01:53:30.568868 [debug] [ThreadPool]: SQL status: OK in 0.88 seconds
01:53:30.836471 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:53:30.836819 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:53:30.837008 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:53:31.011701 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:53:31.012032 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:53:31.012476 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:53:31.012877 [info ] [MainThread]: 
01:53:31.017531 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
01:53:31.017945 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
01:53:31.018589 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
01:53:31.021382 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
01:53:31.021702 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
01:53:31.048398 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
01:53:31.049485 [debug] [Thread-1  ]: finished collecting timing info
01:53:31.049825 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
01:53:31.095981 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
01:53:31.096256 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
drop table if exists test_dbt.hub_customer
01:53:31.096425 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
01:53:31.853209 [debug] [Thread-1  ]: SQL status: OK in 0.76 seconds
01:53:31.885020 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
01:53:31.898864 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
01:53:31.899236 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
01:53:31.899484 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      SELECT '1'
01:53:43.283997 [debug] [Thread-1  ]: SQL status: OK in 11.38 seconds
01:53:43.291343 [debug] [Thread-1  ]: finished collecting timing info
01:53:43.291589 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
01:53:43.291781 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
01:53:43.291961 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
01:53:43.497791 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a9bd5118-09e6-4d2a-9871-419960f95ec9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c62fa0>]}
01:53:43.498443 [info ] [Thread-1  ]: 1 of 1 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 12.48s]
01:53:43.498919 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
01:53:43.500296 [debug] [MainThread]: Acquiring new databricks connection "master"
01:53:43.500585 [debug] [MainThread]: On master: ROLLBACK
01:53:43.500812 [debug] [MainThread]: Opening a new connection, currently in state init
01:53:43.608281 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:53:43.608714 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:53:43.608992 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:53:43.609288 [debug] [MainThread]: On master: ROLLBACK
01:53:43.609553 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:53:43.609809 [debug] [MainThread]: On master: Close
01:53:43.700838 [info ] [MainThread]: 
01:53:43.701442 [info ] [MainThread]: Finished running 1 incremental model in 15.18s.
01:53:43.701921 [debug] [MainThread]: Connection 'master' was properly closed.
01:53:43.702206 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
01:53:43.722780 [info ] [MainThread]: 
01:53:43.723194 [info ] [MainThread]: [32mCompleted successfully[0m
01:53:43.723572 [info ] [MainThread]: 
01:53:43.724268 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
01:53:43.725162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059f3be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059f3b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105aea2b0>]}


============================== 2022-03-23 01:54:07.555318 | 38469fd4-5673-421b-850e-c638f652343a ==============================
01:54:07.555318 [info ] [MainThread]: Running with dbt=1.0.3
01:54:07.556087 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
01:54:07.556426 [debug] [MainThread]: Tracking: tracking
01:54:07.577005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042b7e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042c4dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042c4e50>]}
01:54:07.868357 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
01:54:07.868657 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
01:54:07.877010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '38469fd4-5673-421b-850e-c638f652343a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045b3af0>]}
01:54:07.900414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '38469fd4-5673-421b-850e-c638f652343a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10448a640>]}
01:54:07.900793 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:54:07.902170 [info ] [MainThread]: 
01:54:07.902749 [debug] [MainThread]: Acquiring new databricks connection "master"
01:54:07.903760 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
01:54:07.915038 [debug] [ThreadPool]: Using databricks connection "list_schemas"
01:54:07.915331 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
01:54:07.915544 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:54:08.267718 [debug] [ThreadPool]: SQL status: OK in 0.35 seconds
01:54:08.545489 [debug] [ThreadPool]: On list_schemas: Close
01:54:08.715568 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:54:08.725640 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:54:08.725922 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:54:08.726124 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:54:08.726308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
01:54:09.416147 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
01:54:09.770389 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:54:09.770734 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:54:09.770962 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:54:09.947754 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:54:09.948218 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:54:09.949202 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:54:09.950006 [info ] [MainThread]: 
01:54:09.961218 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
01:54:09.961799 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
01:54:09.962733 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
01:54:09.966963 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
01:54:09.967464 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
01:54:10.028549 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
01:54:10.029445 [debug] [Thread-1  ]: finished collecting timing info
01:54:10.029657 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
01:54:10.086842 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
01:54:10.087187 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    create temporary view hub_customer__dbt_tmp as
    SELECT '1'

  
01:54:10.087677 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
01:54:10.433503 [debug] [Thread-1  ]: SQL status: OK in 0.35 seconds
01:54:10.881220 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
01:54:10.896768 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
01:54:10.897041 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
01:54:10.897234 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    
  
  
    merge into test_dbt.hub_customer as DBT_INTERNAL_DEST
      using hub_customer__dbt_tmp as DBT_INTERNAL_SOURCE
      
      
    
        on false
    
  
      
      when matched then update set
         * 
    
      when not matched then insert *

01:54:14.166431 [debug] [Thread-1  ]: SQL status: OK in 3.27 seconds
01:54:14.172356 [debug] [Thread-1  ]: finished collecting timing info
01:54:14.172647 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
01:54:14.172843 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
01:54:14.173026 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
01:54:14.361501 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '38469fd4-5673-421b-850e-c638f652343a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046621c0>]}
01:54:14.362183 [info ] [Thread-1  ]: 1 of 1 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 4.40s]
01:54:14.362681 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
01:54:14.364017 [debug] [MainThread]: Acquiring new databricks connection "master"
01:54:14.364309 [debug] [MainThread]: On master: ROLLBACK
01:54:14.364540 [debug] [MainThread]: Opening a new connection, currently in state init
01:54:14.491964 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:54:14.492340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
01:54:14.492585 [debug] [MainThread]: Spark adapter: NotImplemented: commit
01:54:14.492832 [debug] [MainThread]: On master: ROLLBACK
01:54:14.493053 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
01:54:14.493231 [debug] [MainThread]: On master: Close
01:54:14.582204 [info ] [MainThread]: 
01:54:14.582658 [info ] [MainThread]: Finished running 1 incremental model in 6.68s.
01:54:14.582993 [debug] [MainThread]: Connection 'master' was properly closed.
01:54:14.583240 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
01:54:14.604901 [info ] [MainThread]: 
01:54:14.605292 [info ] [MainThread]: [32mCompleted successfully[0m
01:54:14.605649 [info ] [MainThread]: 
01:54:14.606002 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
01:54:14.606461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10468e370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10468eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10468ed90>]}


============================== 2022-03-23 01:57:37.265101 | 6e7edcde-6e05-4619-aafa-03ef70c1f024 ==============================
01:57:37.265101 [info ] [MainThread]: Running with dbt=1.0.3
01:57:37.266288 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=True, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
01:57:37.266555 [debug] [MainThread]: Tracking: tracking
01:57:37.285951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106744940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10674dc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10674d790>]}
01:57:37.640185 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
01:57:37.640711 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
01:57:37.655702 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
01:57:37.704119 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
01:57:37.717714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e7edcde-6e05-4619-aafa-03ef70c1f024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a28fd0>]}
01:57:37.729637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e7edcde-6e05-4619-aafa-03ef70c1f024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10691e070>]}
01:57:37.729927 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
01:57:37.731074 [info ] [MainThread]: 
01:57:37.731529 [debug] [MainThread]: Acquiring new databricks connection "master"
01:57:37.732340 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
01:57:37.742414 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
01:57:37.742677 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
01:57:37.742852 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
01:57:37.743018 [debug] [ThreadPool]: Opening a new connection, currently in state init
01:57:38.737210 [debug] [ThreadPool]: SQL status: OK in 0.99 seconds
01:57:39.145120 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
01:57:39.145483 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
01:57:39.145728 [debug] [ThreadPool]: On list_None_test_dbt: Close
01:57:39.327216 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
01:57:39.327828 [info ] [MainThread]: 
01:57:39.334936 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
01:57:39.335500 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
01:57:39.338245 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
01:57:39.338577 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
01:57:39.348927 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
01:57:39.350870 [debug] [Thread-1  ]: finished collecting timing info
01:57:39.351180 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
01:57:39.351454 [debug] [Thread-1  ]: finished collecting timing info
01:57:39.351999 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
01:57:39.352983 [debug] [MainThread]: Connection 'master' was properly closed.
01:57:39.353238 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
01:57:39.369673 [info ] [MainThread]: Done.
01:57:39.370147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5d1c0>]}


============================== 2022-03-23 02:47:10.519085 | ddec0a2d-be83-4968-91b6-789662c45f49 ==============================
02:47:10.519085 [info ] [MainThread]: Running with dbt=1.0.3
02:47:10.520191 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['stg_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
02:47:10.520523 [debug] [MainThread]: Tracking: tracking
02:47:10.567666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111944880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119442b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111944970>]}
02:47:10.925419 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
02:47:10.935775 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
02:47:10.948243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddec0a2d-be83-4968-91b6-789662c45f49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c35fa0>]}
02:47:10.964296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddec0a2d-be83-4968-91b6-789662c45f49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b055e0>]}
02:47:10.964654 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
02:47:10.966054 [info ] [MainThread]: 
02:47:10.966604 [debug] [MainThread]: Acquiring new databricks connection "master"
02:47:10.967588 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
02:47:10.979090 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
02:47:10.979368 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
02:47:10.979570 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
02:47:10.979741 [debug] [ThreadPool]: Opening a new connection, currently in state init
02:47:12.177862 [debug] [ThreadPool]: SQL status: OK in 1.2 seconds
02:47:12.700257 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
02:47:12.700593 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
02:47:12.703452 [debug] [ThreadPool]: On list_None_test_dbt: Close
02:47:12.878116 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
02:47:12.878603 [info ] [MainThread]: 
02:47:12.897850 [debug] [Thread-1  ]: Began running node model.test_dbx.stg_customer
02:47:12.898642 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.stg_customer"
02:47:12.901352 [debug] [Thread-1  ]: Began compiling node model.test_dbx.stg_customer
02:47:12.901628 [debug] [Thread-1  ]: Compiling model.test_dbx.stg_customer
02:47:13.044065 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.stg_customer"
02:47:13.049561 [debug] [Thread-1  ]: finished collecting timing info
02:47:13.049832 [debug] [Thread-1  ]: Began executing node model.test_dbx.stg_customer
02:47:13.050272 [debug] [Thread-1  ]: finished collecting timing info
02:47:13.050741 [debug] [Thread-1  ]: Finished running node model.test_dbx.stg_customer
02:47:13.051506 [debug] [MainThread]: Connection 'master' was properly closed.
02:47:13.051691 [debug] [MainThread]: Connection 'model.test_dbx.stg_customer' was properly closed.
02:47:13.065364 [info ] [MainThread]: Done.
02:47:13.065790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cf5130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cf5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111cf5d30>]}


============================== 2022-03-23 02:47:23.712144 | e507648c-b03c-4e6d-8efb-2e6715016cdc ==============================
02:47:23.712144 [info ] [MainThread]: Running with dbt=1.0.3
02:47:23.713922 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=True, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
02:47:23.714641 [debug] [MainThread]: Tracking: tracking
02:47:23.762504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094cc040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094d52e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094d5490>]}
02:47:24.109584 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
02:47:24.109892 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
02:47:24.119252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e507648c-b03c-4e6d-8efb-2e6715016cdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097c6f40>]}
02:47:24.146702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e507648c-b03c-4e6d-8efb-2e6715016cdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096956a0>]}
02:47:24.147088 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
02:47:24.148455 [info ] [MainThread]: 
02:47:24.149038 [debug] [MainThread]: Acquiring new databricks connection "master"
02:47:24.150078 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
02:47:24.163454 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
02:47:24.163772 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
02:47:24.163991 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
02:47:24.164181 [debug] [ThreadPool]: Opening a new connection, currently in state init
02:47:25.060359 [debug] [ThreadPool]: SQL status: OK in 0.9 seconds
02:47:25.352073 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
02:47:25.352436 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
02:47:25.352672 [debug] [ThreadPool]: On list_None_test_dbt: Close
02:47:25.542076 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
02:47:25.542893 [info ] [MainThread]: 
02:47:25.549606 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
02:47:25.550260 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
02:47:25.553548 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
02:47:25.553946 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
02:47:25.627484 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
02:47:25.643012 [debug] [Thread-1  ]: finished collecting timing info
02:47:25.643292 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
02:47:25.643498 [debug] [Thread-1  ]: finished collecting timing info
02:47:25.643858 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
02:47:25.644638 [debug] [MainThread]: Connection 'master' was properly closed.
02:47:25.644940 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
02:47:25.657991 [info ] [MainThread]: Done.
02:47:25.658488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109915dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109915ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109915f40>]}


============================== 2022-03-23 04:36:35.417248 | 4364ad3a-1c54-4707-9ace-8dd6d8f1220d ==============================
04:36:35.417248 [info ] [MainThread]: Running with dbt=1.0.3
04:36:35.418535 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
04:36:35.418829 [debug] [MainThread]: Tracking: tracking
04:36:35.465335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107e7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107f3df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107f36a0>]}
04:36:35.559585 [info ] [MainThread]: Unable to do partial parsing because profile has changed
04:36:35.560082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4364ad3a-1c54-4707-9ace-8dd6d8f1220d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107f30d0>]}
04:36:35.818234 [debug] [MainThread]: Parsing macros/adapters.sql
04:36:35.855485 [debug] [MainThread]: Parsing macros/materializations/seed.sql
04:36:35.860053 [debug] [MainThread]: Parsing macros/materializations/view.sql
04:36:35.860763 [debug] [MainThread]: Parsing macros/materializations/table.sql
04:36:35.864108 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
04:36:35.885266 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
04:36:35.891272 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
04:36:35.895926 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
04:36:35.902412 [debug] [MainThread]: Parsing macros/adapters.sql
04:36:35.947197 [debug] [MainThread]: Parsing macros/materializations/seed.sql
04:36:35.958868 [debug] [MainThread]: Parsing macros/materializations/view.sql
04:36:35.959603 [debug] [MainThread]: Parsing macros/materializations/table.sql
04:36:35.962980 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
04:36:35.991070 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
04:36:35.996966 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
04:36:36.004329 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
04:36:36.011009 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
04:36:36.015543 [debug] [MainThread]: Parsing macros/materializations/configs.sql
04:36:36.018134 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
04:36:36.019977 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
04:36:36.039654 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
04:36:36.052316 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
04:36:36.066107 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
04:36:36.071208 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
04:36:36.073296 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
04:36:36.075341 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
04:36:36.080432 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
04:36:36.095125 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
04:36:36.114160 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
04:36:36.127743 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
04:36:36.152756 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
04:36:36.162220 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
04:36:36.165737 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
04:36:36.174032 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
04:36:36.175634 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
04:36:36.178616 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
04:36:36.181185 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
04:36:36.188292 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
04:36:36.207064 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
04:36:36.208814 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
04:36:36.211504 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
04:36:36.213296 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
04:36:36.214368 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
04:36:36.215004 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
04:36:36.215789 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
04:36:36.217318 [debug] [MainThread]: Parsing macros/etc/statement.sql
04:36:36.222498 [debug] [MainThread]: Parsing macros/etc/datetime.sql
04:36:36.232162 [debug] [MainThread]: Parsing macros/adapters/schema.sql
04:36:36.234658 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
04:36:36.237826 [debug] [MainThread]: Parsing macros/adapters/relation.sql
04:36:36.248867 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
04:36:36.252196 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
04:36:36.257780 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
04:36:36.266119 [debug] [MainThread]: Parsing macros/adapters/columns.sql
04:36:36.277033 [debug] [MainThread]: Parsing tests/generic/builtin.sql
04:36:36.280646 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
04:36:36.282006 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
04:36:36.283413 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
04:36:36.284752 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
04:36:36.291657 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
04:36:36.293001 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
04:36:36.294485 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
04:36:36.297759 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
04:36:36.299048 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
04:36:36.301095 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
04:36:36.304063 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
04:36:36.316557 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
04:36:36.318764 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
04:36:36.320540 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
04:36:36.322480 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
04:36:36.324436 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
04:36:36.326099 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
04:36:36.327877 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
04:36:36.329033 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
04:36:36.333022 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
04:36:36.339437 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
04:36:36.341514 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
04:36:36.345541 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
04:36:36.347632 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
04:36:36.349294 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
04:36:36.351405 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
04:36:36.379624 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
04:36:36.382135 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
04:36:36.384993 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
04:36:36.386809 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
04:36:36.388219 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
04:36:36.389710 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
04:36:36.391017 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
04:36:36.392365 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
04:36:36.394200 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
04:36:36.396015 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
04:36:36.398544 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
04:36:36.400595 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
04:36:36.402004 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
04:36:36.405130 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
04:36:36.407836 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
04:36:36.409500 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
04:36:36.410926 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
04:36:36.414207 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
04:36:36.416390 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
04:36:36.418577 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
04:36:36.421039 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
04:36:36.424451 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
04:36:36.426090 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
04:36:36.430056 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
04:36:36.440423 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
04:36:36.445112 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
04:36:36.446922 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
04:36:36.450903 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
04:36:36.455910 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
04:36:36.459679 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
04:36:36.461742 [debug] [MainThread]: Parsing macros/sql/star.sql
04:36:36.467209 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
04:36:36.476449 [debug] [MainThread]: Parsing macros/sql/union.sql
04:36:36.488119 [debug] [MainThread]: Parsing macros/sql/groupby.sql
04:36:36.489779 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
04:36:36.494058 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
04:36:36.495879 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
04:36:36.497783 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
04:36:36.504872 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
04:36:36.510790 [debug] [MainThread]: Parsing macros/sql/pivot.sql
04:36:36.515663 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
04:36:36.518336 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
04:36:36.519833 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
04:36:36.526483 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
04:36:36.554592 [debug] [MainThread]: Parsing macros/get_base_dates.sql
04:36:36.560658 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
04:36:36.565375 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
04:36:36.567350 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
04:36:36.568199 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
04:36:36.568904 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
04:36:36.569733 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
04:36:36.570414 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
04:36:36.574039 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
04:36:36.575988 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
04:36:36.576942 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
04:36:36.580208 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
04:36:36.583153 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
04:36:36.584357 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
04:36:36.585023 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
04:36:36.585774 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
04:36:36.586657 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
04:36:36.587293 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
04:36:36.588014 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
04:36:36.590577 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
04:36:36.596966 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
04:36:36.598252 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
04:36:36.599864 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
04:36:36.600921 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
04:36:36.602181 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
04:36:36.602942 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
04:36:36.611631 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
04:36:36.614383 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
04:36:36.615499 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
04:36:36.618825 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
04:36:36.619675 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
04:36:36.622419 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
04:36:36.627643 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
04:36:36.628732 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
04:36:36.632022 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
04:36:36.634702 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
04:36:36.635477 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
04:36:36.636252 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
04:36:36.638323 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
04:36:36.640478 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
04:36:36.641500 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
04:36:36.660808 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
04:36:36.661999 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
04:36:36.672687 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
04:36:36.685924 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
04:36:36.701995 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
04:36:36.715571 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
04:36:36.726005 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
04:36:36.754581 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
04:36:36.770580 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
04:36:36.778622 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
04:36:36.791236 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
04:36:36.806361 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
04:36:36.824456 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
04:36:36.840920 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
04:36:36.888256 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
04:36:36.889443 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
04:36:36.908991 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
04:36:36.910215 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
04:36:36.921051 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
04:36:36.936091 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
04:36:36.951665 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
04:36:36.967006 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
04:36:36.971492 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
04:36:36.973183 [debug] [MainThread]: Parsing macros/staging/stage.sql
04:36:36.985622 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
04:36:36.996423 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
04:36:37.003692 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
04:36:37.007259 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
04:36:37.022821 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
04:36:37.026823 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
04:36:37.029407 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
04:36:37.035391 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
04:36:37.039378 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
04:36:37.041341 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
04:36:37.043951 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
04:36:37.044423 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
04:36:37.049793 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
04:36:37.060498 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
04:36:37.079459 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
04:36:37.085820 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
04:36:37.087268 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
04:36:37.117654 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
04:36:37.120583 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
04:36:37.126779 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
04:36:37.133822 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
04:36:37.137668 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
04:36:37.142319 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
04:36:37.150444 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
04:36:37.163456 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
04:36:37.168666 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
04:36:37.174948 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
04:36:37.180878 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
04:36:37.182227 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
04:36:37.184187 [debug] [MainThread]: Parsing macros/supporting/hash.sql
04:36:37.209847 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
04:36:37.216345 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
04:36:37.218929 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
04:36:38.201522 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
04:36:38.213487 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
04:36:38.219235 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
04:36:38.220516 [debug] [MainThread]: 1699: static parser successfully parsed stage/stg_customer.sql
04:36:38.353992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4364ad3a-1c54-4707-9ace-8dd6d8f1220d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b0cfa0>]}
04:36:38.371593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4364ad3a-1c54-4707-9ace-8dd6d8f1220d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109d0550>]}
04:36:38.371968 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 8 sources, 0 exposures, 0 metrics
04:36:38.373550 [info ] [MainThread]: 
04:36:38.374144 [debug] [MainThread]: Acquiring new databricks connection "master"
04:36:38.375111 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
04:36:38.386302 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
04:36:38.386616 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
04:36:38.386805 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
04:36:38.387059 [debug] [ThreadPool]: Opening a new connection, currently in state init
04:36:40.909649 [debug] [ThreadPool]: SQL status: OK in 2.52 seconds
04:36:41.300210 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
04:36:41.300558 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
04:36:41.300796 [debug] [ThreadPool]: On list_None_test_dbt: Close
04:36:41.480525 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
04:36:41.481086 [info ] [MainThread]: 
04:36:41.493727 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
04:36:41.494415 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
04:36:41.497199 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
04:36:41.497462 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
04:36:41.502936 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
04:36:41.504430 [debug] [Thread-1  ]: finished collecting timing info
04:36:41.504696 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
04:36:41.504920 [debug] [Thread-1  ]: finished collecting timing info
04:36:41.505358 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
04:36:41.506258 [debug] [MainThread]: Connection 'master' was properly closed.
04:36:41.506494 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
04:36:41.556224 [info ] [MainThread]: Done.
04:36:41.556723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bca100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bca0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bca040>]}


============================== 2022-03-23 04:39:04.681077 | d69d144c-5497-456d-a79a-fcf327600687 ==============================
04:39:04.681077 [info ] [MainThread]: Running with dbt=1.0.3
04:39:04.682014 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
04:39:04.682371 [debug] [MainThread]: Tracking: tracking
04:39:04.700602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055af040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b9970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b9b80>]}
04:39:04.992221 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
04:39:04.992735 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
04:39:05.009387 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
04:39:05.042721 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
04:39:05.059874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd69d144c-5497-456d-a79a-fcf327600687', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058ce0d0>]}
04:39:05.076100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd69d144c-5497-456d-a79a-fcf327600687', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105796910>]}
04:39:05.076460 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 8 sources, 0 exposures, 0 metrics
04:39:05.077788 [info ] [MainThread]: 
04:39:05.078311 [debug] [MainThread]: Acquiring new databricks connection "master"
04:39:05.079219 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
04:39:05.090587 [debug] [ThreadPool]: Using databricks connection "list_schemas"
04:39:05.090955 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
04:39:05.091201 [debug] [ThreadPool]: Opening a new connection, currently in state init
04:39:05.731869 [debug] [ThreadPool]: SQL status: OK in 0.64 seconds
04:39:06.092136 [debug] [ThreadPool]: On list_schemas: Close
04:39:06.266053 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
04:39:06.276292 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
04:39:06.276566 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
04:39:06.276842 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
04:39:06.277040 [debug] [ThreadPool]: Opening a new connection, currently in state closed
04:39:07.014202 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
04:39:07.284761 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
04:39:07.285108 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
04:39:07.285313 [debug] [ThreadPool]: On list_None_test_dbt: Close
04:39:07.455158 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:39:07.455493 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:39:07.456037 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
04:39:07.456465 [info ] [MainThread]: 
04:39:07.476977 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
04:39:07.477400 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
04:39:07.477962 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
04:39:07.480549 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
04:39:07.480830 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
04:39:07.489450 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
04:39:07.490461 [debug] [Thread-1  ]: finished collecting timing info
04:39:07.490714 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
04:39:07.538286 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:39:07.538592 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
drop table if exists test_dbt.hub_customer
04:39:07.538780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
04:39:08.303459 [debug] [Thread-1  ]: SQL status: OK in 0.76 seconds
04:39:08.335653 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
04:39:08.336780 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
04:39:08.336990 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:39:08.337163 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert
04:39:08.516466 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert
04:39:08.516918 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
----^^^
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
----^^^
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

04:39:08.517510 [debug] [Thread-1  ]: finished collecting timing info
04:39:08.517780 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
04:39:08.518006 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
04:39:08.518222 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
04:39:08.703231 [debug] [Thread-1  ]: Runtime Error in model hub_customer (models/datavault/hub_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 25, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
  
        create or replace table test_dbt.hub_customer
      
      
      using delta
      
      
      
      
      location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
      
      as
        
  
  
  WITH row_rank_1 AS (
      SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
             ROW_NUMBER() OVER(
                 PARTITION BY CUSTOMER_HK
                 ORDER BY LOAD_DATE
             ) AS row_number
      FROM test_dbt.stg_customer
      WHERE CUSTOMER_HK IS NOT NULL
      QUALIFY row_number = 1
  ----^^^
  ),
  
  records_to_insert AS (
      SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
      FROM row_rank_1 AS a
      
  )
  
  SELECT * FROM records_to_insert
04:39:08.703867 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd69d144c-5497-456d-a79a-fcf327600687', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a5c130>]}
04:39:08.704363 [error] [Thread-1  ]: 1 of 1 ERROR creating incremental model test_dbt.hub_customer................... [[31mERROR[0m in 1.23s]
04:39:08.704857 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
04:39:08.706538 [debug] [MainThread]: Acquiring new databricks connection "master"
04:39:08.706869 [debug] [MainThread]: On master: ROLLBACK
04:39:08.707107 [debug] [MainThread]: Opening a new connection, currently in state init
04:39:08.821573 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:39:08.821956 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:39:08.822190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:39:08.822432 [debug] [MainThread]: On master: ROLLBACK
04:39:08.822649 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:39:08.822911 [debug] [MainThread]: On master: Close
04:39:08.913820 [info ] [MainThread]: 
04:39:08.914372 [info ] [MainThread]: Finished running 1 incremental model in 3.84s.
04:39:08.914758 [debug] [MainThread]: Connection 'master' was properly closed.
04:39:08.915025 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
04:39:08.934142 [info ] [MainThread]: 
04:39:08.934528 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
04:39:08.934885 [info ] [MainThread]: 
04:39:08.935204 [error] [MainThread]: [33mRuntime Error in model hub_customer (models/datavault/hub_customer.sql)[0m
04:39:08.935541 [error] [MainThread]:   Query execution failed.
04:39:08.935920 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
04:39:08.936253 [error] [MainThread]:   mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 25, pos 4)
04:39:08.936576 [error] [MainThread]:   
04:39:08.936872 [error] [MainThread]:   == SQL ==
04:39:08.937163 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
04:39:08.937485 [error] [MainThread]:   
04:39:08.937855 [error] [MainThread]:         create or replace table test_dbt.hub_customer
04:39:08.938156 [error] [MainThread]:       
04:39:08.938435 [error] [MainThread]:       
04:39:08.938812 [error] [MainThread]:       using delta
04:39:08.939410 [error] [MainThread]:       
04:39:08.939752 [error] [MainThread]:       
04:39:08.940125 [error] [MainThread]:       
04:39:08.940674 [error] [MainThread]:       
04:39:08.940989 [error] [MainThread]:       location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
04:39:08.941274 [error] [MainThread]:       
04:39:08.941591 [error] [MainThread]:       as
04:39:08.942013 [error] [MainThread]:         
04:39:08.942590 [error] [MainThread]:   
04:39:08.942916 [error] [MainThread]:   
04:39:08.943556 [error] [MainThread]:   WITH row_rank_1 AS (
04:39:08.944017 [error] [MainThread]:       SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
04:39:08.944321 [error] [MainThread]:              ROW_NUMBER() OVER(
04:39:08.944765 [error] [MainThread]:                  PARTITION BY CUSTOMER_HK
04:39:08.945188 [error] [MainThread]:                  ORDER BY LOAD_DATE
04:39:08.945770 [error] [MainThread]:              ) AS row_number
04:39:08.946174 [error] [MainThread]:       FROM test_dbt.stg_customer
04:39:08.946505 [error] [MainThread]:       WHERE CUSTOMER_HK IS NOT NULL
04:39:08.946813 [error] [MainThread]:       QUALIFY row_number = 1
04:39:08.947200 [error] [MainThread]:   ----^^^
04:39:08.947723 [error] [MainThread]:   ),
04:39:08.948126 [error] [MainThread]:   
04:39:08.948513 [error] [MainThread]:   records_to_insert AS (
04:39:08.949075 [error] [MainThread]:       SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
04:39:08.949554 [error] [MainThread]:       FROM row_rank_1 AS a
04:39:08.949866 [error] [MainThread]:       
04:39:08.950204 [error] [MainThread]:   )
04:39:08.950496 [error] [MainThread]:   
04:39:08.950891 [error] [MainThread]:   SELECT * FROM records_to_insert
04:39:08.951327 [info ] [MainThread]: 
04:39:08.951655 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
04:39:08.952114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10592fc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10592fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a58a30>]}


============================== 2022-03-23 04:41:52.385464 | e130ad8b-09be-4452-b703-3367c93190e4 ==============================
04:41:52.385464 [info ] [MainThread]: Running with dbt=1.0.3
04:41:52.386528 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
04:41:52.386921 [debug] [MainThread]: Tracking: tracking
04:41:52.406473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a95790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a95d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a956a0>]}
04:41:52.709986 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
04:41:52.710505 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
04:41:52.726648 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
04:41:52.748916 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
04:41:52.783750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e130ad8b-09be-4452-b703-3367c93190e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d8c0d0>]}
04:41:52.801552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e130ad8b-09be-4452-b703-3367c93190e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c6e8e0>]}
04:41:52.801958 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 8 sources, 0 exposures, 0 metrics
04:41:52.803716 [info ] [MainThread]: 
04:41:52.804428 [debug] [MainThread]: Acquiring new databricks connection "master"
04:41:52.805479 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
04:41:52.817189 [debug] [ThreadPool]: Using databricks connection "list_schemas"
04:41:52.817517 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
04:41:52.817757 [debug] [ThreadPool]: Opening a new connection, currently in state init
04:41:53.457028 [debug] [ThreadPool]: SQL status: OK in 0.64 seconds
04:41:53.784465 [debug] [ThreadPool]: On list_schemas: Close
04:41:53.978029 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
04:41:53.988020 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
04:41:53.988352 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
04:41:53.988581 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
04:41:53.988837 [debug] [ThreadPool]: Opening a new connection, currently in state closed
04:41:54.609281 [debug] [ThreadPool]: SQL status: OK in 0.62 seconds
04:41:54.879156 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
04:41:54.879483 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
04:41:54.879712 [debug] [ThreadPool]: On list_None_test_dbt: Close
04:41:55.066549 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:41:55.066986 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:41:55.067472 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
04:41:55.067973 [info ] [MainThread]: 
04:41:55.080360 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
04:41:55.080788 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
04:41:55.081374 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
04:41:55.084142 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
04:41:55.084435 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
04:41:55.091050 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
04:41:55.092116 [debug] [Thread-1  ]: finished collecting timing info
04:41:55.092353 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
04:41:55.165856 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
04:41:55.176569 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
04:41:55.176867 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:41:55.177091 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
),
row_rank_2 AS (
    SELECT 
        *
    FROM test_dbt.stg_customer
    WHERE row_number = 1
),
records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert
04:41:55.177291 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
04:41:55.653049 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATE, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATE
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
),
row_rank_2 AS (
    SELECT 
        *
    FROM test_dbt.stg_customer
    WHERE row_number = 1
),
records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATE, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert
04:41:55.653408 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: cannot resolve '`LOAD_DATE`' given input columns: [spark_catalog.test_dbt.stg_customer.CUSTOMER_HASHDIFF, spark_catalog.test_dbt.stg_customer.CUSTOMER_HK, spark_catalog.test_dbt.stg_customer.CUSTOMER_ID, spark_catalog.test_dbt.stg_customer.EMAIL, spark_catalog.test_dbt.stg_customer.FIRST_NAME, spark_catalog.test_dbt.stg_customer.LAST_NAME, spark_catalog.test_dbt.stg_customer.LOAD_DATETIME, spark_catalog.test_dbt.stg_customer.RECORD_SOURCE]; line 18 pos 37;
'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@476789d, test_dbt.hub_customer, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/hub_customer], true
+- 'Project [*]
   +- 'SubqueryAlias records_to_insert
      +- 'Project ['a.CUSTOMER_HK, 'a.CUSTOMER_ID, 'a.LOAD_DATE, 'a.RECORD_SOURCE]
         +- 'SubqueryAlias a
            +- 'SubqueryAlias row_rank_1
               +- 'Project [CUSTOMER_HK#79079, CUSTOMER_ID#79074, 'LOAD_DATE, RECORD_SOURCE#79078, row_number() windowspecdefinition(CUSTOMER_HK#79079, 'LOAD_DATE ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#79069]
                  +- Filter isnotnull(CUSTOMER_HK#79079)
                     +- SubqueryAlias spark_catalog.test_dbt.stg_customer
                        +- View (`test_dbt`.`stg_customer`, [CUSTOMER_ID#79074,FIRST_NAME#79075,LAST_NAME#79076,EMAIL#79077,RECORD_SOURCE#79078,CUSTOMER_HK#79079,CUSTOMER_HASHDIFF#79080,LOAD_DATETIME#79081])
                           +- Project [cast(CUSTOMER_ID#79086 as int) AS CUSTOMER_ID#79074, cast(FIRST_NAME#79087 as string) AS FIRST_NAME#79075, cast(LAST_NAME#79088 as string) AS LAST_NAME#79076, cast(EMAIL#79089 as string) AS EMAIL#79077, cast(RECORD_SOURCE#79071 as string) AS RECORD_SOURCE#79078, cast(CUSTOMER_HK#79072 as string) AS CUSTOMER_HK#79079, cast(CUSTOMER_HASHDIFF#79073 as string) AS CUSTOMER_HASHDIFF#79080, cast(LOAD_DATETIME#79070 as timestamp) AS LOAD_DATETIME#79081]
                              +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073, current_timestamp() AS LOAD_DATETIME#79070]
                                 +- SubqueryAlias columns_to_select
                                    +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073]
                                       +- SubqueryAlias hashed_columns
                                          +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, sha2(cast(coalesce(trim(cast(CUSTOMER_ID#79086 as string), None), ^^) as binary), 256) AS CUSTOMER_HK#79072, sha2(cast(concat_ws(||, coalesce(cast(FIRST_NAME#79087 as string), ^^), coalesce(cast(LAST_NAME#79088 as string), ^^), coalesce(cast(EMAIL#79089 as string), ^^)) as binary), 256) AS CUSTOMER_HASHDIFF#79073]
                                             +- SubqueryAlias derived_columns
                                                +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, SAP AS RECORD_SOURCE#79071]
                                                   +- SubqueryAlias source_data
                                                      +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089]
                                                         +- SubqueryAlias spark_catalog.test_dbt.raw_customer
                                                            +- View (`test_dbt`.`raw_customer`, [CUSTOMER_ID#79086,FIRST_NAME#79087,LAST_NAME#79088,EMAIL#79089])
                                                               +- Project [cast(CUSTOMER_ID#79082 as int) AS CUSTOMER_ID#79086, cast(FIRST_NAME#79083 as string) AS FIRST_NAME#79087, cast(LAST_NAME#79084 as string) AS LAST_NAME#79088, cast(EMAIL#79085 as string) AS EMAIL#79089]
                                                                  +- Project [customer_id#79094 AS CUSTOMER_ID#79082, first_name#79095 AS FIRST_NAME#79083, last_name#79096 AS LAST_NAME#79084, email#79097 AS EMAIL#79085]
                                                                     +- SubqueryAlias spark_catalog.test_dbt.customer
                                                                        +- Relation[customer_id#79094,first_name#79095,last_name#79096,email#79097] parquet

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: cannot resolve '`LOAD_DATE`' given input columns: [spark_catalog.test_dbt.stg_customer.CUSTOMER_HASHDIFF, spark_catalog.test_dbt.stg_customer.CUSTOMER_HK, spark_catalog.test_dbt.stg_customer.CUSTOMER_ID, spark_catalog.test_dbt.stg_customer.EMAIL, spark_catalog.test_dbt.stg_customer.FIRST_NAME, spark_catalog.test_dbt.stg_customer.LAST_NAME, spark_catalog.test_dbt.stg_customer.LOAD_DATETIME, spark_catalog.test_dbt.stg_customer.RECORD_SOURCE]; line 18 pos 37;
'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@476789d, test_dbt.hub_customer, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/hub_customer], true
+- 'Project [*]
   +- 'SubqueryAlias records_to_insert
      +- 'Project ['a.CUSTOMER_HK, 'a.CUSTOMER_ID, 'a.LOAD_DATE, 'a.RECORD_SOURCE]
         +- 'SubqueryAlias a
            +- 'SubqueryAlias row_rank_1
               +- 'Project [CUSTOMER_HK#79079, CUSTOMER_ID#79074, 'LOAD_DATE, RECORD_SOURCE#79078, row_number() windowspecdefinition(CUSTOMER_HK#79079, 'LOAD_DATE ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#79069]
                  +- Filter isnotnull(CUSTOMER_HK#79079)
                     +- SubqueryAlias spark_catalog.test_dbt.stg_customer
                        +- View (`test_dbt`.`stg_customer`, [CUSTOMER_ID#79074,FIRST_NAME#79075,LAST_NAME#79076,EMAIL#79077,RECORD_SOURCE#79078,CUSTOMER_HK#79079,CUSTOMER_HASHDIFF#79080,LOAD_DATETIME#79081])
                           +- Project [cast(CUSTOMER_ID#79086 as int) AS CUSTOMER_ID#79074, cast(FIRST_NAME#79087 as string) AS FIRST_NAME#79075, cast(LAST_NAME#79088 as string) AS LAST_NAME#79076, cast(EMAIL#79089 as string) AS EMAIL#79077, cast(RECORD_SOURCE#79071 as string) AS RECORD_SOURCE#79078, cast(CUSTOMER_HK#79072 as string) AS CUSTOMER_HK#79079, cast(CUSTOMER_HASHDIFF#79073 as string) AS CUSTOMER_HASHDIFF#79080, cast(LOAD_DATETIME#79070 as timestamp) AS LOAD_DATETIME#79081]
                              +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073, current_timestamp() AS LOAD_DATETIME#79070]
                                 +- SubqueryAlias columns_to_select
                                    +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073]
                                       +- SubqueryAlias hashed_columns
                                          +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, sha2(cast(coalesce(trim(cast(CUSTOMER_ID#79086 as string), None), ^^) as binary), 256) AS CUSTOMER_HK#79072, sha2(cast(concat_ws(||, coalesce(cast(FIRST_NAME#79087 as string), ^^), coalesce(cast(LAST_NAME#79088 as string), ^^), coalesce(cast(EMAIL#79089 as string), ^^)) as binary), 256) AS CUSTOMER_HASHDIFF#79073]
                                             +- SubqueryAlias derived_columns
                                                +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, SAP AS RECORD_SOURCE#79071]
                                                   +- SubqueryAlias source_data
                                                      +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089]
                                                         +- SubqueryAlias spark_catalog.test_dbt.raw_customer
                                                            +- View (`test_dbt`.`raw_customer`, [CUSTOMER_ID#79086,FIRST_NAME#79087,LAST_NAME#79088,EMAIL#79089])
                                                               +- Project [cast(CUSTOMER_ID#79082 as int) AS CUSTOMER_ID#79086, cast(FIRST_NAME#79083 as string) AS FIRST_NAME#79087, cast(LAST_NAME#79084 as string) AS LAST_NAME#79088, cast(EMAIL#79085 as string) AS EMAIL#79089]
                                                                  +- Project [customer_id#79094 AS CUSTOMER_ID#79082, first_name#79095 AS FIRST_NAME#79083, last_name#79096 AS LAST_NAME#79084, email#79097 AS EMAIL#79085]
                                                                     +- SubqueryAlias spark_catalog.test_dbt.customer
                                                                        +- Relation[customer_id#79094,first_name#79095,last_name#79096,email#79097] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$2$2.applyOrElse(CheckAnalysis.scala:178)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$2$2.applyOrElse(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:538)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:538)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:221)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:221)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:322)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:173)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$2$adapted(CheckAnalysis.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:261)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:261)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:99)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:191)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:347)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:180)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:180)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:841)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

04:41:55.654192 [debug] [Thread-1  ]: finished collecting timing info
04:41:55.654689 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
04:41:55.654935 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
04:41:55.655135 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
04:41:55.820664 [debug] [Thread-1  ]: Runtime Error in model hub_customer (models/datavault/hub_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.AnalysisException: cannot resolve '`LOAD_DATE`' given input columns: [spark_catalog.test_dbt.stg_customer.CUSTOMER_HASHDIFF, spark_catalog.test_dbt.stg_customer.CUSTOMER_HK, spark_catalog.test_dbt.stg_customer.CUSTOMER_ID, spark_catalog.test_dbt.stg_customer.EMAIL, spark_catalog.test_dbt.stg_customer.FIRST_NAME, spark_catalog.test_dbt.stg_customer.LAST_NAME, spark_catalog.test_dbt.stg_customer.LOAD_DATETIME, spark_catalog.test_dbt.stg_customer.RECORD_SOURCE]; line 18 pos 37;
  'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@476789d, test_dbt.hub_customer, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/hub_customer], true
  +- 'Project [*]
     +- 'SubqueryAlias records_to_insert
        +- 'Project ['a.CUSTOMER_HK, 'a.CUSTOMER_ID, 'a.LOAD_DATE, 'a.RECORD_SOURCE]
           +- 'SubqueryAlias a
              +- 'SubqueryAlias row_rank_1
                 +- 'Project [CUSTOMER_HK#79079, CUSTOMER_ID#79074, 'LOAD_DATE, RECORD_SOURCE#79078, row_number() windowspecdefinition(CUSTOMER_HK#79079, 'LOAD_DATE ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#79069]
                    +- Filter isnotnull(CUSTOMER_HK#79079)
                       +- SubqueryAlias spark_catalog.test_dbt.stg_customer
                          +- View (`test_dbt`.`stg_customer`, [CUSTOMER_ID#79074,FIRST_NAME#79075,LAST_NAME#79076,EMAIL#79077,RECORD_SOURCE#79078,CUSTOMER_HK#79079,CUSTOMER_HASHDIFF#79080,LOAD_DATETIME#79081])
                             +- Project [cast(CUSTOMER_ID#79086 as int) AS CUSTOMER_ID#79074, cast(FIRST_NAME#79087 as string) AS FIRST_NAME#79075, cast(LAST_NAME#79088 as string) AS LAST_NAME#79076, cast(EMAIL#79089 as string) AS EMAIL#79077, cast(RECORD_SOURCE#79071 as string) AS RECORD_SOURCE#79078, cast(CUSTOMER_HK#79072 as string) AS CUSTOMER_HK#79079, cast(CUSTOMER_HASHDIFF#79073 as string) AS CUSTOMER_HASHDIFF#79080, cast(LOAD_DATETIME#79070 as timestamp) AS LOAD_DATETIME#79081]
                                +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073, current_timestamp() AS LOAD_DATETIME#79070]
                                   +- SubqueryAlias columns_to_select
                                      +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073]
                                         +- SubqueryAlias hashed_columns
                                            +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, sha2(cast(coalesce(trim(cast(CUSTOMER_ID#79086 as string), None), ^^) as binary), 256) AS CUSTOMER_HK#79072, sha2(cast(concat_ws(||, coalesce(cast(FIRST_NAME#79087 as string), ^^), coalesce(cast(LAST_NAME#79088 as string), ^^), coalesce(cast(EMAIL#79089 as string), ^^)) as binary), 256) AS CUSTOMER_HASHDIFF#79073]
                                               +- SubqueryAlias derived_columns
                                                  +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, SAP AS RECORD_SOURCE#79071]
                                                     +- SubqueryAlias source_data
                                                        +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089]
                                                           +- SubqueryAlias spark_catalog.test_dbt.raw_customer
                                                              +- View (`test_dbt`.`raw_customer`, [CUSTOMER_ID#79086,FIRST_NAME#79087,LAST_NAME#79088,EMAIL#79089])
                                                                 +- Project [cast(CUSTOMER_ID#79082 as int) AS CUSTOMER_ID#79086, cast(FIRST_NAME#79083 as string) AS FIRST_NAME#79087, cast(LAST_NAME#79084 as string) AS LAST_NAME#79088, cast(EMAIL#79085 as string) AS EMAIL#79089]
                                                                    +- Project [customer_id#79094 AS CUSTOMER_ID#79082, first_name#79095 AS FIRST_NAME#79083, last_name#79096 AS LAST_NAME#79084, email#79097 AS EMAIL#79085]
                                                                       +- SubqueryAlias spark_catalog.test_dbt.customer
                                                                          +- Relation[customer_id#79094,first_name#79095,last_name#79096,email#79097] parquet
04:41:55.821333 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e130ad8b-09be-4452-b703-3367c93190e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a9e910>]}
04:41:55.821877 [error] [Thread-1  ]: 1 of 1 ERROR creating incremental model test_dbt.hub_customer................... [[31mERROR[0m in 0.74s]
04:41:55.822384 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
04:41:55.823925 [debug] [MainThread]: Acquiring new databricks connection "master"
04:41:55.824236 [debug] [MainThread]: On master: ROLLBACK
04:41:55.824461 [debug] [MainThread]: Opening a new connection, currently in state init
04:41:55.924001 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:41:55.924304 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:41:55.924497 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:41:55.924684 [debug] [MainThread]: On master: ROLLBACK
04:41:55.924853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:41:55.925017 [debug] [MainThread]: On master: Close
04:41:56.009930 [info ] [MainThread]: 
04:41:56.010414 [info ] [MainThread]: Finished running 1 incremental model in 3.21s.
04:41:56.010804 [debug] [MainThread]: Connection 'master' was properly closed.
04:41:56.011035 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
04:41:56.029337 [info ] [MainThread]: 
04:41:56.029902 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
04:41:56.030305 [info ] [MainThread]: 
04:41:56.030644 [error] [MainThread]: [33mRuntime Error in model hub_customer (models/datavault/hub_customer.sql)[0m
04:41:56.030973 [error] [MainThread]:   Query execution failed.
04:41:56.031263 [error] [MainThread]:   Error message: org.apache.spark.sql.AnalysisException: cannot resolve '`LOAD_DATE`' given input columns: [spark_catalog.test_dbt.stg_customer.CUSTOMER_HASHDIFF, spark_catalog.test_dbt.stg_customer.CUSTOMER_HK, spark_catalog.test_dbt.stg_customer.CUSTOMER_ID, spark_catalog.test_dbt.stg_customer.EMAIL, spark_catalog.test_dbt.stg_customer.FIRST_NAME, spark_catalog.test_dbt.stg_customer.LAST_NAME, spark_catalog.test_dbt.stg_customer.LOAD_DATETIME, spark_catalog.test_dbt.stg_customer.RECORD_SOURCE]; line 18 pos 37;
04:41:56.031624 [error] [MainThread]:   'ReplaceTableAsSelect com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog@476789d, test_dbt.hub_customer, [provider=delta, location=/mnt/adls_ss_finance/SS Finance/test/hub_customer], true
04:41:56.031923 [error] [MainThread]:   +- 'Project [*]
04:41:56.032213 [error] [MainThread]:      +- 'SubqueryAlias records_to_insert
04:41:56.032510 [error] [MainThread]:         +- 'Project ['a.CUSTOMER_HK, 'a.CUSTOMER_ID, 'a.LOAD_DATE, 'a.RECORD_SOURCE]
04:41:56.032839 [error] [MainThread]:            +- 'SubqueryAlias a
04:41:56.033168 [error] [MainThread]:               +- 'SubqueryAlias row_rank_1
04:41:56.033479 [error] [MainThread]:                  +- 'Project [CUSTOMER_HK#79079, CUSTOMER_ID#79074, 'LOAD_DATE, RECORD_SOURCE#79078, row_number() windowspecdefinition(CUSTOMER_HK#79079, 'LOAD_DATE ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#79069]
04:41:56.033982 [error] [MainThread]:                     +- Filter isnotnull(CUSTOMER_HK#79079)
04:41:56.034372 [error] [MainThread]:                        +- SubqueryAlias spark_catalog.test_dbt.stg_customer
04:41:56.034827 [error] [MainThread]:                           +- View (`test_dbt`.`stg_customer`, [CUSTOMER_ID#79074,FIRST_NAME#79075,LAST_NAME#79076,EMAIL#79077,RECORD_SOURCE#79078,CUSTOMER_HK#79079,CUSTOMER_HASHDIFF#79080,LOAD_DATETIME#79081])
04:41:56.035362 [error] [MainThread]:                              +- Project [cast(CUSTOMER_ID#79086 as int) AS CUSTOMER_ID#79074, cast(FIRST_NAME#79087 as string) AS FIRST_NAME#79075, cast(LAST_NAME#79088 as string) AS LAST_NAME#79076, cast(EMAIL#79089 as string) AS EMAIL#79077, cast(RECORD_SOURCE#79071 as string) AS RECORD_SOURCE#79078, cast(CUSTOMER_HK#79072 as string) AS CUSTOMER_HK#79079, cast(CUSTOMER_HASHDIFF#79073 as string) AS CUSTOMER_HASHDIFF#79080, cast(LOAD_DATETIME#79070 as timestamp) AS LOAD_DATETIME#79081]
04:41:56.035746 [error] [MainThread]:                                 +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073, current_timestamp() AS LOAD_DATETIME#79070]
04:41:56.036132 [error] [MainThread]:                                    +- SubqueryAlias columns_to_select
04:41:56.036468 [error] [MainThread]:                                       +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, CUSTOMER_HK#79072, CUSTOMER_HASHDIFF#79073]
04:41:56.036813 [error] [MainThread]:                                          +- SubqueryAlias hashed_columns
04:41:56.037104 [error] [MainThread]:                                             +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, RECORD_SOURCE#79071, sha2(cast(coalesce(trim(cast(CUSTOMER_ID#79086 as string), None), ^^) as binary), 256) AS CUSTOMER_HK#79072, sha2(cast(concat_ws(||, coalesce(cast(FIRST_NAME#79087 as string), ^^), coalesce(cast(LAST_NAME#79088 as string), ^^), coalesce(cast(EMAIL#79089 as string), ^^)) as binary), 256) AS CUSTOMER_HASHDIFF#79073]
04:41:56.037418 [error] [MainThread]:                                                +- SubqueryAlias derived_columns
04:41:56.037692 [error] [MainThread]:                                                   +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089, SAP AS RECORD_SOURCE#79071]
04:41:56.038094 [error] [MainThread]:                                                      +- SubqueryAlias source_data
04:41:56.038622 [error] [MainThread]:                                                         +- Project [CUSTOMER_ID#79086, FIRST_NAME#79087, LAST_NAME#79088, EMAIL#79089]
04:41:56.038966 [error] [MainThread]:                                                            +- SubqueryAlias spark_catalog.test_dbt.raw_customer
04:41:56.039322 [error] [MainThread]:                                                               +- View (`test_dbt`.`raw_customer`, [CUSTOMER_ID#79086,FIRST_NAME#79087,LAST_NAME#79088,EMAIL#79089])
04:41:56.039749 [error] [MainThread]:                                                                  +- Project [cast(CUSTOMER_ID#79082 as int) AS CUSTOMER_ID#79086, cast(FIRST_NAME#79083 as string) AS FIRST_NAME#79087, cast(LAST_NAME#79084 as string) AS LAST_NAME#79088, cast(EMAIL#79085 as string) AS EMAIL#79089]
04:41:56.040262 [error] [MainThread]:                                                                     +- Project [customer_id#79094 AS CUSTOMER_ID#79082, first_name#79095 AS FIRST_NAME#79083, last_name#79096 AS LAST_NAME#79084, email#79097 AS EMAIL#79085]
04:41:56.040678 [error] [MainThread]:                                                                        +- SubqueryAlias spark_catalog.test_dbt.customer
04:41:56.041131 [error] [MainThread]:                                                                           +- Relation[customer_id#79094,first_name#79095,last_name#79096,email#79097] parquet
04:41:56.041442 [info ] [MainThread]: 
04:41:56.041748 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
04:41:56.042151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e113a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e0d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f26160>]}


============================== 2022-03-23 04:42:33.985857 | 07aebf0e-894e-47a5-9808-d442294e857f ==============================
04:42:33.985857 [info ] [MainThread]: Running with dbt=1.0.3
04:42:33.986729 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
04:42:33.987074 [debug] [MainThread]: Tracking: tracking
04:42:34.007328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c465400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c466e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c466880>]}
04:42:34.295963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
04:42:34.296629 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
04:42:34.314487 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
04:42:34.349263 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
04:42:34.384308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07aebf0e-894e-47a5-9808-d442294e857f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c75d0d0>]}
04:42:34.401600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07aebf0e-894e-47a5-9808-d442294e857f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c640880>]}
04:42:34.402051 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 8 sources, 0 exposures, 0 metrics
04:42:34.403529 [info ] [MainThread]: 
04:42:34.404208 [debug] [MainThread]: Acquiring new databricks connection "master"
04:42:34.405470 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
04:42:34.416981 [debug] [ThreadPool]: Using databricks connection "list_schemas"
04:42:34.417306 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
04:42:34.417512 [debug] [ThreadPool]: Opening a new connection, currently in state init
04:42:34.757280 [debug] [ThreadPool]: SQL status: OK in 0.34 seconds
04:42:35.030309 [debug] [ThreadPool]: On list_schemas: Close
04:42:35.194558 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
04:42:35.204695 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
04:42:35.204988 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
04:42:35.205193 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
04:42:35.205387 [debug] [ThreadPool]: Opening a new connection, currently in state closed
04:42:35.811794 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
04:42:36.089645 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
04:42:36.089936 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
04:42:36.090134 [debug] [ThreadPool]: On list_None_test_dbt: Close
04:42:36.277414 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:42:36.277783 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:42:36.278271 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
04:42:36.278698 [info ] [MainThread]: 
04:42:36.283429 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
04:42:36.283900 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
04:42:36.284533 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
04:42:36.287023 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
04:42:36.287284 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
04:42:36.293494 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
04:42:36.294594 [debug] [Thread-1  ]: finished collecting timing info
04:42:36.294838 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
04:42:36.370095 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
04:42:36.377270 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
04:42:36.377541 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:42:36.377727 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
),
row_rank_2 AS (
    SELECT 
        *
    FROM test_dbt.stg_customer
    WHERE row_number = 1
),
records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    
)

SELECT * FROM records_to_insert
04:42:36.377905 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
04:42:41.510647 [debug] [Thread-1  ]: SQL status: OK in 5.13 seconds
04:42:41.518409 [debug] [Thread-1  ]: finished collecting timing info
04:42:41.518752 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
04:42:41.518967 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
04:42:41.519159 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
04:42:41.721480 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07aebf0e-894e-47a5-9808-d442294e857f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7cba60>]}
04:42:41.722146 [info ] [Thread-1  ]: 1 of 1 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 5.44s]
04:42:41.722644 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
04:42:41.724048 [debug] [MainThread]: Acquiring new databricks connection "master"
04:42:41.724319 [debug] [MainThread]: On master: ROLLBACK
04:42:41.724512 [debug] [MainThread]: Opening a new connection, currently in state init
04:42:41.827652 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:42:41.828094 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:42:41.828420 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:42:41.828722 [debug] [MainThread]: On master: ROLLBACK
04:42:41.828998 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:42:41.829267 [debug] [MainThread]: On master: Close
04:42:41.919682 [info ] [MainThread]: 
04:42:41.920136 [info ] [MainThread]: Finished running 1 incremental model in 7.52s.
04:42:41.920478 [debug] [MainThread]: Connection 'master' was properly closed.
04:42:41.920687 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
04:42:41.941923 [info ] [MainThread]: 
04:42:41.942318 [info ] [MainThread]: [32mCompleted successfully[0m
04:42:41.942678 [info ] [MainThread]: 
04:42:41.943019 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
04:42:41.943461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7e8cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7e03a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7f0c70>]}


============================== 2022-03-23 04:48:31.841904 | 80fb4d9a-670f-4a05-9b9c-329455d36f24 ==============================
04:48:31.841904 [info ] [MainThread]: Running with dbt=1.0.3
04:48:31.842656 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
04:48:31.842991 [debug] [MainThread]: Tracking: tracking
04:48:31.864189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d1880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d12b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d1970>]}
04:48:32.162936 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
04:48:32.163495 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
04:48:32.179731 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
04:48:32.202014 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
04:48:32.223315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80fb4d9a-670f-4a05-9b9c-329455d36f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ac20d0>]}
04:48:32.239326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80fb4d9a-670f-4a05-9b9c-329455d36f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089ab8e0>]}
04:48:32.239806 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 631 macros, 0 operations, 0 seed files, 8 sources, 0 exposures, 0 metrics
04:48:32.241714 [info ] [MainThread]: 
04:48:32.242350 [debug] [MainThread]: Acquiring new databricks connection "master"
04:48:32.243326 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
04:48:32.254860 [debug] [ThreadPool]: Using databricks connection "list_schemas"
04:48:32.255165 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
04:48:32.255365 [debug] [ThreadPool]: Opening a new connection, currently in state init
04:48:32.910978 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
04:48:33.205806 [debug] [ThreadPool]: On list_schemas: Close
04:48:33.370558 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
04:48:33.381007 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
04:48:33.381367 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
04:48:33.381691 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
04:48:33.382258 [debug] [ThreadPool]: Opening a new connection, currently in state closed
04:48:34.371505 [debug] [ThreadPool]: SQL status: OK in 0.99 seconds
04:48:34.655049 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
04:48:34.655317 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
04:48:34.655509 [debug] [ThreadPool]: On list_None_test_dbt: Close
04:48:34.821677 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:48:34.822035 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:48:34.822442 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
04:48:34.822775 [info ] [MainThread]: 
04:48:34.837219 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
04:48:34.837635 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
04:48:34.838234 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
04:48:34.841258 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
04:48:34.841688 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
04:48:34.850661 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
04:48:34.851686 [debug] [Thread-1  ]: finished collecting timing info
04:48:34.851921 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
04:48:34.906438 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:48:34.907594 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
drop table if exists test_dbt.hub_customer
04:48:34.907856 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
04:48:35.710381 [debug] [Thread-1  ]: SQL status: OK in 0.8 seconds
04:48:35.744754 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
04:48:35.745694 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
04:48:35.745902 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
04:48:35.746084 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      


WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY CUSTOMER_HK
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
),
row_rank_2 AS (
    SELECT 
        *
    FROM row_rank_1
    WHERE row_number = 1
),
records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_2 AS a
    
)

SELECT * FROM records_to_insert
04:48:40.220935 [debug] [Thread-1  ]: SQL status: OK in 4.47 seconds
04:48:40.227381 [debug] [Thread-1  ]: finished collecting timing info
04:48:40.227744 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
04:48:40.227974 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
04:48:40.228174 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
04:48:40.407594 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80fb4d9a-670f-4a05-9b9c-329455d36f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ac6040>]}
04:48:40.408091 [info ] [Thread-1  ]: 1 of 1 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 5.57s]
04:48:40.408491 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
04:48:40.409582 [debug] [MainThread]: Acquiring new databricks connection "master"
04:48:40.409825 [debug] [MainThread]: On master: ROLLBACK
04:48:40.410011 [debug] [MainThread]: Opening a new connection, currently in state init
04:48:40.514069 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:48:40.514353 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
04:48:40.514546 [debug] [MainThread]: Spark adapter: NotImplemented: commit
04:48:40.514745 [debug] [MainThread]: On master: ROLLBACK
04:48:40.514923 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
04:48:40.515098 [debug] [MainThread]: On master: Close
04:48:40.632841 [info ] [MainThread]: 
04:48:40.633343 [info ] [MainThread]: Finished running 1 incremental model in 8.39s.
04:48:40.633791 [debug] [MainThread]: Connection 'master' was properly closed.
04:48:40.634100 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
04:48:40.654711 [info ] [MainThread]: 
04:48:40.655109 [info ] [MainThread]: [32mCompleted successfully[0m
04:48:40.655585 [info ] [MainThread]: 
04:48:40.656066 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
04:48:40.656558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a8f9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c5f280>]}


============================== 2022-03-23 05:55:26.095976 | 48acbdce-0aca-47c1-9109-fdc2a48b9377 ==============================
05:55:26.095976 [info ] [MainThread]: Running with dbt=1.0.3
05:55:26.096668 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=True, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
05:55:26.096942 [debug] [MainThread]: Tracking: tracking
05:55:26.135522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cdbd370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cdc5460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cdc50d0>]}
05:55:27.160779 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 5 files changed.
05:55:27.161237 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/tables/databricks/sat.sql
05:55:27.161650 [debug] [MainThread]: Partial parsing: deleted source source.test_dbx.stg_layer.stg_customer
05:55:27.161826 [debug] [MainThread]: Partial parsing: deleted source source.test_dbx.stg_layer.stg_product
05:55:27.161984 [debug] [MainThread]: Partial parsing: deleted source source.test_dbx.stg_layer.stg_order
05:55:27.162134 [debug] [MainThread]: Partial parsing: deleted source source.test_dbx.stg_layer.stg_order_product
05:55:27.162301 [debug] [MainThread]: Partial parsing: update schema file: test_dbx://models/schema.yaml
05:55:27.162532 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
05:55:27.162899 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/tables/databricks/link.sql
05:55:27.163256 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/tables/databricks/hub.sql
05:55:27.163547 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/stage/stg_customer.sql
05:55:27.163768 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
05:55:27.176018 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
05:55:27.191877 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
05:55:27.203797 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
05:55:27.216081 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
05:55:27.273911 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
05:55:27.361669 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
05:55:27.362776 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
05:55:27.452552 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
05:55:27.473465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48acbdce-0aca-47c1-9109-fdc2a48b9377', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1c90d0>]}
05:55:27.546030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48acbdce-0aca-47c1-9109-fdc2a48b9377', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cdbde80>]}
05:55:27.546545 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
05:55:27.548253 [info ] [MainThread]: 
05:55:27.548809 [debug] [MainThread]: Acquiring new databricks connection "master"
05:55:27.549803 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
05:55:27.562224 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
05:55:27.562532 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
05:55:27.562731 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
05:55:27.562920 [debug] [ThreadPool]: Opening a new connection, currently in state init
05:55:30.759404 [debug] [ThreadPool]: SQL status: OK in 3.2 seconds
05:55:31.195258 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
05:55:31.195594 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
05:55:31.195830 [debug] [ThreadPool]: On list_None_test_dbt: Close
05:55:31.367827 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
05:55:31.368455 [info ] [MainThread]: 
05:55:31.384223 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
05:55:31.384785 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
05:55:31.387675 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
05:55:31.387950 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
05:55:31.405524 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
05:55:31.409853 [debug] [Thread-1  ]: finished collecting timing info
05:55:31.410271 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
05:55:31.410617 [debug] [Thread-1  ]: finished collecting timing info
05:55:31.411264 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
05:55:31.412420 [debug] [MainThread]: Connection 'master' was properly closed.
05:55:31.412668 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
05:55:31.433150 [info ] [MainThread]: Done.
05:55:31.433667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfa3100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfa3880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cfa3430>]}


============================== 2022-03-23 05:56:48.279214 | 24af9164-663a-4901-b0e3-f391063e4e50 ==============================
05:56:48.279214 [info ] [MainThread]: Running with dbt=1.0.3
05:56:48.279755 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=True, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
05:56:48.280150 [debug] [MainThread]: Tracking: tracking
05:56:48.297085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b593790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b593d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5936a0>]}
05:56:48.495588 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
05:56:48.495892 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
05:56:48.504378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24af9164-663a-4901-b0e3-f391063e4e50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b88f0d0>]}
05:56:48.519395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24af9164-663a-4901-b0e3-f391063e4e50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b75c910>]}
05:56:48.519785 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
05:56:48.521116 [info ] [MainThread]: 
05:56:48.521650 [debug] [MainThread]: Acquiring new databricks connection "master"
05:56:48.522552 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
05:56:48.533774 [debug] [ThreadPool]: Using databricks connection "list_schemas"
05:56:48.534048 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
05:56:48.534241 [debug] [ThreadPool]: Opening a new connection, currently in state init
05:56:48.929469 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
05:56:49.210854 [debug] [ThreadPool]: On list_schemas: Close
05:56:49.395799 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
05:56:49.405979 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
05:56:49.406245 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
05:56:49.406442 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
05:56:49.406628 [debug] [ThreadPool]: Opening a new connection, currently in state closed
05:56:50.003780 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
05:56:50.285269 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
05:56:50.285571 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
05:56:50.285767 [debug] [ThreadPool]: On list_None_test_dbt: Close
05:56:50.460708 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
05:56:50.461127 [debug] [MainThread]: Spark adapter: NotImplemented: commit
05:56:50.461688 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
05:56:50.462213 [info ] [MainThread]: 
05:56:50.469682 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
05:56:50.470106 [info ] [Thread-1  ]: 1 of 1 START incremental model test_dbt.hub_customer............................ [RUN]
05:56:50.470686 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
05:56:50.473273 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
05:56:50.473523 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
05:56:50.572283 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
05:56:50.573215 [debug] [Thread-1  ]: finished collecting timing info
05:56:50.573449 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
05:56:50.634008 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
05:56:50.637815 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
05:56:50.638059 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
05:56:50.638228 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY (CUSTOMER_HK)
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert
05:56:50.638395 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
05:56:50.905417 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY (CUSTOMER_HK)
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert
05:56:50.905862 [debug] [Thread-1  ]: Databricks adapter: Query execution failed. State: ERROR_STATE; Error code: 0; SQLSTATE: None; Error message: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 26, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY (CUSTOMER_HK)
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
----^^^
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert

	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:1012)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:752)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:112)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:730)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:715)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:764)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 26, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
           ROW_NUMBER() OVER(
               PARTITION BY (CUSTOMER_HK)
               ORDER BY LOAD_DATETIME
           ) AS row_number
    FROM test_dbt.stg_customer
    WHERE CUSTOMER_HK IS NOT NULL
    QUALIFY row_number = 1
----^^^
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:265)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:134)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:64)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:85)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:67)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:838)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:838)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:835)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:889)
	... 16 more

05:56:50.906420 [debug] [Thread-1  ]: finished collecting timing info
05:56:50.906684 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
05:56:50.906906 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
05:56:50.907118 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
05:56:51.069946 [debug] [Thread-1  ]: Runtime Error in model hub_customer (models/datavault/hub_customer.sql)
  Query execution failed.
  Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 26, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
  
        create or replace table test_dbt.hub_customer
      
      
      using delta
      
      
      
      
      location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
      
      as
        
  
  -- Generated by dbtvault.
  
  WITH row_rank_1 AS (
      SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
             ROW_NUMBER() OVER(
                 PARTITION BY (CUSTOMER_HK)
                 ORDER BY LOAD_DATETIME
             ) AS row_number
      FROM test_dbt.stg_customer
      WHERE CUSTOMER_HK IS NOT NULL
      QUALIFY row_number = 1
  ----^^^
  ),
  
  records_to_insert AS (
      SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
      FROM row_rank_1 AS a
  )
  
  SELECT * FROM records_to_insert
05:56:51.070485 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24af9164-663a-4901-b0e3-f391063e4e50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9806d0>]}
05:56:51.070958 [error] [Thread-1  ]: 1 of 1 ERROR creating incremental model test_dbt.hub_customer................... [[31mERROR[0m in 0.60s]
05:56:51.071440 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
05:56:51.072910 [debug] [MainThread]: Acquiring new databricks connection "master"
05:56:51.073169 [debug] [MainThread]: On master: ROLLBACK
05:56:51.073360 [debug] [MainThread]: Opening a new connection, currently in state init
05:56:51.179961 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
05:56:51.180285 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
05:56:51.180482 [debug] [MainThread]: Spark adapter: NotImplemented: commit
05:56:51.180704 [debug] [MainThread]: On master: ROLLBACK
05:56:51.180980 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
05:56:51.181284 [debug] [MainThread]: On master: Close
05:56:51.294211 [info ] [MainThread]: 
05:56:51.295017 [info ] [MainThread]: Finished running 1 incremental model in 2.77s.
05:56:51.295695 [debug] [MainThread]: Connection 'master' was properly closed.
05:56:51.296251 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
05:56:51.317986 [info ] [MainThread]: 
05:56:51.318378 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
05:56:51.318722 [info ] [MainThread]: 
05:56:51.319022 [error] [MainThread]: [33mRuntime Error in model hub_customer (models/datavault/hub_customer.sql)[0m
05:56:51.319320 [error] [MainThread]:   Query execution failed.
05:56:51.319626 [error] [MainThread]:   Error message: org.apache.spark.sql.catalyst.parser.ParseException: 
05:56:51.319908 [error] [MainThread]:   mismatched input 'QUALIFY' expecting {')', 'AND', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'GROUP', 'HAVING', 'INTERSECT', 'LIMIT', 'OR', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WINDOW'}(line 26, pos 4)
05:56:51.320290 [error] [MainThread]:   
05:56:51.320581 [error] [MainThread]:   == SQL ==
05:56:51.320870 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */
05:56:51.321139 [error] [MainThread]:   
05:56:51.321408 [error] [MainThread]:         create or replace table test_dbt.hub_customer
05:56:51.321700 [error] [MainThread]:       
05:56:51.321997 [error] [MainThread]:       
05:56:51.322266 [error] [MainThread]:       using delta
05:56:51.322526 [error] [MainThread]:       
05:56:51.322789 [error] [MainThread]:       
05:56:51.323257 [error] [MainThread]:       
05:56:51.323637 [error] [MainThread]:       
05:56:51.323954 [error] [MainThread]:       location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
05:56:51.324302 [error] [MainThread]:       
05:56:51.324676 [error] [MainThread]:       as
05:56:51.325063 [error] [MainThread]:         
05:56:51.325449 [error] [MainThread]:   
05:56:51.325775 [error] [MainThread]:   -- Generated by dbtvault.
05:56:51.326134 [error] [MainThread]:   
05:56:51.326486 [error] [MainThread]:   WITH row_rank_1 AS (
05:56:51.326785 [error] [MainThread]:       SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE,
05:56:51.327161 [error] [MainThread]:              ROW_NUMBER() OVER(
05:56:51.327447 [error] [MainThread]:                  PARTITION BY (CUSTOMER_HK)
05:56:51.327855 [error] [MainThread]:                  ORDER BY LOAD_DATETIME
05:56:51.328166 [error] [MainThread]:              ) AS row_number
05:56:51.328591 [error] [MainThread]:       FROM test_dbt.stg_customer
05:56:51.329123 [error] [MainThread]:       WHERE CUSTOMER_HK IS NOT NULL
05:56:51.329569 [error] [MainThread]:       QUALIFY row_number = 1
05:56:51.330001 [error] [MainThread]:   ----^^^
05:56:51.330464 [error] [MainThread]:   ),
05:56:51.330816 [error] [MainThread]:   
05:56:51.331170 [error] [MainThread]:   records_to_insert AS (
05:56:51.331715 [error] [MainThread]:       SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
05:56:51.332287 [error] [MainThread]:       FROM row_rank_1 AS a
05:56:51.332781 [error] [MainThread]:   )
05:56:51.333295 [error] [MainThread]:   
05:56:51.333623 [error] [MainThread]:   SELECT * FROM records_to_insert
05:56:51.333920 [info ] [MainThread]: 
05:56:51.334213 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
05:56:51.334605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9ded60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9ec8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b97d940>]}


============================== 2022-03-23 07:31:51.273306 | db1ec46f-e688-49ce-a40e-080a1137b282 ==============================
07:31:51.273306 [info ] [MainThread]: Running with dbt=1.0.3
07:31:51.274495 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=True, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
07:31:51.274973 [debug] [MainThread]: Tracking: tracking
07:31:51.431878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070eb220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070dd370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070dd340>]}
07:31:53.900552 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
07:31:53.901194 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/tables/databricks/hub.sql
07:31:53.901428 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
07:31:53.935338 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
07:31:53.951488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107392610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b2a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b2730>]}


============================== 2022-03-23 08:53:58.186078 | e83bf76d-bd43-442e-a15e-e697cd0ca62a ==============================
08:53:58.186078 [info ] [MainThread]: Running with dbt=1.0.3
08:53:58.186785 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['hub_customer'], exclude=None, selector_name=None, state=None, full_refresh=True, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
08:53:58.187056 [debug] [MainThread]: Tracking: tracking
08:53:58.228042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b545580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b557580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b557cd0>]}
08:53:59.847252 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
08:53:59.847909 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/tables/databricks/hub.sql
08:53:59.848148 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
08:53:59.869855 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
08:53:59.918196 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
08:54:00.025218 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
08:54:00.043608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e83bf76d-bd43-442e-a15e-e697cd0ca62a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8fa0d0>]}
08:54:00.138257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e83bf76d-bd43-442e-a15e-e697cd0ca62a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b71aa60>]}
08:54:00.138680 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
08:54:00.140108 [info ] [MainThread]: 
08:54:00.140646 [debug] [MainThread]: Acquiring new databricks connection "master"
08:54:00.141587 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
08:54:00.152130 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
08:54:00.152406 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
08:54:00.152572 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
08:54:00.152732 [debug] [ThreadPool]: Opening a new connection, currently in state init
08:54:03.855686 [debug] [ThreadPool]: SQL status: OK in 3.7 seconds
08:54:04.199567 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
08:54:04.199875 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
08:54:04.200095 [debug] [ThreadPool]: On list_None_test_dbt: Close
08:54:04.408860 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
08:54:04.409439 [info ] [MainThread]: 
08:54:04.424133 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
08:54:04.424643 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
08:54:04.427312 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
08:54:04.427602 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
08:54:04.441185 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
08:54:04.461339 [debug] [Thread-1  ]: finished collecting timing info
08:54:04.461616 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
08:54:04.461892 [debug] [Thread-1  ]: finished collecting timing info
08:54:04.462362 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
08:54:04.463278 [debug] [MainThread]: Connection 'master' was properly closed.
08:54:04.463472 [debug] [MainThread]: Connection 'model.test_dbx.hub_customer' was properly closed.
08:54:04.489008 [info ] [MainThread]: Done.
08:54:04.489471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b96c6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b96cf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b96ce80>]}


============================== 2022-03-23 09:01:41.980468 | 012252c5-42ed-4e10-9a83-cc219b16cb8c ==============================
09:01:41.980468 [info ] [MainThread]: Running with dbt=1.0.3
09:01:41.981625 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['sat_customer'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
09:01:41.981979 [debug] [MainThread]: Tracking: tracking
09:01:42.003062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f699730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6aaee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6aa5e0>]}
09:01:42.436552 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
09:01:42.437018 [debug] [MainThread]: Partial parsing: added file: test_dbx://models/datavault/sat_customer.sql
09:01:42.452479 [debug] [MainThread]: 1603: static parser failed on datavault/sat_customer.sql
09:01:42.557533 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/sat_customer.sql
09:01:42.575967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '012252c5-42ed-4e10-9a83-cc219b16cb8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa350d0>]}
09:01:42.592301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '012252c5-42ed-4e10-9a83-cc219b16cb8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f873100>]}
09:01:42.592767 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
09:01:42.594188 [info ] [MainThread]: 
09:01:42.594709 [debug] [MainThread]: Acquiring new databricks connection "master"
09:01:42.595706 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
09:01:42.606327 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
09:01:42.606593 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
09:01:42.606761 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
09:01:42.606923 [debug] [ThreadPool]: Opening a new connection, currently in state init
09:01:43.850363 [debug] [ThreadPool]: SQL status: OK in 1.24 seconds
09:01:44.210937 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
09:01:44.211279 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
09:01:44.211517 [debug] [ThreadPool]: On list_None_test_dbt: Close
09:01:44.398096 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
09:01:44.398670 [info ] [MainThread]: 
09:01:44.421618 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
09:01:44.422488 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
09:01:44.425282 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
09:01:44.425510 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
09:01:44.442665 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
09:01:44.444182 [debug] [Thread-1  ]: finished collecting timing info
09:01:44.444507 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
09:01:44.444755 [debug] [Thread-1  ]: finished collecting timing info
09:01:44.445190 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
09:01:44.446077 [debug] [MainThread]: Connection 'master' was properly closed.
09:01:44.446279 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
09:01:44.471859 [info ] [MainThread]: Done.
09:01:44.472319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa6d070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa6d160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa6d3a0>]}


============================== 2022-03-23 09:04:18.987489 | 4681b1db-2795-483f-a56c-c83c7e46feb9 ==============================
09:04:18.987489 [info ] [MainThread]: Running with dbt=1.0.3
09:04:18.988814 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
09:04:18.989204 [debug] [MainThread]: Tracking: tracking
09:04:19.027920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f7d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f7dbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f7d220>]}
09:04:19.117919 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
09:04:19.118348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4681b1db-2795-483f-a56c-c83c7e46feb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f7d2b0>]}
09:04:19.412211 [debug] [MainThread]: Parsing macros/adapters.sql
09:04:19.436923 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:04:19.441039 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:04:19.441658 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:04:19.444558 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:04:19.461779 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:04:19.466647 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:04:19.470420 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:04:19.475725 [debug] [MainThread]: Parsing macros/adapters.sql
09:04:19.512287 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:04:19.521201 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:04:19.521729 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:04:19.524543 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:04:19.547681 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:04:19.552463 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:04:19.558313 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:04:19.563580 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
09:04:19.567163 [debug] [MainThread]: Parsing macros/materializations/configs.sql
09:04:19.569307 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
09:04:19.570817 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
09:04:19.586667 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
09:04:19.597167 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
09:04:19.608025 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
09:04:19.612181 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
09:04:19.613862 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
09:04:19.615482 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
09:04:19.619568 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
09:04:19.630109 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
09:04:19.631526 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
09:04:19.640727 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
09:04:19.655265 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
09:04:19.662037 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
09:04:19.664733 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
09:04:19.671215 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
09:04:19.672443 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
09:04:19.674900 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
09:04:19.677033 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
09:04:19.682517 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
09:04:19.697851 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
09:04:19.699257 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
09:04:19.701580 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
09:04:19.703029 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
09:04:19.703854 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
09:04:19.704436 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
09:04:19.705085 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
09:04:19.706336 [debug] [MainThread]: Parsing macros/etc/statement.sql
09:04:19.710331 [debug] [MainThread]: Parsing macros/etc/datetime.sql
09:04:19.718046 [debug] [MainThread]: Parsing macros/adapters/schema.sql
09:04:19.720065 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
09:04:19.722607 [debug] [MainThread]: Parsing macros/adapters/relation.sql
09:04:19.731906 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
09:04:19.734691 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
09:04:19.738787 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
09:04:19.746011 [debug] [MainThread]: Parsing macros/adapters/columns.sql
09:04:19.755181 [debug] [MainThread]: Parsing tests/generic/builtin.sql
09:04:19.758335 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
09:04:19.759434 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
09:04:19.760596 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
09:04:19.761720 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
09:04:19.767422 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
09:04:19.768498 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
09:04:19.769694 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
09:04:19.772620 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
09:04:19.773629 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
09:04:19.775378 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
09:04:19.777569 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
09:04:19.786763 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
09:04:19.788522 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
09:04:19.789830 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
09:04:19.791155 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
09:04:19.792678 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
09:04:19.793981 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
09:04:19.795500 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
09:04:19.796403 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
09:04:19.799661 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
09:04:19.804493 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
09:04:19.805942 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
09:04:19.809205 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
09:04:19.810886 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
09:04:19.812261 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
09:04:19.814017 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
09:04:19.836645 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
09:04:19.838583 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
09:04:19.840935 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
09:04:19.842383 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
09:04:19.843411 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
09:04:19.844630 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
09:04:19.845666 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
09:04:19.846812 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
09:04:19.848340 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
09:04:19.849895 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
09:04:19.852089 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
09:04:19.853683 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
09:04:19.854863 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
09:04:19.857076 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
09:04:19.859166 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
09:04:19.860481 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
09:04:19.861689 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
09:04:19.864339 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
09:04:19.866152 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
09:04:19.867898 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
09:04:19.870035 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
09:04:19.872589 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
09:04:19.873950 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
09:04:19.877161 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
09:04:19.885468 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
09:04:19.889312 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
09:04:19.890892 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
09:04:19.893994 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
09:04:19.897883 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
09:04:19.901019 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
09:04:19.902596 [debug] [MainThread]: Parsing macros/sql/star.sql
09:04:19.907268 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
09:04:19.914601 [debug] [MainThread]: Parsing macros/sql/union.sql
09:04:19.924329 [debug] [MainThread]: Parsing macros/sql/groupby.sql
09:04:19.925590 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
09:04:19.928732 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
09:04:19.930358 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
09:04:19.931889 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
09:04:19.937674 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
09:04:19.942504 [debug] [MainThread]: Parsing macros/sql/pivot.sql
09:04:19.946441 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
09:04:19.948541 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
09:04:19.949705 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
09:04:19.955066 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
09:04:19.977762 [debug] [MainThread]: Parsing macros/get_base_dates.sql
09:04:19.982662 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
09:04:19.986615 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
09:04:19.988430 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
09:04:19.989051 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
09:04:19.989588 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
09:04:19.990260 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
09:04:19.990788 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
09:04:19.993711 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
09:04:19.996469 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
09:04:19.997342 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
09:04:19.999979 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
09:04:20.002334 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
09:04:20.003440 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
09:04:20.003974 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
09:04:20.004542 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
09:04:20.005227 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
09:04:20.005746 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
09:04:20.006276 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
09:04:20.008075 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
09:04:20.013185 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
09:04:20.014073 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
09:04:20.015365 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
09:04:20.016257 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
09:04:20.017227 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
09:04:20.017828 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
09:04:20.024661 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
09:04:20.026709 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
09:04:20.027609 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
09:04:20.030399 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
09:04:20.031083 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
09:04:20.032770 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
09:04:20.037175 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
09:04:20.038056 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
09:04:20.040761 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
09:04:20.042950 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
09:04:20.043627 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
09:04:20.044388 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
09:04:20.052117 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
09:04:20.064712 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
09:04:20.075747 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
09:04:20.076557 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
09:04:20.092229 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
09:04:20.093129 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
09:04:20.101792 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
09:04:20.112592 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
09:04:20.125315 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
09:04:20.136343 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
09:04:20.144224 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
09:04:20.168271 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
09:04:20.181359 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
09:04:20.187932 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
09:04:20.197831 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
09:04:20.209670 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
09:04:20.224107 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
09:04:20.237201 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
09:04:20.276074 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
09:04:20.276922 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
09:04:20.292058 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
09:04:20.292935 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
09:04:20.301464 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
09:04:20.313493 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
09:04:20.325662 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
09:04:20.337882 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
09:04:20.341372 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
09:04:20.342493 [debug] [MainThread]: Parsing macros/staging/stage.sql
09:04:20.352528 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
09:04:20.361792 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
09:04:20.367599 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
09:04:20.369945 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
09:04:20.382639 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
09:04:20.386339 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
09:04:20.388345 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
09:04:20.393240 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
09:04:20.396543 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
09:04:20.398038 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
09:04:20.400129 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
09:04:20.400497 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
09:04:20.404752 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
09:04:20.413388 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
09:04:20.428890 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
09:04:20.433858 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
09:04:20.434887 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
09:04:20.459518 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
09:04:20.461751 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
09:04:20.466737 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
09:04:20.472679 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
09:04:20.475836 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
09:04:20.479428 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
09:04:20.486532 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
09:04:20.497153 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
09:04:20.501506 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
09:04:20.506708 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
09:04:20.511521 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
09:04:20.512571 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
09:04:20.514183 [debug] [MainThread]: Parsing macros/supporting/hash.sql
09:04:20.535306 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
09:04:20.540548 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
09:04:20.542631 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
09:04:21.350004 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
09:04:21.359825 [debug] [MainThread]: 1603: static parser failed on datavault/sat_customer.sql
09:04:21.429199 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/sat_customer.sql
09:04:21.430311 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
09:04:21.479276 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
09:04:21.480384 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
09:04:21.570303 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
09:04:21.648889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4681b1db-2795-483f-a56c-c83c7e46feb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042cb0d0>]}
09:04:21.677668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4681b1db-2795-483f-a56c-c83c7e46feb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10424f160>]}
09:04:21.678117 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
09:04:21.678732 [warn ] [MainThread]: The selection criterion 'tag:datavault' does not match any nodes
09:04:21.679900 [info ] [MainThread]: 
09:04:21.680275 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
09:04:21.695754 [info ] [MainThread]: Done.
09:04:21.696166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f60bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f616d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f61160>]}


============================== 2022-03-23 09:04:59.077125 | 1541cc7d-9f3b-4496-928e-44b0e59ab4c4 ==============================
09:04:59.077125 [info ] [MainThread]: Running with dbt=1.0.3
09:04:59.078178 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
09:04:59.078471 [debug] [MainThread]: Tracking: tracking
09:04:59.097070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b03190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112afe550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112afe4c0>]}
09:04:59.190957 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
09:04:59.191393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1541cc7d-9f3b-4496-928e-44b0e59ab4c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b03490>]}
09:04:59.482933 [debug] [MainThread]: Parsing macros/adapters.sql
09:04:59.505631 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:04:59.509774 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:04:59.510402 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:04:59.513427 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:04:59.530064 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:04:59.534844 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:04:59.538907 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:04:59.544316 [debug] [MainThread]: Parsing macros/adapters.sql
09:04:59.581563 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:04:59.590280 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:04:59.590804 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:04:59.593870 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:04:59.616881 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:04:59.621769 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:04:59.627780 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:04:59.633262 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
09:04:59.636885 [debug] [MainThread]: Parsing macros/materializations/configs.sql
09:04:59.639028 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
09:04:59.640642 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
09:04:59.656091 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
09:04:59.666873 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
09:04:59.678722 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
09:04:59.682830 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
09:04:59.684463 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
09:04:59.686143 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
09:04:59.690394 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
09:04:59.701023 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
09:04:59.702408 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
09:04:59.711738 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
09:04:59.726194 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
09:04:59.733196 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
09:04:59.735760 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
09:04:59.742658 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
09:04:59.743843 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
09:04:59.746239 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
09:04:59.748530 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
09:04:59.754081 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
09:04:59.769350 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
09:04:59.770729 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
09:04:59.772910 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
09:04:59.774327 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
09:04:59.775123 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
09:04:59.775630 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
09:04:59.776268 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
09:04:59.777495 [debug] [MainThread]: Parsing macros/etc/statement.sql
09:04:59.781475 [debug] [MainThread]: Parsing macros/etc/datetime.sql
09:04:59.789334 [debug] [MainThread]: Parsing macros/adapters/schema.sql
09:04:59.791291 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
09:04:59.793835 [debug] [MainThread]: Parsing macros/adapters/relation.sql
09:04:59.802595 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
09:04:59.805324 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
09:04:59.809394 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
09:04:59.816106 [debug] [MainThread]: Parsing macros/adapters/columns.sql
09:04:59.825187 [debug] [MainThread]: Parsing tests/generic/builtin.sql
09:04:59.827982 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
09:04:59.828966 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
09:04:59.830120 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
09:04:59.831169 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
09:04:59.836843 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
09:04:59.837867 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
09:04:59.839088 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
09:04:59.841828 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
09:04:59.842822 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
09:04:59.844514 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
09:04:59.846684 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
09:04:59.855605 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
09:04:59.857288 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
09:04:59.858576 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
09:04:59.860006 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
09:04:59.861535 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
09:04:59.862840 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
09:04:59.864358 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
09:04:59.865242 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
09:04:59.868592 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
09:04:59.873372 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
09:04:59.874816 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
09:04:59.878499 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
09:04:59.880214 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
09:04:59.881561 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
09:04:59.883357 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
09:04:59.906207 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
09:04:59.908049 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
09:04:59.910507 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
09:04:59.911936 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
09:04:59.912953 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
09:04:59.914047 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
09:04:59.915058 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
09:04:59.916126 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
09:04:59.917649 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
09:04:59.919131 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
09:04:59.921246 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
09:04:59.922825 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
09:04:59.923952 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
09:04:59.926188 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
09:04:59.928140 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
09:04:59.929447 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
09:04:59.930598 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
09:04:59.933206 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
09:04:59.935016 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
09:04:59.936755 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
09:04:59.938754 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
09:04:59.941401 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
09:04:59.942850 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
09:04:59.945998 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
09:04:59.954357 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
09:04:59.958222 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
09:04:59.959693 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
09:04:59.962788 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
09:04:59.966729 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
09:04:59.969860 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
09:04:59.971474 [debug] [MainThread]: Parsing macros/sql/star.sql
09:04:59.975714 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
09:04:59.982795 [debug] [MainThread]: Parsing macros/sql/union.sql
09:04:59.992255 [debug] [MainThread]: Parsing macros/sql/groupby.sql
09:04:59.993588 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
09:04:59.996768 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
09:04:59.998272 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
09:04:59.999837 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
09:05:00.005767 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
09:05:00.010646 [debug] [MainThread]: Parsing macros/sql/pivot.sql
09:05:00.014362 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
09:05:00.016448 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
09:05:00.017996 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
09:05:00.024327 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
09:05:00.048639 [debug] [MainThread]: Parsing macros/get_base_dates.sql
09:05:00.053952 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
09:05:00.057904 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
09:05:00.059537 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
09:05:00.060225 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
09:05:00.060758 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
09:05:00.061431 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
09:05:00.062029 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
09:05:00.064931 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
09:05:00.066577 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
09:05:00.067330 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
09:05:00.069859 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
09:05:00.072238 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
09:05:00.073219 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
09:05:00.073749 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
09:05:00.074315 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
09:05:00.074997 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
09:05:00.075511 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
09:05:00.076034 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
09:05:00.077747 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
09:05:00.082858 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
09:05:00.083763 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
09:05:00.085073 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
09:05:00.085934 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
09:05:00.086910 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
09:05:00.087510 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
09:05:00.094400 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
09:05:00.096436 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
09:05:00.097319 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
09:05:00.100211 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
09:05:00.100904 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
09:05:00.102645 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
09:05:00.106798 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
09:05:00.107652 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
09:05:00.110113 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
09:05:00.112280 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
09:05:00.112964 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
09:05:00.113609 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
09:05:00.120954 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
09:05:00.133377 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
09:05:00.144529 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
09:05:00.145527 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
09:05:00.161396 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
09:05:00.162304 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
09:05:00.171013 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
09:05:00.181785 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
09:05:00.194219 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
09:05:00.205186 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
09:05:00.213115 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
09:05:00.236928 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
09:05:00.250089 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
09:05:00.256576 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
09:05:00.266452 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
09:05:00.278248 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
09:05:00.292546 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
09:05:00.305769 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
09:05:00.345313 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
09:05:00.346155 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
09:05:00.361801 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
09:05:00.362714 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
09:05:00.371559 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
09:05:00.383683 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
09:05:00.395875 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
09:05:00.407920 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
09:05:00.411277 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
09:05:00.412387 [debug] [MainThread]: Parsing macros/staging/stage.sql
09:05:00.422118 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
09:05:00.430686 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
09:05:00.436502 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
09:05:00.438799 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
09:05:00.451101 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
09:05:00.454338 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
09:05:00.456372 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
09:05:00.461187 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
09:05:00.464090 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
09:05:00.465562 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
09:05:00.467672 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
09:05:00.468245 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
09:05:00.472497 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
09:05:00.481232 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
09:05:00.498031 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
09:05:00.503012 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
09:05:00.504691 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
09:05:00.529630 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
09:05:00.532038 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
09:05:00.536864 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
09:05:00.542713 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
09:05:00.546024 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
09:05:00.549463 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
09:05:00.556272 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
09:05:00.567439 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
09:05:00.572133 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
09:05:00.577217 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
09:05:00.581938 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
09:05:00.583026 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
09:05:00.584580 [debug] [MainThread]: Parsing macros/supporting/hash.sql
09:05:00.605855 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
09:05:00.611041 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
09:05:00.613096 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
09:05:01.442174 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
09:05:01.451722 [debug] [MainThread]: 1603: static parser failed on datavault/sat_customer.sql
09:05:01.520647 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/sat_customer.sql
09:05:01.521870 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
09:05:01.569741 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
09:05:01.570786 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
09:05:01.659861 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
09:05:01.743478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1541cc7d-9f3b-4496-928e-44b0e59ab4c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ebe0d0>]}
09:05:01.771769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1541cc7d-9f3b-4496-928e-44b0e59ab4c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b7daf0>]}
09:05:01.772212 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
09:05:01.773629 [info ] [MainThread]: 
09:05:01.774169 [debug] [MainThread]: Acquiring new databricks connection "master"
09:05:01.775161 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
09:05:01.786133 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
09:05:01.786409 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
09:05:01.786578 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
09:05:01.786735 [debug] [ThreadPool]: Opening a new connection, currently in state init
09:05:02.741937 [debug] [ThreadPool]: SQL status: OK in 0.96 seconds
09:05:03.100361 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
09:05:03.100696 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
09:05:03.100917 [debug] [ThreadPool]: On list_None_test_dbt: Close
09:05:03.270761 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
09:05:03.271313 [info ] [MainThread]: 
09:05:03.277548 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
09:05:03.278140 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
09:05:03.280997 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
09:05:03.281309 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
09:05:03.294684 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
09:05:03.295855 [debug] [Thread-1  ]: finished collecting timing info
09:05:03.296088 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
09:05:03.296301 [debug] [Thread-1  ]: finished collecting timing info
09:05:03.296714 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
09:05:03.296946 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
09:05:03.297395 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
09:05:03.297585 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
09:05:03.297765 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
09:05:03.314114 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
09:05:03.321306 [debug] [Thread-1  ]: finished collecting timing info
09:05:03.321650 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
09:05:03.321861 [debug] [Thread-1  ]: finished collecting timing info
09:05:03.322264 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
09:05:03.323039 [debug] [MainThread]: Connection 'master' was properly closed.
09:05:03.323226 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
09:05:03.370126 [info ] [MainThread]: Done.
09:05:03.370570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ede880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112edec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112eded30>]}


============================== 2022-03-23 09:06:53.995086 | e5c8b2e4-7c96-4428-8262-93640b4f38aa ==============================
09:06:53.995086 [info ] [MainThread]: Running with dbt=1.0.3
09:06:53.995856 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
09:06:53.996190 [debug] [MainThread]: Tracking: tracking
09:06:54.030325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0a5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a08c820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a08c100>]}
09:06:54.394188 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
09:06:54.394452 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
09:06:54.402764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e5c8b2e4-7c96-4428-8262-93640b4f38aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a39a0d0>]}
09:06:54.423717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e5c8b2e4-7c96-4428-8262-93640b4f38aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a25f3d0>]}
09:06:54.424102 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
09:06:54.425434 [info ] [MainThread]: 
09:06:54.425930 [debug] [MainThread]: Acquiring new databricks connection "master"
09:06:54.426874 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
09:06:54.438442 [debug] [ThreadPool]: Using databricks connection "list_schemas"
09:06:54.438742 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
09:06:54.438943 [debug] [ThreadPool]: Opening a new connection, currently in state init
09:06:55.055880 [debug] [ThreadPool]: SQL status: OK in 0.62 seconds
09:06:55.364242 [debug] [ThreadPool]: On list_schemas: Close
09:06:55.530333 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
09:06:55.539379 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
09:06:55.539673 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
09:06:55.539867 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
09:06:55.540022 [debug] [ThreadPool]: Opening a new connection, currently in state closed
09:06:56.387194 [debug] [ThreadPool]: SQL status: OK in 0.85 seconds
09:06:56.657552 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
09:06:56.657849 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
09:06:56.658038 [debug] [ThreadPool]: On list_None_test_dbt: Close
09:06:56.807010 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
09:06:56.807293 [debug] [MainThread]: Spark adapter: NotImplemented: commit
09:06:56.807671 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
09:06:56.807975 [info ] [MainThread]: 
09:06:56.825903 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
09:06:56.826307 [info ] [Thread-1  ]: 1 of 2 START incremental model test_dbt.hub_customer............................ [RUN]
09:06:56.826848 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
09:06:56.829676 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
09:06:56.830090 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
09:06:56.943563 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
09:06:56.944676 [debug] [Thread-1  ]: finished collecting timing info
09:06:56.944893 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
09:06:57.016187 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
09:06:57.024499 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
09:06:57.024760 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
09:06:57.024938 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

      create or replace table test_dbt.hub_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/hub_customer'
    
    as
      

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE
    FROM
    (
        SELECT rr.CUSTOMER_HK, rr.CUSTOMER_ID, rr.LOAD_DATETIME, rr.RECORD_SOURCE,
               ROW_NUMBER() OVER(
                   PARTITION BY rr.CUSTOMER_HK
                   ORDER BY rr.LOAD_DATETIME
               ) AS row_number
        FROM test_dbt.stg_customer AS rr
        WHERE rr.CUSTOMER_HK IS NOT NULL
    ) h
    WHERE h.row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert
09:06:57.025100 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
09:07:01.928632 [debug] [Thread-1  ]: SQL status: OK in 4.9 seconds
09:07:01.936547 [debug] [Thread-1  ]: finished collecting timing info
09:07:01.936825 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
09:07:01.937013 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
09:07:01.937184 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
09:07:02.153771 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5c8b2e4-7c96-4428-8262-93640b4f38aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4466d0>]}
09:07:02.154360 [info ] [Thread-1  ]: 1 of 2 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 5.33s]
09:07:02.154804 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
09:07:02.155075 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
09:07:02.155820 [info ] [Thread-1  ]: 2 of 2 START incremental model test_dbt.sat_customer............................ [RUN]
09:07:02.156423 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
09:07:02.159128 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
09:07:02.159344 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
09:07:02.205937 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
09:07:02.222558 [debug] [Thread-1  ]: finished collecting timing info
09:07:02.222988 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
09:07:02.229904 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.sat_customer"
09:07:02.230778 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
09:07:02.230982 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
09:07:02.231160 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

      create or replace table test_dbt.sat_customer
    
    
    using delta
    
    
    
    
    location '/mnt/adls_ss_finance/SS Finance/test/sat_customer'
    
    as
      

-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_HASHDIFF, a.FIRST_NAME, a.LAST_NAME, a.EMAIL, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM test_dbt.stg_customer AS a
    WHERE a.CUSTOMER_HK IS NOT NULL
),



records_to_insert AS (
    SELECT DISTINCT stage.CUSTOMER_HK, stage.CUSTOMER_HASHDIFF AS HASHDIFF, stage.FIRST_NAME, stage.LAST_NAME, stage.EMAIL, stage.LOAD_DATETIME, stage.RECORD_SOURCE
    FROM source_data AS stage
)

SELECT * FROM records_to_insert
09:07:02.231332 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
09:07:06.086318 [debug] [Thread-1  ]: SQL status: OK in 3.85 seconds
09:07:06.088741 [debug] [Thread-1  ]: finished collecting timing info
09:07:06.089009 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: ROLLBACK
09:07:06.089225 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
09:07:06.089426 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: Close
09:07:06.654362 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5c8b2e4-7c96-4428-8262-93640b4f38aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4ffa00>]}
09:07:06.655057 [info ] [Thread-1  ]: 2 of 2 OK created incremental model test_dbt.sat_customer....................... [[32mOK[0m in 4.50s]
09:07:06.655587 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
09:07:06.657014 [debug] [MainThread]: Acquiring new databricks connection "master"
09:07:06.657284 [debug] [MainThread]: On master: ROLLBACK
09:07:06.657492 [debug] [MainThread]: Opening a new connection, currently in state init
09:07:06.951062 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
09:07:06.951470 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
09:07:06.951724 [debug] [MainThread]: Spark adapter: NotImplemented: commit
09:07:06.951992 [debug] [MainThread]: On master: ROLLBACK
09:07:06.952230 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
09:07:06.952464 [debug] [MainThread]: On master: Close
09:07:07.249664 [info ] [MainThread]: 
09:07:07.250232 [info ] [MainThread]: Finished running 2 incremental models in 12.82s.
09:07:07.250668 [debug] [MainThread]: Connection 'master' was properly closed.
09:07:07.250918 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
09:07:07.272621 [info ] [MainThread]: 
09:07:07.273171 [info ] [MainThread]: [32mCompleted successfully[0m
09:07:07.273518 [info ] [MainThread]: 
09:07:07.273857 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
09:07:07.274563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4ebdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3c8940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a48d580>]}


============================== 2022-03-23 09:49:54.119547 | 030b807c-0bd0-44d1-8d59-c7113c89b196 ==============================
09:49:54.119547 [info ] [MainThread]: Running with dbt=1.0.3
09:49:54.120503 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
09:49:54.120808 [debug] [MainThread]: Tracking: tracking
09:49:54.146592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041bff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043cfdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043cf4c0>]}
09:49:54.242986 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
09:49:54.243397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '030b807c-0bd0-44d1-8d59-c7113c89b196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043cfd90>]}
09:49:54.499539 [debug] [MainThread]: Parsing macros/adapters.sql
09:49:54.525864 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:49:54.530138 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:49:54.530729 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:49:54.533564 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:49:54.550063 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:49:54.554744 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:49:54.558518 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:49:54.563786 [debug] [MainThread]: Parsing macros/adapters.sql
09:49:54.601783 [debug] [MainThread]: Parsing macros/materializations/seed.sql
09:49:54.610887 [debug] [MainThread]: Parsing macros/materializations/view.sql
09:49:54.611470 [debug] [MainThread]: Parsing macros/materializations/table.sql
09:49:54.614240 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
09:49:54.638509 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
09:49:54.643266 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
09:49:54.649081 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
09:49:54.654378 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
09:49:54.657900 [debug] [MainThread]: Parsing macros/materializations/configs.sql
09:49:54.660037 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
09:49:54.661705 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
09:49:54.677237 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
09:49:54.688715 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
09:49:54.699488 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
09:49:54.703601 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
09:49:54.705245 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
09:49:54.706880 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
09:49:54.710956 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
09:49:54.721375 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
09:49:54.722741 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
09:49:54.732053 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
09:49:54.746460 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
09:49:54.753237 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
09:49:54.755828 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
09:49:54.762311 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
09:49:54.763534 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
09:49:54.765973 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
09:49:54.768060 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
09:49:54.773549 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
09:49:54.788749 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
09:49:54.790095 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
09:49:54.792320 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
09:49:54.793747 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
09:49:54.794546 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
09:49:54.795059 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
09:49:54.795696 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
09:49:54.796937 [debug] [MainThread]: Parsing macros/etc/statement.sql
09:49:54.800978 [debug] [MainThread]: Parsing macros/etc/datetime.sql
09:49:54.808635 [debug] [MainThread]: Parsing macros/adapters/schema.sql
09:49:54.810631 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
09:49:54.813081 [debug] [MainThread]: Parsing macros/adapters/relation.sql
09:49:54.821977 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
09:49:54.824614 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
09:49:54.828617 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
09:49:54.835478 [debug] [MainThread]: Parsing macros/adapters/columns.sql
09:49:54.844533 [debug] [MainThread]: Parsing tests/generic/builtin.sql
09:49:54.847309 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
09:49:54.848308 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
09:49:54.849466 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
09:49:54.850513 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
09:49:54.856116 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
09:49:54.857157 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
09:49:54.858348 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
09:49:54.860965 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
09:49:54.861954 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
09:49:54.863646 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
09:49:54.865832 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
09:49:54.874786 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
09:49:54.876555 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
09:49:54.877802 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
09:49:54.879073 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
09:49:54.880514 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
09:49:54.881748 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
09:49:54.883171 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
09:49:54.884015 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
09:49:54.887123 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
09:49:54.891765 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
09:49:54.893150 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
09:49:54.896227 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
09:49:54.897828 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
09:49:54.899222 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
09:49:54.900952 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
09:49:54.923240 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
09:49:54.924946 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
09:49:54.927196 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
09:49:54.928572 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
09:49:54.929555 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
09:49:54.930630 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
09:49:54.931610 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
09:49:54.932649 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
09:49:54.934359 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
09:49:54.935899 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
09:49:54.937897 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
09:49:54.939425 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
09:49:54.940511 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
09:49:54.942629 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
09:49:54.944472 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
09:49:54.945735 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
09:49:54.946830 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
09:49:54.949285 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
09:49:54.951222 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
09:49:54.952895 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
09:49:54.954805 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
09:49:54.957275 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
09:49:54.958577 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
09:49:54.963742 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
09:49:54.972141 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
09:49:54.976028 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
09:49:54.977442 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
09:49:54.980515 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
09:49:54.984355 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
09:49:54.987486 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
09:49:54.988953 [debug] [MainThread]: Parsing macros/sql/star.sql
09:49:54.992973 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
09:49:55.000029 [debug] [MainThread]: Parsing macros/sql/union.sql
09:49:55.009119 [debug] [MainThread]: Parsing macros/sql/groupby.sql
09:49:55.010363 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
09:49:55.013343 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
09:49:55.015474 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
09:49:55.016921 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
09:49:55.022924 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
09:49:55.027528 [debug] [MainThread]: Parsing macros/sql/pivot.sql
09:49:55.031139 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
09:49:55.034876 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
09:49:55.036156 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
09:49:55.042312 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
09:49:55.064873 [debug] [MainThread]: Parsing macros/get_base_dates.sql
09:49:55.069716 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
09:49:55.073588 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
09:49:55.075229 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
09:49:55.075836 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
09:49:55.076373 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
09:49:55.077052 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
09:49:55.077592 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
09:49:55.080622 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
09:49:55.082220 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
09:49:55.082991 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
09:49:55.085674 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
09:49:55.088044 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
09:49:55.089059 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
09:49:55.089615 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
09:49:55.090195 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
09:49:55.090891 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
09:49:55.091418 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
09:49:55.091951 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
09:49:55.093715 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
09:49:55.098860 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
09:49:55.099889 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
09:49:55.101228 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
09:49:55.102093 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
09:49:55.103079 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
09:49:55.103684 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
09:49:55.111213 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
09:49:55.113370 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
09:49:55.114279 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
09:49:55.117022 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
09:49:55.117703 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
09:49:55.119412 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
09:49:55.123608 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
09:49:55.124462 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
09:49:55.127500 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
09:49:55.129944 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
09:49:55.130644 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
09:49:55.131326 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
09:49:55.139499 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
09:49:55.152206 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
09:49:55.163558 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
09:49:55.164411 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
09:49:55.180514 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
09:49:55.181450 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
09:49:55.190076 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
09:49:55.200724 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
09:49:55.213204 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
09:49:55.224087 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
09:49:55.231883 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
09:49:55.255493 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
09:49:55.268530 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
09:49:55.275215 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
09:49:55.285403 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
09:49:55.297230 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
09:49:55.311218 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
09:49:55.324995 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
09:49:55.364362 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
09:49:55.365176 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
09:49:55.380685 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
09:49:55.381992 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
09:49:55.391842 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
09:49:55.403986 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
09:49:55.416101 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
09:49:55.428144 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
09:49:55.431623 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
09:49:55.432743 [debug] [MainThread]: Parsing macros/staging/stage.sql
09:49:55.442857 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
09:49:55.451507 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
09:49:55.457304 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
09:49:55.459647 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
09:49:55.472302 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
09:49:55.475535 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
09:49:55.477576 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
09:49:55.482471 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
09:49:55.485474 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
09:49:55.486974 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
09:49:55.489150 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
09:49:55.489528 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
09:49:55.493808 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
09:49:55.502720 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
09:49:55.518174 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
09:49:55.523240 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
09:49:55.524258 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
09:49:55.548972 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
09:49:55.551220 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
09:49:55.556304 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
09:49:55.562273 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
09:49:55.565503 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
09:49:55.569705 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
09:49:55.576511 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
09:49:55.587208 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
09:49:55.591346 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
09:49:55.596435 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
09:49:55.601170 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
09:49:55.602216 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
09:49:55.603823 [debug] [MainThread]: Parsing macros/supporting/hash.sql
09:49:55.624191 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
09:49:55.629365 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
09:49:55.631456 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
09:49:56.438881 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
09:49:56.448638 [debug] [MainThread]: 1603: static parser failed on datavault/sat_customer.sql
09:49:56.520255 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/sat_customer.sql
09:49:56.521384 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
09:49:56.570441 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
09:49:56.571486 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
09:49:56.660765 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
09:49:56.769894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '030b807c-0bd0-44d1-8d59-c7113c89b196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046dbf10>]}
09:49:56.786055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '030b807c-0bd0-44d1-8d59-c7113c89b196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10478e520>]}
09:49:56.786384 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
09:49:56.787556 [info ] [MainThread]: 
09:49:56.788031 [debug] [MainThread]: Acquiring new databricks connection "master"
09:49:56.788856 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
09:49:56.799405 [debug] [ThreadPool]: Using databricks connection "list_schemas"
09:49:56.799675 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
09:49:56.799848 [debug] [ThreadPool]: Opening a new connection, currently in state init
09:49:57.717058 [debug] [ThreadPool]: SQL status: OK in 0.92 seconds
09:49:58.096643 [debug] [ThreadPool]: On list_schemas: Close
09:49:58.325797 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
09:49:58.335903 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
09:49:58.336154 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
09:49:58.336337 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
09:49:58.336505 [debug] [ThreadPool]: Opening a new connection, currently in state closed
09:49:59.028670 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
09:49:59.302706 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
09:49:59.303020 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
09:49:59.303227 [debug] [ThreadPool]: On list_None_test_dbt: Close
09:49:59.465395 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
09:49:59.465771 [debug] [MainThread]: Spark adapter: NotImplemented: commit
09:49:59.466285 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
09:49:59.466745 [info ] [MainThread]: 
09:49:59.473568 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
09:49:59.473999 [info ] [Thread-1  ]: 1 of 2 START incremental model test_dbt.hub_customer............................ [RUN]
09:49:59.474583 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
09:49:59.477215 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
09:49:59.477438 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
09:49:59.495572 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
09:49:59.510876 [debug] [Thread-1  ]: finished collecting timing info
09:49:59.511151 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
09:49:59.560547 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
09:49:59.560796 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    create temporary view hub_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE
    FROM
    (
        SELECT rr.CUSTOMER_HK, rr.CUSTOMER_ID, rr.LOAD_DATETIME, rr.RECORD_SOURCE,
               ROW_NUMBER() OVER(
                   PARTITION BY rr.CUSTOMER_HK
                   ORDER BY rr.LOAD_DATETIME
               ) AS row_number
        FROM test_dbt.stg_customer AS rr
        WHERE rr.CUSTOMER_HK IS NOT NULL
    ) h
    WHERE h.row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    LEFT JOIN test_dbt.hub_customer AS d
    ON a.CUSTOMER_HK = d.CUSTOMER_HK
    WHERE d.CUSTOMER_HK IS NULL
)

SELECT * FROM records_to_insert

  
09:49:59.560956 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
09:49:59.942882 [debug] [Thread-1  ]: SQL status: OK in 0.38 seconds
09:50:00.143591 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
09:50:00.160921 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
09:50:00.161188 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
09:50:00.161361 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    
  
  
    merge into test_dbt.hub_customer as DBT_INTERNAL_DEST
      using hub_customer__dbt_tmp as DBT_INTERNAL_SOURCE
      
      
    
        on false
    
  
      
      when matched then update set
         * 
    
      when not matched then insert *

09:50:04.257936 [debug] [Thread-1  ]: SQL status: OK in 4.1 seconds
09:50:04.300069 [debug] [Thread-1  ]: finished collecting timing info
09:50:04.300413 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
09:50:04.300744 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
09:50:04.300958 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
09:50:04.633953 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '030b807c-0bd0-44d1-8d59-c7113c89b196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ac070>]}
09:50:04.634520 [info ] [Thread-1  ]: 1 of 2 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 5.16s]
09:50:04.634956 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
09:50:04.635221 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
09:50:04.635797 [info ] [Thread-1  ]: 2 of 2 START incremental model test_dbt.sat_customer............................ [RUN]
09:50:04.636458 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
09:50:04.639276 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
09:50:04.639585 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
09:50:04.661892 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
09:50:04.662912 [debug] [Thread-1  ]: finished collecting timing info
09:50:04.663152 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
09:50:04.667423 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
09:50:04.667671 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    create temporary view sat_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_HASHDIFF, a.FIRST_NAME, a.LAST_NAME, a.EMAIL, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM test_dbt.stg_customer AS a
    WHERE a.CUSTOMER_HK IS NOT NULL
),


latest_records_pre AS (
    SELECT current_records.CUSTOMER_HK, current_records.HASHDIFF, current_records.LOAD_DATETIME,
           RANK() OVER (
               PARTITION BY current_records.CUSTOMER_HK
               ORDER BY current_records.LOAD_DATETIME DESC
           ) AS rank
    FROM test_dbt.sat_customer AS current_records
    JOIN (
        SELECT DISTINCT source_data.CUSTOMER_HK
        FROM source_data
    ) AS source_records
    ON current_records.CUSTOMER_HK = source_records.CUSTOMER_HK
),
latest_records AS (

    SELECT *
    FROM latest_records_pre
    WHERE rank = 1
),

records_to_insert AS (
    SELECT DISTINCT stage.CUSTOMER_HK, stage.CUSTOMER_HASHDIFF AS HASHDIFF, stage.FIRST_NAME, stage.LAST_NAME, stage.EMAIL, stage.LOAD_DATETIME, stage.RECORD_SOURCE
    FROM source_data AS stage
    LEFT JOIN latest_records
    ON latest_records.CUSTOMER_HK = stage.CUSTOMER_HK
    WHERE latest_records.HASHDIFF != stage.CUSTOMER_HASHDIFF
        OR latest_records.HASHDIFF IS NULL
)

SELECT * FROM records_to_insert

  
09:50:04.667847 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
09:50:05.049850 [debug] [Thread-1  ]: SQL status: OK in 0.38 seconds
09:50:05.217118 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.sat_customer"
09:50:05.220750 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
09:50:05.220995 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
09:50:05.221159 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    
  
  
    merge into test_dbt.sat_customer as DBT_INTERNAL_DEST
      using sat_customer__dbt_tmp as DBT_INTERNAL_SOURCE
      
      
    
        on false
    
  
      
      when matched then update set
         * 
    
      when not matched then insert *

09:50:09.044572 [debug] [Thread-1  ]: SQL status: OK in 3.82 seconds
09:50:09.047004 [debug] [Thread-1  ]: finished collecting timing info
09:50:09.047321 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: ROLLBACK
09:50:09.047538 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
09:50:09.047745 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: Close
09:50:09.400847 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '030b807c-0bd0-44d1-8d59-c7113c89b196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046773d0>]}
09:50:09.401490 [info ] [Thread-1  ]: 2 of 2 OK created incremental model test_dbt.sat_customer....................... [[32mOK[0m in 4.76s]
09:50:09.402028 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
09:50:09.403450 [debug] [MainThread]: Acquiring new databricks connection "master"
09:50:09.403727 [debug] [MainThread]: On master: ROLLBACK
09:50:09.403942 [debug] [MainThread]: Opening a new connection, currently in state init
09:50:09.518683 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
09:50:09.519099 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
09:50:09.519364 [debug] [MainThread]: Spark adapter: NotImplemented: commit
09:50:09.519640 [debug] [MainThread]: On master: ROLLBACK
09:50:09.519888 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
09:50:09.520142 [debug] [MainThread]: On master: Close
09:50:09.613843 [info ] [MainThread]: 
09:50:09.614413 [info ] [MainThread]: Finished running 2 incremental models in 12.83s.
09:50:09.614864 [debug] [MainThread]: Connection 'master' was properly closed.
09:50:09.615155 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
09:50:09.636830 [info ] [MainThread]: 
09:50:09.637216 [info ] [MainThread]: [32mCompleted successfully[0m
09:50:09.637570 [info ] [MainThread]: 
09:50:09.638010 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
09:50:09.638441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045061f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104506f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104506040>]}


============================== 2022-03-23 12:32:43.539603 | d255ecab-8d47-446e-9e5e-efe52c6abc76 ==============================
12:32:43.539603 [info ] [MainThread]: Running with dbt=1.0.3
12:32:43.540495 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
12:32:43.540858 [debug] [MainThread]: Tracking: tracking
12:32:43.616933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1251f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d15ad00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d15a070>]}
12:32:45.951625 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:32:45.951933 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:32:45.967712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd255ecab-8d47-446e-9e5e-efe52c6abc76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4340d0>]}
12:32:46.112578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd255ecab-8d47-446e-9e5e-efe52c6abc76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2fa400>]}
12:32:46.112970 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
12:32:46.114446 [info ] [MainThread]: 
12:32:46.115031 [debug] [MainThread]: Acquiring new databricks connection "master"
12:32:46.116129 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
12:32:46.130118 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:32:46.130423 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
12:32:46.130613 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
12:32:46.130790 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:32:50.868042 [debug] [ThreadPool]: SQL status: OK in 4.74 seconds
12:32:51.226352 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
12:32:51.226668 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:32:51.226882 [debug] [ThreadPool]: On list_None_test_dbt: Close
12:32:51.490119 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:32:51.490582 [info ] [MainThread]: 
12:32:51.522602 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
12:32:51.523132 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
12:32:51.525743 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
12:32:51.526015 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
12:32:51.637627 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
12:32:51.641860 [debug] [Thread-1  ]: finished collecting timing info
12:32:51.642125 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
12:32:51.642325 [debug] [Thread-1  ]: finished collecting timing info
12:32:51.642741 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
12:32:51.642952 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
12:32:51.643363 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
12:32:51.643534 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
12:32:51.643694 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
12:32:51.693874 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
12:32:51.712005 [debug] [Thread-1  ]: finished collecting timing info
12:32:51.712272 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
12:32:51.712479 [debug] [Thread-1  ]: finished collecting timing info
12:32:51.712928 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
12:32:51.713837 [debug] [MainThread]: Connection 'master' was properly closed.
12:32:51.714045 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
12:32:51.736901 [info ] [MainThread]: Done.
12:32:51.737451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d502c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d502a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d502be0>]}


============================== 2022-03-23 12:41:17.080409 | f7b70195-bc31-4652-aeee-32090ed03c23 ==============================
12:41:17.080409 [info ] [MainThread]: Running with dbt=1.0.3
12:41:17.081832 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
12:41:17.082454 [debug] [MainThread]: Tracking: tracking
12:41:17.128853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11274a550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11277ff40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11277fd00>]}
12:41:17.588012 [debug] [MainThread]: Partial parsing enabled: 23 files deleted, 12 files added, 10 files changed.
12:41:17.588530 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/alias.sql
12:41:17.588832 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/expand_column_list.sql
12:41:17.589110 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/check_required_parameters.sql
12:41:17.589359 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/materialisations/rank_mat_helpers.sql
12:41:17.589734 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/alias_all.sql
12:41:17.590048 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/as_constant.sql
12:41:17.590296 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/prepend_generated_by.sql
12:41:17.590570 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/supporting/date_timestamp.sql
12:41:17.590811 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/stage_processing_macros.sql
12:41:17.591038 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/materialisations/period_mat_helpers.sql
12:41:17.591315 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/is_checks.sql
12:41:17.591620 [debug] [MainThread]: Partial parsing: added file: dbtvault://macros/internal/multikey.sql
12:41:17.592065 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/supporting/get_query_results_as_dict.sql
12:41:17.592260 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
12:41:17.593056 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/as_constant.sql
12:41:17.594090 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/alias_all.sql
12:41:17.595426 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/multikey.sql
12:41:17.595723 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/alias.sql
12:41:17.595913 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/incremental_pit_materialization.sql
12:41:17.596102 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/period_mat_helpers/get_period_of_load.sql
12:41:17.596513 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/mat_is_checks.sql
12:41:17.596757 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/helpers/stage_processing_macros.sql
12:41:17.596983 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/helpers/prepend_generated_by.sql
12:41:17.597512 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/escape_column_names.sql
12:41:17.597737 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/incremental_pit_bridge_replace.sql
12:41:17.598116 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/helpers/is_checks.sql
12:41:17.598346 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/expand_column_list.sql
12:41:17.598626 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/check_required_parameters.sql
12:41:17.598856 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
12:41:17.599130 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
12:41:17.599365 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
12:41:17.599576 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/incremental_bridge_materialization.sql
12:41:17.599865 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
12:41:17.600050 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/internal/metadata_processing/concat_ws.sql
12:41:17.600227 [debug] [MainThread]: Partial parsing: deleted file: dbtvault://macros/materialisations/period_mat_helpers/get_period_boundaries.sql
12:41:17.600558 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/supporting/max_datetime.sql
12:41:17.601396 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/supporting/current_timestamp.sql
12:41:17.601646 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/materialisations/vault_insert_by_rank_materialization.sql
12:41:17.601872 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/materialisations/vault_insert_by_period_materialization.sql
12:41:17.602185 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/supporting/datatypes.sql
12:41:17.602433 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/supporting/hash.sql
12:41:17.602761 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/materialisations/shared_helpers.sql
12:41:17.603091 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/staging/derive_columns.sql
12:41:17.603519 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/staging/stage.sql
12:41:17.603788 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros/staging/rank_columns.sql
12:41:17.603998 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
12:41:17.629511 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
12:41:17.659754 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
12:41:17.663261 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
12:41:17.683499 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
12:41:17.696529 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
12:41:17.709963 [debug] [MainThread]: Parsing macros/internal/alias.sql
12:41:17.714438 [debug] [MainThread]: Parsing macros/internal/expand_column_list.sql
12:41:17.718548 [debug] [MainThread]: Parsing macros/internal/check_required_parameters.sql
12:41:17.720469 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers.sql
12:41:17.728174 [debug] [MainThread]: Parsing macros/internal/alias_all.sql
12:41:17.730996 [debug] [MainThread]: Parsing macros/internal/as_constant.sql
12:41:17.733652 [debug] [MainThread]: Parsing macros/internal/prepend_generated_by.sql
12:41:17.734180 [debug] [MainThread]: Parsing macros/supporting/date_timestamp.sql
12:41:17.736362 [debug] [MainThread]: Parsing macros/internal/stage_processing_macros.sql
12:41:17.747451 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers.sql
12:41:17.768236 [debug] [MainThread]: Parsing macros/internal/is_checks.sql
12:41:17.771616 [debug] [MainThread]: Parsing macros/internal/multikey.sql
12:41:17.777307 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
12:41:17.799203 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
12:41:17.815361 [debug] [MainThread]: Parsing macros/supporting/hash.sql
12:41:17.823391 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
12:41:17.830243 [debug] [MainThread]: Parsing macros/staging/stage.sql
12:41:17.842661 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
12:41:17.847124 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
12:41:17.862983 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
12:41:17.882517 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
12:41:17.893242 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
12:41:17.910230 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
12:41:17.926963 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
12:41:17.939712 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
12:41:17.948833 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
12:41:17.964468 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
12:41:17.984583 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
12:41:17.995223 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
12:41:18.008214 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
12:41:18.022167 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
12:41:18.071397 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
12:41:18.087032 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
12:41:18.104488 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
12:41:18.134267 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
12:41:18.142553 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
12:41:18.143779 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
12:41:18.144945 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
12:41:18.154514 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
12:41:18.155557 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
12:41:18.156503 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
12:41:18.173157 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
12:41:18.190410 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
12:41:18.207618 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
12:41:18.236422 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
12:41:18.253079 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
12:41:18.254501 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
12:41:18.256870 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
12:41:18.258407 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
12:41:18.766705 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
12:41:18.816736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a44be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ab7e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ab7ca0>]}


============================== 2022-03-23 12:43:21.073930 | 547b5c8b-cecb-4e02-b817-cb981afe5c7c ==============================
12:43:21.073930 [info ] [MainThread]: Running with dbt=1.0.3
12:43:21.075044 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
12:43:21.075351 [debug] [MainThread]: Tracking: tracking
12:43:21.098882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6e61c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d71ad00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d71af40>]}
12:43:21.491933 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
12:43:21.492275 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
12:43:21.518570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '547b5c8b-cecb-4e02-b817-cb981afe5c7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9f40d0>]}
12:43:21.535625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '547b5c8b-cecb-4e02-b817-cb981afe5c7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8be310>]}
12:43:21.536061 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
12:43:21.537719 [info ] [MainThread]: 
12:43:21.538569 [debug] [MainThread]: Acquiring new databricks connection "master"
12:43:21.540293 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
12:43:21.554913 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
12:43:21.555271 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
12:43:21.555543 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
12:43:21.555756 [debug] [ThreadPool]: Opening a new connection, currently in state init
12:43:22.655211 [debug] [ThreadPool]: SQL status: OK in 1.1 seconds
12:43:23.075762 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
12:43:23.076050 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
12:43:23.076238 [debug] [ThreadPool]: On list_None_test_dbt: Close
12:43:23.250239 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
12:43:23.250736 [info ] [MainThread]: 
12:43:23.269771 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
12:43:23.270446 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
12:43:23.273110 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
12:43:23.273420 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
12:43:23.397780 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
12:43:23.414960 [debug] [Thread-1  ]: finished collecting timing info
12:43:23.415347 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
12:43:23.415639 [debug] [Thread-1  ]: finished collecting timing info
12:43:23.416211 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
12:43:23.416598 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
12:43:23.417487 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
12:43:23.417716 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
12:43:23.417983 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
12:43:23.487025 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
12:43:23.488234 [debug] [Thread-1  ]: finished collecting timing info
12:43:23.488524 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
12:43:23.489012 [debug] [Thread-1  ]: finished collecting timing info
12:43:23.489687 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
12:43:23.490868 [debug] [MainThread]: Connection 'master' was properly closed.
12:43:23.491241 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
12:43:23.515732 [info ] [MainThread]: Done.
12:43:23.516223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db5a820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db5a040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db5a190>]}


============================== 2022-03-23 22:34:50.398367 | 3d61d4a5-8b17-47b7-980d-8300b6c465f6 ==============================
22:34:50.398367 [info ] [MainThread]: Running with dbt=1.0.3
22:34:50.400197 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
22:34:50.400689 [debug] [MainThread]: Tracking: tracking
22:34:50.466934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067183d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10674df40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10674dd00>]}
22:34:50.595011 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
22:34:50.595421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3d61d4a5-8b17-47b7-980d-8300b6c465f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067195b0>]}
22:34:52.486711 [debug] [MainThread]: Parsing macros/adapters.sql
22:34:52.512161 [debug] [MainThread]: Parsing macros/materializations/seed.sql
22:34:52.516319 [debug] [MainThread]: Parsing macros/materializations/view.sql
22:34:52.516917 [debug] [MainThread]: Parsing macros/materializations/table.sql
22:34:52.519712 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
22:34:52.536230 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
22:34:52.540899 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
22:34:52.544675 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
22:34:52.549867 [debug] [MainThread]: Parsing macros/adapters.sql
22:34:52.586299 [debug] [MainThread]: Parsing macros/materializations/seed.sql
22:34:52.594913 [debug] [MainThread]: Parsing macros/materializations/view.sql
22:34:52.595426 [debug] [MainThread]: Parsing macros/materializations/table.sql
22:34:52.598302 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
22:34:52.621879 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
22:34:52.626578 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
22:34:52.632382 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
22:34:52.637626 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
22:34:52.641103 [debug] [MainThread]: Parsing macros/materializations/configs.sql
22:34:52.643196 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
22:34:52.644675 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
22:34:52.661433 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
22:34:52.672495 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
22:34:52.683715 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
22:34:52.687823 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
22:34:52.689423 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
22:34:52.691068 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
22:34:52.695079 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
22:34:52.705613 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
22:34:52.706971 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
22:34:52.716184 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
22:34:52.731062 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
22:34:52.737802 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
22:34:52.740372 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
22:34:52.746960 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
22:34:52.748128 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
22:34:52.750521 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
22:34:52.752636 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
22:34:52.758099 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
22:34:52.773198 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
22:34:52.774527 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
22:34:52.776715 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
22:34:52.778126 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
22:34:52.778922 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
22:34:52.779426 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
22:34:52.780055 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
22:34:52.781291 [debug] [MainThread]: Parsing macros/etc/statement.sql
22:34:52.785264 [debug] [MainThread]: Parsing macros/etc/datetime.sql
22:34:52.792839 [debug] [MainThread]: Parsing macros/adapters/schema.sql
22:34:52.794798 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
22:34:52.797309 [debug] [MainThread]: Parsing macros/adapters/relation.sql
22:34:52.806209 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
22:34:52.808846 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
22:34:52.812818 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
22:34:52.819424 [debug] [MainThread]: Parsing macros/adapters/columns.sql
22:34:52.828214 [debug] [MainThread]: Parsing tests/generic/builtin.sql
22:34:52.830924 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
22:34:52.831884 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
22:34:52.833048 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
22:34:52.834068 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
22:34:52.839648 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
22:34:52.840666 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
22:34:52.841832 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
22:34:52.844393 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
22:34:52.845359 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
22:34:52.847208 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
22:34:52.849419 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
22:34:52.858403 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
22:34:52.860080 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
22:34:52.861374 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
22:34:52.862672 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
22:34:52.864307 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
22:34:52.865611 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
22:34:52.867029 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
22:34:52.867895 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
22:34:52.871071 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
22:34:52.875839 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
22:34:52.877297 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
22:34:52.880467 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
22:34:52.882112 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
22:34:52.883457 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
22:34:52.885178 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
22:34:52.907975 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
22:34:52.909791 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
22:34:52.912151 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
22:34:52.913591 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
22:34:52.914649 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
22:34:52.915766 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
22:34:52.916792 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
22:34:52.917865 [debug] [MainThread]: Parsing macros/schema_tests/fewer_rows_than.sql
22:34:52.919390 [debug] [MainThread]: Parsing macros/schema_tests/equal_rowcount.sql
22:34:52.921003 [debug] [MainThread]: Parsing macros/schema_tests/relationships_where.sql
22:34:52.923077 [debug] [MainThread]: Parsing macros/schema_tests/recency.sql
22:34:52.924676 [debug] [MainThread]: Parsing macros/schema_tests/not_constant.sql
22:34:52.925819 [debug] [MainThread]: Parsing macros/schema_tests/accepted_range.sql
22:34:52.928031 [debug] [MainThread]: Parsing macros/schema_tests/not_accepted_values.sql
22:34:52.929963 [debug] [MainThread]: Parsing macros/schema_tests/test_unique_where.sql
22:34:52.931270 [debug] [MainThread]: Parsing macros/schema_tests/at_least_one.sql
22:34:52.932417 [debug] [MainThread]: Parsing macros/schema_tests/unique_combination_of_columns.sql
22:34:52.935003 [debug] [MainThread]: Parsing macros/schema_tests/cardinality_equality.sql
22:34:52.936792 [debug] [MainThread]: Parsing macros/schema_tests/expression_is_true.sql
22:34:52.938522 [debug] [MainThread]: Parsing macros/schema_tests/not_null_proportion.sql
22:34:52.940507 [debug] [MainThread]: Parsing macros/schema_tests/sequential_values.sql
22:34:52.943070 [debug] [MainThread]: Parsing macros/schema_tests/test_not_null_where.sql
22:34:52.944377 [debug] [MainThread]: Parsing macros/schema_tests/equality.sql
22:34:52.947588 [debug] [MainThread]: Parsing macros/schema_tests/mutually_exclusive_ranges.sql
22:34:52.955876 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
22:34:52.960245 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
22:34:52.961733 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
22:34:52.964816 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
22:34:52.968737 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
22:34:52.972126 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
22:34:52.973752 [debug] [MainThread]: Parsing macros/sql/star.sql
22:34:52.977992 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
22:34:52.985095 [debug] [MainThread]: Parsing macros/sql/union.sql
22:34:52.994396 [debug] [MainThread]: Parsing macros/sql/groupby.sql
22:34:52.995609 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
22:34:52.999303 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
22:34:53.000825 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
22:34:53.002312 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
22:34:53.008075 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
22:34:53.012795 [debug] [MainThread]: Parsing macros/sql/pivot.sql
22:34:53.016500 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
22:34:53.018582 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
22:34:53.019745 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
22:34:53.025370 [debug] [MainThread]: Parsing macros/get_date_dimension.sql
22:34:53.048036 [debug] [MainThread]: Parsing macros/get_base_dates.sql
22:34:53.052860 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_year_dates.sql
22:34:53.056703 [debug] [MainThread]: Parsing macros/fiscal_date/get_fiscal_periods.sql
22:34:53.058323 [debug] [MainThread]: Parsing macros/calendar_date/tomorrow.sql
22:34:53.058960 [debug] [MainThread]: Parsing macros/calendar_date/next_week.sql
22:34:53.059488 [debug] [MainThread]: Parsing macros/calendar_date/next_month_name.sql
22:34:53.060155 [debug] [MainThread]: Parsing macros/calendar_date/next_month.sql
22:34:53.060678 [debug] [MainThread]: Parsing macros/calendar_date/day_name.sql
22:34:53.063595 [debug] [MainThread]: Parsing macros/calendar_date/to_unixtimestamp.sql
22:34:53.065180 [debug] [MainThread]: Parsing macros/calendar_date/n_days_away.sql
22:34:53.065999 [debug] [MainThread]: Parsing macros/calendar_date/week_start.sql
22:34:53.068502 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_start.sql
22:34:53.070887 [debug] [MainThread]: Parsing macros/calendar_date/n_days_ago.sql
22:34:53.071866 [debug] [MainThread]: Parsing macros/calendar_date/last_week.sql
22:34:53.072396 [debug] [MainThread]: Parsing macros/calendar_date/now.sql
22:34:53.072957 [debug] [MainThread]: Parsing macros/calendar_date/periods_since.sql
22:34:53.073632 [debug] [MainThread]: Parsing macros/calendar_date/today.sql
22:34:53.074144 [debug] [MainThread]: Parsing macros/calendar_date/last_month.sql
22:34:53.074668 [debug] [MainThread]: Parsing macros/calendar_date/day_of_year.sql
22:34:53.076390 [debug] [MainThread]: Parsing macros/calendar_date/from_unixtimestamp.sql
22:34:53.081475 [debug] [MainThread]: Parsing macros/calendar_date/n_months_ago.sql
22:34:53.082499 [debug] [MainThread]: Parsing macros/calendar_date/date_part.sql
22:34:53.083790 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_away.sql
22:34:53.084630 [debug] [MainThread]: Parsing macros/calendar_date/day_of_month.sql
22:34:53.085596 [debug] [MainThread]: Parsing macros/calendar_date/yesterday.sql
22:34:53.086183 [debug] [MainThread]: Parsing macros/calendar_date/day_of_week.sql
22:34:53.092848 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_end.sql
22:34:53.094875 [debug] [MainThread]: Parsing macros/calendar_date/n_weeks_ago.sql
22:34:53.095746 [debug] [MainThread]: Parsing macros/calendar_date/month_name.sql
22:34:53.098613 [debug] [MainThread]: Parsing macros/calendar_date/last_month_name.sql
22:34:53.099365 [debug] [MainThread]: Parsing macros/calendar_date/week_of_year.sql
22:34:53.101300 [debug] [MainThread]: Parsing macros/calendar_date/convert_timezone.sql
22:34:53.105497 [debug] [MainThread]: Parsing macros/calendar_date/n_months_away.sql
22:34:53.106339 [debug] [MainThread]: Parsing macros/calendar_date/iso_week_of_year.sql
22:34:53.108808 [debug] [MainThread]: Parsing macros/calendar_date/week_end.sql
22:34:53.111087 [debug] [MainThread]: Parsing macros/calendar_date/next_month_number.sql
22:34:53.111707 [debug] [MainThread]: Parsing macros/calendar_date/last_month_number.sql
22:34:53.112375 [debug] [MainThread]: Parsing macros/tables/databricks/sat.sql
22:34:53.119821 [debug] [MainThread]: Parsing macros/tables/databricks/hub.sql
22:34:53.132109 [debug] [MainThread]: Parsing macros/tables/databricks/link.sql
22:34:53.143230 [debug] [MainThread]: Parsing macros/tables/bigquery/xts.sql
22:34:53.144032 [debug] [MainThread]: Parsing macros/tables/bigquery/ma_sat.sql
22:34:53.162504 [debug] [MainThread]: Parsing macros/tables/bigquery/t_link.sql
22:34:53.163528 [debug] [MainThread]: Parsing macros/tables/bigquery/sat.sql
22:34:53.172577 [debug] [MainThread]: Parsing macros/tables/bigquery/hub.sql
22:34:53.183263 [debug] [MainThread]: Parsing macros/tables/bigquery/eff_sat.sql
22:34:53.195458 [debug] [MainThread]: Parsing macros/tables/bigquery/link.sql
22:34:53.207353 [debug] [MainThread]: Parsing macros/tables/snowflake/xts.sql
22:34:53.216055 [debug] [MainThread]: Parsing macros/tables/snowflake/pit.sql
22:34:53.240937 [debug] [MainThread]: Parsing macros/tables/snowflake/ma_sat.sql
22:34:53.262808 [debug] [MainThread]: Parsing macros/tables/snowflake/t_link.sql
22:34:53.274581 [debug] [MainThread]: Parsing macros/tables/snowflake/sat.sql
22:34:53.305109 [debug] [MainThread]: Parsing macros/tables/snowflake/hub.sql
22:34:53.327823 [debug] [MainThread]: Parsing macros/tables/snowflake/eff_sat.sql
22:34:53.342778 [debug] [MainThread]: Parsing macros/tables/snowflake/link.sql
22:34:53.356250 [debug] [MainThread]: Parsing macros/tables/snowflake/bridge.sql
22:34:53.396550 [debug] [MainThread]: Parsing macros/tables/sqlserver/xts.sql
22:34:53.397494 [debug] [MainThread]: Parsing macros/tables/sqlserver/ma_sat.sql
22:34:53.413350 [debug] [MainThread]: Parsing macros/tables/sqlserver/t_link.sql
22:34:53.414352 [debug] [MainThread]: Parsing macros/tables/sqlserver/sat.sql
22:34:53.423168 [debug] [MainThread]: Parsing macros/tables/sqlserver/hub.sql
22:34:53.436135 [debug] [MainThread]: Parsing macros/tables/sqlserver/eff_sat.sql
22:34:53.448685 [debug] [MainThread]: Parsing macros/tables/sqlserver/link.sql
22:34:53.461417 [debug] [MainThread]: Parsing macros/staging/hash_columns.sql
22:34:53.465044 [debug] [MainThread]: Parsing macros/staging/source_columns.sql
22:34:53.466199 [debug] [MainThread]: Parsing macros/staging/stage.sql
22:34:53.476533 [debug] [MainThread]: Parsing macros/staging/rank_columns.sql
22:34:53.485212 [debug] [MainThread]: Parsing macros/staging/derive_columns.sql
22:34:53.491035 [debug] [MainThread]: Parsing macros/internal/metadata_processing/as_constant.sql
22:34:53.493415 [debug] [MainThread]: Parsing macros/internal/metadata_processing/escape_column_names.sql
22:34:53.506287 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias.sql
22:34:53.509520 [debug] [MainThread]: Parsing macros/internal/metadata_processing/concat_ws.sql
22:34:53.511534 [debug] [MainThread]: Parsing macros/internal/metadata_processing/multikey.sql
22:34:53.516361 [debug] [MainThread]: Parsing macros/internal/metadata_processing/expand_column_list.sql
22:34:53.519426 [debug] [MainThread]: Parsing macros/internal/metadata_processing/check_required_parameters.sql
22:34:53.521027 [debug] [MainThread]: Parsing macros/internal/metadata_processing/alias_all.sql
22:34:53.523251 [debug] [MainThread]: Parsing macros/internal/helpers/prepend_generated_by.sql
22:34:53.523626 [debug] [MainThread]: Parsing macros/internal/helpers/is_checks.sql
22:34:53.527898 [debug] [MainThread]: Parsing macros/internal/helpers/stage_processing_macros.sql
22:34:53.536610 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_rank_materialization.sql
22:34:53.552341 [debug] [MainThread]: Parsing macros/materialisations/incremental_bridge_materialization.sql
22:34:53.560042 [debug] [MainThread]: Parsing macros/materialisations/shared_helpers.sql
22:34:53.561415 [debug] [MainThread]: Parsing macros/materialisations/vault_insert_by_period_materialization.sql
22:34:53.586789 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_bridge_replace.sql
22:34:53.589067 [debug] [MainThread]: Parsing macros/materialisations/incremental_pit_materialization.sql
22:34:53.594026 [debug] [MainThread]: Parsing macros/materialisations/mat_is_checks.sql
22:34:53.600159 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/get_min_max_ranks.sql
22:34:53.603354 [debug] [MainThread]: Parsing macros/materialisations/rank_mat_helpers/replace_placeholder_with_rank_filter.sql
22:34:53.606926 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/replace_placeholder_with_period_filter.sql
22:34:53.613857 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_boundaries.sql
22:34:53.624855 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_start_stop_dates.sql
22:34:53.629062 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_filter_sql.sql
22:34:53.634185 [debug] [MainThread]: Parsing macros/materialisations/period_mat_helpers/get_period_of_load.sql
22:34:53.638921 [debug] [MainThread]: Parsing macros/supporting/datatypes.sql
22:34:53.639956 [debug] [MainThread]: Parsing macros/supporting/max_datetime.sql
22:34:53.641558 [debug] [MainThread]: Parsing macros/supporting/hash.sql
22:34:53.663079 [debug] [MainThread]: Parsing macros/supporting/prefix.sql
22:34:53.668352 [debug] [MainThread]: Parsing macros/supporting/current_timestamp.sql
22:34:53.670612 [debug] [MainThread]: Parsing macros/supporting/get_query_results_as_dict.sql
22:34:54.497383 [debug] [MainThread]: 1699: static parser successfully parsed raw_stage/raw_customer.sql
22:34:54.509021 [debug] [MainThread]: 1603: static parser failed on datavault/sat_customer.sql
22:34:54.579456 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/sat_customer.sql
22:34:54.580660 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
22:34:54.629855 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
22:34:54.630939 [debug] [MainThread]: 1603: static parser failed on stage/stg_customer.sql
22:34:54.742044 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_customer.sql
22:34:54.855112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d61d4a5-8b17-47b7-980d-8300b6c465f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a480d0>]}
22:34:54.969944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d61d4a5-8b17-47b7-980d-8300b6c465f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10679bca0>]}
22:34:54.970361 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
22:34:54.971639 [info ] [MainThread]: 
22:34:54.972134 [debug] [MainThread]: Acquiring new databricks connection "master"
22:34:54.973044 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
22:34:54.984759 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
22:34:54.985023 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
22:34:54.985192 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
22:34:54.985353 [debug] [ThreadPool]: Opening a new connection, currently in state init
22:36:57.734895 [debug] [MainThread]: Connection 'master' was properly closed.
22:36:57.735518 [debug] [MainThread]: Connection 'list_None_test_dbt' was properly closed.


============================== 2022-03-23 22:37:06.860186 | 8ed6ab57-b001-413b-aaea-749e2b70aa7c ==============================
22:37:06.860186 [info ] [MainThread]: Running with dbt=1.0.3
22:37:06.861362 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
22:37:06.861889 [debug] [MainThread]: Tracking: tracking
22:37:06.882581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f96550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fcaf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fcad00>]}
22:37:07.324958 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
22:37:07.325280 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
22:37:07.334538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ed6ab57-b001-413b-aaea-749e2b70aa7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a40d0>]}
22:37:07.352728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ed6ab57-b001-413b-aaea-749e2b70aa7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106168400>]}
22:37:07.353065 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
22:37:07.354351 [info ] [MainThread]: 
22:37:07.354864 [debug] [MainThread]: Acquiring new databricks connection "master"
22:37:07.355806 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
22:37:07.367291 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
22:37:07.367557 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
22:37:07.367732 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
22:37:07.367899 [debug] [ThreadPool]: Opening a new connection, currently in state init
22:37:40.801268 [debug] [MainThread]: Connection 'master' was properly closed.
22:37:40.801688 [debug] [MainThread]: Connection 'list_None_test_dbt' was left open.
22:37:40.801922 [debug] [MainThread]: On list_None_test_dbt: ROLLBACK
22:37:40.802134 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
22:37:40.802342 [debug] [MainThread]: On list_None_test_dbt: Close
22:37:43.582509 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
22:37:43.583445 [debug] [ThreadPool]: Databricks adapter: No query yet
22:37:43.583913 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro list_relations_without_caching
22:37:43.584184 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  No query yet
22:37:43.584658 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about test_dbt: Runtime Error
  No query yet
22:37:45.552215 [debug] [ThreadPool]: On list_None_test_dbt: Close
22:37:46.641475 [debug] [MainThread]: Connection 'master' was properly closed.


============================== 2022-03-23 22:37:57.953584 | eb7aea0a-d83b-42ea-9cff-8edbb263f884 ==============================
22:37:57.953584 [info ] [MainThread]: Running with dbt=1.0.3
22:37:57.954198 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, config_dir=False, defer=None, state=None, cls=<class 'dbt.task.debug.DebugTask'>, which='debug', rpc_method=None)
22:37:57.954396 [debug] [MainThread]: Tracking: tracking
22:37:57.972764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fc8370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fc8e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fc89d0>]}
22:37:58.245913 [debug] [MainThread]: Executing "git --help"
22:37:58.258585 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
22:37:58.259433 [debug] [MainThread]: STDERR: "b''"
22:37:58.266420 [debug] [MainThread]: Acquiring new databricks connection "debug"
22:37:58.267657 [debug] [MainThread]: Using databricks connection "debug"
22:37:58.268044 [debug] [MainThread]: On debug: select 1 as id
22:37:58.268372 [debug] [MainThread]: Opening a new connection, currently in state init
22:38:12.338385 [debug] [MainThread]: SQL status: OK in 14.07 seconds
22:38:12.339923 [debug] [MainThread]: On debug: Close
22:38:12.566327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107205df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107205a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107205310>]}
22:38:13.632748 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-03-23 22:38:21.059315 | fb848805-0bfc-4365-af55-b9a6c9fe13a6 ==============================
22:38:21.059315 [info ] [MainThread]: Running with dbt=1.0.3
22:38:21.060301 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, parse_only=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, full_refresh=False, defer=None, cls=<class 'dbt.task.compile.CompileTask'>, which='compile', rpc_method='compile')
22:38:21.060621 [debug] [MainThread]: Tracking: tracking
22:38:21.080102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8a6340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8a60d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8a62b0>]}
22:38:21.434281 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
22:38:21.434602 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
22:38:21.454973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb848805-0bfc-4365-af55-b9a6c9fe13a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abae0d0>]}
22:38:21.471006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb848805-0bfc-4365-af55-b9a6c9fe13a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa77370>]}
22:38:21.471347 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
22:38:21.472664 [info ] [MainThread]: 
22:38:21.473168 [debug] [MainThread]: Acquiring new databricks connection "master"
22:38:21.474148 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
22:38:21.485275 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
22:38:21.485558 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
22:38:21.485759 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
22:38:21.485961 [debug] [ThreadPool]: Opening a new connection, currently in state init
22:38:27.081644 [debug] [ThreadPool]: SQL status: OK in 5.6 seconds
22:38:27.580179 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
22:38:27.580486 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
22:38:27.580709 [debug] [ThreadPool]: On list_None_test_dbt: Close
22:38:27.769851 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
22:38:27.770407 [info ] [MainThread]: 
22:38:27.803052 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
22:38:27.803617 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
22:38:27.806348 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
22:38:27.806571 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
22:38:27.905589 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
22:38:27.910957 [debug] [Thread-1  ]: finished collecting timing info
22:38:27.911217 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
22:38:27.911406 [debug] [Thread-1  ]: finished collecting timing info
22:38:27.911778 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
22:38:27.911970 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
22:38:27.912298 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
22:38:27.912455 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
22:38:27.912594 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
22:38:27.952849 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
22:38:27.957743 [debug] [Thread-1  ]: finished collecting timing info
22:38:27.957995 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
22:38:27.958208 [debug] [Thread-1  ]: finished collecting timing info
22:38:27.958691 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
22:38:27.959465 [debug] [MainThread]: Connection 'master' was properly closed.
22:38:27.959632 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
22:38:27.979671 [info ] [MainThread]: Done.
22:38:27.980122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac334f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac33040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac33520>]}


============================== 2022-03-23 22:41:41.700170 | 61fc0dcb-abe9-4ed6-8480-2fe4f9a96df9 ==============================
22:41:41.700170 [info ] [MainThread]: Running with dbt=1.0.3
22:41:41.701333 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
22:41:41.701680 [debug] [MainThread]: Tracking: tracking
22:41:41.754167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a735f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7698b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a769af0>]}
22:41:43.664960 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
22:41:43.665219 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
22:41:43.675499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61fc0dcb-abe9-4ed6-8480-2fe4f9a96df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa420d0>]}
22:41:43.773906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61fc0dcb-abe9-4ed6-8480-2fe4f9a96df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a909430>]}
22:41:43.774293 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
22:41:43.775698 [info ] [MainThread]: 
22:41:43.776297 [debug] [MainThread]: Acquiring new databricks connection "master"
22:41:43.777405 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
22:41:43.789523 [debug] [ThreadPool]: Using databricks connection "list_schemas"
22:41:43.789793 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
22:41:43.789971 [debug] [ThreadPool]: Opening a new connection, currently in state init
22:41:44.547646 [debug] [ThreadPool]: SQL status: OK in 0.76 seconds
22:41:44.885801 [debug] [ThreadPool]: On list_schemas: Close
22:41:45.061529 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
22:41:45.071647 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
22:41:45.071911 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
22:41:45.072091 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
22:41:45.072256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
22:41:46.101239 [debug] [ThreadPool]: SQL status: OK in 1.03 seconds
22:41:46.601724 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
22:41:46.601978 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
22:41:46.602146 [debug] [ThreadPool]: On list_None_test_dbt: Close
22:41:46.802863 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
22:41:46.803125 [debug] [MainThread]: Spark adapter: NotImplemented: commit
22:41:46.803487 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
22:41:46.803772 [info ] [MainThread]: 
22:41:46.817738 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
22:41:46.818150 [info ] [Thread-1  ]: 1 of 2 START incremental model test_dbt.hub_customer............................ [RUN]
22:41:46.818717 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
22:41:46.821681 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
22:41:46.821987 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
22:41:46.943637 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
22:41:46.950584 [debug] [Thread-1  ]: finished collecting timing info
22:41:46.950859 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
22:41:46.999266 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
22:41:46.999600 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    create temporary view hub_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE
    FROM
    (
        SELECT rr.CUSTOMER_HK, rr.CUSTOMER_ID, rr.LOAD_DATETIME, rr.RECORD_SOURCE,
               ROW_NUMBER() OVER(
                   PARTITION BY rr.CUSTOMER_HK
                   ORDER BY rr.LOAD_DATETIME
               ) AS row_number
        FROM test_dbt.stg_customer AS rr
        WHERE rr.CUSTOMER_HK IS NOT NULL
    ) h
    WHERE h.row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    LEFT JOIN test_dbt.hub_customer AS d
    ON a.CUSTOMER_HK = d.CUSTOMER_HK
    WHERE d.CUSTOMER_HK IS NULL
)

SELECT * FROM records_to_insert

  
22:41:46.999836 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
22:41:48.104717 [debug] [Thread-1  ]: SQL status: OK in 1.1 seconds
22:41:48.310622 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
22:41:48.313627 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
22:41:48.313844 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
22:41:48.314019 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    insert into table test_dbt.hub_customer
    select `CUSTOMER_HK`, `CUSTOMER_ID`, `LOAD_DATETIME`, `RECORD_SOURCE` from hub_customer__dbt_tmp


22:42:04.455233 [debug] [Thread-1  ]: SQL status: OK in 16.14 seconds
22:42:04.462908 [debug] [Thread-1  ]: finished collecting timing info
22:42:04.463168 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
22:42:04.463356 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
22:42:04.463535 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
22:42:05.056596 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61fc0dcb-abe9-4ed6-8480-2fe4f9a96df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aad8670>]}
22:42:05.057297 [info ] [Thread-1  ]: 1 of 2 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 18.24s]
22:42:05.057866 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
22:42:05.058216 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
22:42:05.058742 [info ] [Thread-1  ]: 2 of 2 START incremental model test_dbt.sat_customer............................ [RUN]
22:42:05.059358 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
22:42:05.062367 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
22:42:05.062604 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
22:42:05.114434 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
22:42:05.118256 [debug] [Thread-1  ]: finished collecting timing info
22:42:05.118550 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
22:42:05.123372 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
22:42:05.123579 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    create temporary view sat_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_HASHDIFF, a.FIRST_NAME, a.LAST_NAME, a.EMAIL, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM test_dbt.stg_customer AS a
    WHERE a.CUSTOMER_HK IS NOT NULL
),


latest_records_pre AS (
    SELECT current_records.CUSTOMER_HK, current_records.HASHDIFF, current_records.LOAD_DATETIME,
           RANK() OVER (
               PARTITION BY current_records.CUSTOMER_HK
               ORDER BY current_records.LOAD_DATETIME DESC
           ) AS rank
    FROM test_dbt.sat_customer AS current_records
    JOIN (
        SELECT DISTINCT source_data.CUSTOMER_HK
        FROM source_data
    ) AS source_records
    ON current_records.CUSTOMER_HK = source_records.CUSTOMER_HK
),
latest_records AS (

    SELECT *
    FROM latest_records_pre
    WHERE rank = 1
),

records_to_insert AS (
    SELECT DISTINCT stage.CUSTOMER_HK, stage.CUSTOMER_HASHDIFF AS HASHDIFF, stage.FIRST_NAME, stage.LAST_NAME, stage.EMAIL, stage.LOAD_DATETIME, stage.RECORD_SOURCE
    FROM source_data AS stage
    LEFT JOIN latest_records
    ON latest_records.CUSTOMER_HK = stage.CUSTOMER_HK
    WHERE latest_records.HASHDIFF != stage.CUSTOMER_HASHDIFF
        OR latest_records.HASHDIFF IS NULL
)

SELECT * FROM records_to_insert

  
22:42:05.123740 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
22:42:05.785990 [debug] [Thread-1  ]: SQL status: OK in 0.66 seconds
22:42:05.971591 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.sat_customer"
22:42:05.974387 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
22:42:05.974619 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
22:42:05.974789 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    insert into table test_dbt.sat_customer
    select `CUSTOMER_HK`, `HASHDIFF`, `FIRST_NAME`, `LAST_NAME`, `EMAIL`, `LOAD_DATETIME`, `RECORD_SOURCE` from sat_customer__dbt_tmp


22:42:11.343338 [debug] [Thread-1  ]: SQL status: OK in 5.37 seconds
22:42:11.345766 [debug] [Thread-1  ]: finished collecting timing info
22:42:11.346040 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: ROLLBACK
22:42:11.346258 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
22:42:11.346464 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: Close
22:42:11.761254 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61fc0dcb-abe9-4ed6-8480-2fe4f9a96df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ababb20>]}
22:42:11.761916 [info ] [Thread-1  ]: 2 of 2 OK created incremental model test_dbt.sat_customer....................... [[32mOK[0m in 6.70s]
22:42:11.762453 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
22:42:11.763838 [debug] [MainThread]: Acquiring new databricks connection "master"
22:42:11.764105 [debug] [MainThread]: On master: ROLLBACK
22:42:11.764314 [debug] [MainThread]: Opening a new connection, currently in state init
22:42:11.896593 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
22:42:11.897013 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
22:42:11.897293 [debug] [MainThread]: Spark adapter: NotImplemented: commit
22:42:11.897574 [debug] [MainThread]: On master: ROLLBACK
22:42:11.897854 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
22:42:11.898124 [debug] [MainThread]: On master: Close
22:42:11.986001 [info ] [MainThread]: 
22:42:11.986578 [info ] [MainThread]: Finished running 2 incremental models in 28.21s.
22:42:11.987031 [debug] [MainThread]: Connection 'master' was properly closed.
22:42:11.987290 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
22:42:12.015523 [info ] [MainThread]: 
22:42:12.015953 [info ] [MainThread]: [32mCompleted successfully[0m
22:42:12.016320 [info ] [MainThread]: 
22:42:12.016625 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
22:42:12.017054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7c3a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba4940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abb59a0>]}


============================== 2022-03-23 23:00:23.224554 | 5006a2db-b590-4ef5-8def-fdcbcc3e6a62 ==============================
23:00:23.224554 [info ] [MainThread]: Running with dbt=1.0.3
23:00:23.226005 [debug] [MainThread]: running dbt with arguments Namespace(record_timing_info=None, debug=None, log_format=None, write_json=None, use_colors=None, printer_width=None, warn_error=None, version_check=None, partial_parse=None, single_threaded=False, use_experimental_parser=None, static_parser=None, profiles_dir='/Users/ananda.dwi/.dbt', send_anonymous_usage_stats=None, fail_fast=None, event_buffer_size=None, project_dir=None, profile=None, target=None, vars='{}', log_cache_events=False, threads=None, select=['tag:datavault'], exclude=None, selector_name=None, state=None, defer=None, full_refresh=False, cls=<class 'dbt.task.run.RunTask'>, which='run', rpc_method='run')
23:00:23.226350 [debug] [MainThread]: Tracking: tracking
23:00:23.244623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110299970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104a9dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104a94c0>]}
23:00:23.604806 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
23:00:23.605302 [debug] [MainThread]: Partial parsing: updated file: test_dbx://models/datavault/hub_customer.sql
23:00:23.620318 [debug] [MainThread]: 1603: static parser failed on datavault/hub_customer.sql
23:00:23.717954 [debug] [MainThread]: 1602: parser fallback to jinja rendering on datavault/hub_customer.sql
23:00:23.733275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5006a2db-b590-4ef5-8def-fdcbcc3e6a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108180d0>]}
23:00:23.748368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5006a2db-b590-4ef5-8def-fdcbcc3e6a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11064c430>]}
23:00:23.748669 [info ] [MainThread]: Found 4 models, 0 tests, 0 snapshots, 0 analyses, 632 macros, 0 operations, 0 seed files, 4 sources, 0 exposures, 0 metrics
23:00:23.749843 [info ] [MainThread]: 
23:00:23.750341 [debug] [MainThread]: Acquiring new databricks connection "master"
23:00:23.751285 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
23:00:23.761261 [debug] [ThreadPool]: Using databricks connection "list_schemas"
23:00:23.761544 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
23:00:23.761719 [debug] [ThreadPool]: Opening a new connection, currently in state init
23:00:24.422747 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
23:00:24.809228 [debug] [ThreadPool]: On list_schemas: Close
23:00:24.985080 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_test_dbt"
23:00:24.998439 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
23:00:24.998727 [debug] [ThreadPool]: Using databricks connection "list_None_test_dbt"
23:00:24.998909 [debug] [ThreadPool]: On list_None_test_dbt: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "connection_name": "list_None_test_dbt"} */
show table extended in test_dbt like '*'
  
23:00:24.999079 [debug] [ThreadPool]: Opening a new connection, currently in state closed
23:00:25.838070 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
23:00:26.166117 [debug] [ThreadPool]: On list_None_test_dbt: ROLLBACK
23:00:26.166439 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
23:00:26.166657 [debug] [ThreadPool]: On list_None_test_dbt: Close
23:00:26.347045 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
23:00:26.347432 [debug] [MainThread]: Spark adapter: NotImplemented: commit
23:00:26.347926 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
23:00:26.348330 [info ] [MainThread]: 
23:00:26.371052 [debug] [Thread-1  ]: Began running node model.test_dbx.hub_customer
23:00:26.371493 [info ] [Thread-1  ]: 1 of 2 START incremental model test_dbt.hub_customer............................ [RUN]
23:00:26.372067 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.hub_customer"
23:00:26.374556 [debug] [Thread-1  ]: Began compiling node model.test_dbx.hub_customer
23:00:26.374792 [debug] [Thread-1  ]: Compiling model.test_dbx.hub_customer
23:00:26.393024 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.hub_customer"
23:00:26.394666 [debug] [Thread-1  ]: finished collecting timing info
23:00:26.394910 [debug] [Thread-1  ]: Began executing node model.test_dbx.hub_customer
23:00:26.443252 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
23:00:26.443523 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    create temporary view hub_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT CUSTOMER_HK, CUSTOMER_ID, LOAD_DATETIME, RECORD_SOURCE
    FROM
    (
        SELECT rr.CUSTOMER_HK, rr.CUSTOMER_ID, rr.LOAD_DATETIME, rr.RECORD_SOURCE,
               ROW_NUMBER() OVER(
                   PARTITION BY rr.CUSTOMER_HK
                   ORDER BY rr.LOAD_DATETIME
               ) AS row_number
        FROM test_dbt.stg_customer AS rr
        WHERE rr.CUSTOMER_HK IS NOT NULL
    ) h
    WHERE h.row_number = 1
),

records_to_insert AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_ID, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM row_rank_1 AS a
    LEFT JOIN test_dbt.hub_customer AS d
    ON a.CUSTOMER_HK = d.CUSTOMER_HK
    WHERE d.CUSTOMER_HK IS NULL
)

SELECT * FROM records_to_insert

  
23:00:26.443691 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:00:27.108146 [debug] [Thread-1  ]: SQL status: OK in 0.66 seconds
23:00:27.316817 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.hub_customer"
23:00:27.318898 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:00:27.319185 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.hub_customer"
23:00:27.319376 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.hub_customer"} */

    
  
  
    merge into test_dbt.hub_customer as DBT_INTERNAL_DEST
      using hub_customer__dbt_tmp as DBT_INTERNAL_SOURCE
      
      
    
        on DBT_INTERNAL_SOURCE.CUSTOMER_HK = DBT_INTERNAL_DEST.CUSTOMER_HK
    
  
      
      when matched then update set
         * 
    
      when not matched then insert *

23:00:33.742634 [debug] [Thread-1  ]: SQL status: OK in 6.42 seconds
23:00:33.749996 [debug] [Thread-1  ]: finished collecting timing info
23:00:33.750239 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: ROLLBACK
23:00:33.750428 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:00:33.750605 [debug] [Thread-1  ]: On model.test_dbx.hub_customer: Close
23:00:34.174506 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5006a2db-b590-4ef5-8def-fdcbcc3e6a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109c9dc0>]}
23:00:34.175196 [info ] [Thread-1  ]: 1 of 2 OK created incremental model test_dbt.hub_customer....................... [[32mOK[0m in 7.80s]
23:00:34.175752 [debug] [Thread-1  ]: Finished running node model.test_dbx.hub_customer
23:00:34.176089 [debug] [Thread-1  ]: Began running node model.test_dbx.sat_customer
23:00:34.176731 [info ] [Thread-1  ]: 2 of 2 START incremental model test_dbt.sat_customer............................ [RUN]
23:00:34.177347 [debug] [Thread-1  ]: Acquiring new databricks connection "model.test_dbx.sat_customer"
23:00:34.180297 [debug] [Thread-1  ]: Began compiling node model.test_dbx.sat_customer
23:00:34.180536 [debug] [Thread-1  ]: Compiling model.test_dbx.sat_customer
23:00:34.233520 [debug] [Thread-1  ]: Writing injected SQL for node "model.test_dbx.sat_customer"
23:00:34.236068 [debug] [Thread-1  ]: finished collecting timing info
23:00:34.236359 [debug] [Thread-1  ]: Began executing node model.test_dbx.sat_customer
23:00:34.240435 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
23:00:34.240620 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    create temporary view sat_customer__dbt_tmp as
    

-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.CUSTOMER_HK, a.CUSTOMER_HASHDIFF, a.FIRST_NAME, a.LAST_NAME, a.EMAIL, a.LOAD_DATETIME, a.RECORD_SOURCE
    FROM test_dbt.stg_customer AS a
    WHERE a.CUSTOMER_HK IS NOT NULL
),


latest_records_pre AS (
    SELECT current_records.CUSTOMER_HK, current_records.HASHDIFF, current_records.LOAD_DATETIME,
           RANK() OVER (
               PARTITION BY current_records.CUSTOMER_HK
               ORDER BY current_records.LOAD_DATETIME DESC
           ) AS rank
    FROM test_dbt.sat_customer AS current_records
    JOIN (
        SELECT DISTINCT source_data.CUSTOMER_HK
        FROM source_data
    ) AS source_records
    ON current_records.CUSTOMER_HK = source_records.CUSTOMER_HK
),
latest_records AS (

    SELECT *
    FROM latest_records_pre
    WHERE rank = 1
),

records_to_insert AS (
    SELECT DISTINCT stage.CUSTOMER_HK, stage.CUSTOMER_HASHDIFF AS HASHDIFF, stage.FIRST_NAME, stage.LAST_NAME, stage.EMAIL, stage.LOAD_DATETIME, stage.RECORD_SOURCE
    FROM source_data AS stage
    LEFT JOIN latest_records
    ON latest_records.CUSTOMER_HK = stage.CUSTOMER_HK
    WHERE latest_records.HASHDIFF != stage.CUSTOMER_HASHDIFF
        OR latest_records.HASHDIFF IS NULL
)

SELECT * FROM records_to_insert

  
23:00:34.240784 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
23:00:34.677732 [debug] [Thread-1  ]: SQL status: OK in 0.44 seconds
23:00:34.859872 [debug] [Thread-1  ]: Writing runtime SQL for node "model.test_dbx.sat_customer"
23:00:34.867899 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
23:00:34.868287 [debug] [Thread-1  ]: Using databricks connection "model.test_dbx.sat_customer"
23:00:34.868570 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: /* {"app": "dbt", "dbt_version": "1.0.3", "profile_name": "test_dbx", "target_name": "dev", "node_id": "model.test_dbx.sat_customer"} */

    insert into table test_dbt.sat_customer
    select `CUSTOMER_HK`, `HASHDIFF`, `FIRST_NAME`, `LAST_NAME`, `EMAIL`, `LOAD_DATETIME`, `RECORD_SOURCE` from sat_customer__dbt_tmp


23:00:39.019558 [debug] [Thread-1  ]: SQL status: OK in 4.15 seconds
23:00:39.022052 [debug] [Thread-1  ]: finished collecting timing info
23:00:39.022359 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: ROLLBACK
23:00:39.022578 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
23:00:39.022785 [debug] [Thread-1  ]: On model.test_dbx.sat_customer: Close
23:00:39.450977 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5006a2db-b590-4ef5-8def-fdcbcc3e6a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11082ac70>]}
23:00:39.451664 [info ] [Thread-1  ]: 2 of 2 OK created incremental model test_dbt.sat_customer....................... [[32mOK[0m in 5.27s]
23:00:39.452204 [debug] [Thread-1  ]: Finished running node model.test_dbx.sat_customer
23:00:39.453654 [debug] [MainThread]: Acquiring new databricks connection "master"
23:00:39.453925 [debug] [MainThread]: On master: ROLLBACK
23:00:39.454135 [debug] [MainThread]: Opening a new connection, currently in state init
23:00:39.568001 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
23:00:39.568414 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
23:00:39.568677 [debug] [MainThread]: Spark adapter: NotImplemented: commit
23:00:39.568952 [debug] [MainThread]: On master: ROLLBACK
23:00:39.569198 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
23:00:39.569437 [debug] [MainThread]: On master: Close
23:00:39.709688 [info ] [MainThread]: 
23:00:39.710266 [info ] [MainThread]: Finished running 2 incremental models in 15.96s.
23:00:39.710720 [debug] [MainThread]: Connection 'master' was properly closed.
23:00:39.711040 [debug] [MainThread]: Connection 'model.test_dbx.sat_customer' was properly closed.
23:00:39.733056 [info ] [MainThread]: 
23:00:39.733441 [info ] [MainThread]: [32mCompleted successfully[0m
23:00:39.733786 [info ] [MainThread]: 
23:00:39.734122 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
23:00:39.734873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104939a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110714a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110859820>]}
